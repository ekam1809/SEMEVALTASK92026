{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8b06af",
   "metadata": {},
   "source": [
    "## Setup: env, device, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a318f310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 16:57:41.613995: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-07 16:57:41.627130: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765155461.640305 3894870 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765155461.644280 3894870 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765155461.656625 3894870 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765155461.656640 3894870 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765155461.656642 3894870 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765155461.656644 3894870 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-07 16:57:41.660776: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.0\n",
      "Transformers: 4.57.1\n",
      "Using GPU: NVIDIA H100 80GB HBM3 MIG 2g.20gb\n",
      "LANG=eng, EN_MODEL=microsoft/deberta-v3-base, EPOCHS=3, LR=2e-05, BATCH_TRAIN=8, FOLDS=3\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0) SETUP: environment, device toggle, imports, config\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    pipeline,\n",
    ")\n",
    "import transformers\n",
    "import inspect\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Device selection\n",
    "# ------------------------------------------------------------\n",
    "RUN_DEVICE = \"gpu\"   # \"gpu\" or \"cpu\"\n",
    "\n",
    "if RUN_DEVICE.lower() == \"gpu\" and torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# ------------------------------------------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# High-level config\n",
    "# ------------------------------------------------------------\n",
    "LANG = \"eng\"                          # e.g. \"eng\", \"ben\", \"hin\"\n",
    "BASE = \"../dev_phase\"                 # root of organizer data\n",
    "EN_MODEL = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "MAX_LEN = 192\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "\n",
    "BATCH_TRAIN_GPU = 8\n",
    "BATCH_TRAIN_CPU = 4\n",
    "BATCH_EVAL = 8\n",
    "BATCH_TRAIN = BATCH_TRAIN_GPU if DEVICE.type == \"cuda\" else BATCH_TRAIN_CPU\n",
    "\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.1\n",
    "GRAD_ACCUM = 1\n",
    "N_FOLDS = 3  \n",
    "\n",
    "print(f\"LANG={LANG}, EN_MODEL={EN_MODEL}, EPOCHS={EPOCHS}, LR={LR}, \"\n",
    "      f\"BATCH_TRAIN={BATCH_TRAIN}, FOLDS={N_FOLDS}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Paths / dirs\n",
    "# ------------------------------------------------------------\n",
    "lang_fname = LANG  # adjust if filenames differ from LANG\n",
    "\n",
    "# TRAIN + DEV (DEV is UNLABELED)\n",
    "T1_TRAIN = f\"{BASE}/subtask1/train/{lang_fname}.csv\"\n",
    "T1_DEV   = f\"{BASE}/subtask1/dev/{lang_fname}.csv\"\n",
    "\n",
    "T2_TRAIN = f\"{BASE}/subtask2/train/{lang_fname}.csv\"\n",
    "T2_DEV   = f\"{BASE}/subtask2/dev/{lang_fname}.csv\"\n",
    "\n",
    "T3_TRAIN = f\"{BASE}/subtask3/train/{lang_fname}.csv\"\n",
    "T3_DEV   = f\"{BASE}/subtask3/dev/{lang_fname}.csv\"\n",
    "\n",
    "# roots for this method-2 DeBERTa pipeline\n",
    "ART_ROOT   = Path(\"artifacts\") / \"deberta_cv\" / LANG\n",
    "CACHE_ROOT = Path(\"cache\")     / \"deberta_cv\" / LANG   # both translation & probs\n",
    "OUT_ROOT   = Path(\"outputs\")   / \"deberta_cv\" / LANG\n",
    "SUB_ROOT   = Path(\"submissions\") / \"deberta\"\n",
    "\n",
    "for d in [ART_ROOT, CACHE_ROOT, OUT_ROOT, SUB_ROOT]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Submission subfolders\n",
    "(SUB_ROOT / \"subtask_1\").mkdir(parents=True, exist_ok=True)\n",
    "(SUB_ROOT / \"subtask_2\").mkdir(parents=True, exist_ok=True)\n",
    "(SUB_ROOT / \"subtask_3\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Label orders (training view)\n",
    "T2_LABELS = [\"gender/sexual\", \"political\", \"religious\", \"racial/ethnic\", \"other\"]\n",
    "T3_LABELS = [\"vilification\", \"extreme_language\", \"stereotype\",\n",
    "             \"invalidation\", \"lack_of_empathy\", \"dehumanization\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TrainingArguments capability detection\n",
    "# ------------------------------------------------------------\n",
    "_TA_PARAMS = inspect.signature(TrainingArguments.__init__).parameters\n",
    "TRAINER_CAPS = {\n",
    "    \"evaluation_strategy\": \"evaluation_strategy\" in _TA_PARAMS,\n",
    "    \"save_strategy\":       \"save_strategy\" in _TA_PARAMS,\n",
    "    \"warmup_ratio\":        \"warmup_ratio\" in _TA_PARAMS,\n",
    "    \"fp16\":                \"fp16\" in _TA_PARAMS,\n",
    "    \"no_cuda\":             \"no_cuda\" in _TA_PARAMS,\n",
    "    \"use_mps_device\":      \"use_mps_device\" in _TA_PARAMS,\n",
    "    \"report_to\":           \"report_to\" in _TA_PARAMS,\n",
    "    \"grad_accum\":          \"gradient_accumulation_steps\" in _TA_PARAMS,\n",
    "    \"eval_accum\":          \"eval_accumulation_steps\" in _TA_PARAMS,\n",
    "}\n",
    "\n",
    "def build_training_args(\n",
    "    output_dir,\n",
    "    per_device_train_batch_size,\n",
    "    per_device_eval_batch_size,\n",
    "    num_train_epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    logging_steps=50,\n",
    "    evaluation=\"epoch\",\n",
    "    save=\"no\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    warmup_steps=0,\n",
    "):\n",
    "    use_cuda_flag = (DEVICE.type == \"cuda\")\n",
    "    kwargs = dict(\n",
    "        output_dir=str(output_dir),\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        logging_steps=logging_steps,\n",
    "        dataloader_pin_memory=use_cuda_flag,\n",
    "        dataloader_num_workers=0,\n",
    "    )\n",
    "    if TRAINER_CAPS[\"evaluation_strategy\"]:\n",
    "        kwargs[\"evaluation_strategy\"] = evaluation\n",
    "    if TRAINER_CAPS[\"save_strategy\"]:\n",
    "        kwargs[\"save_strategy\"] = save\n",
    "    if TRAINER_CAPS[\"warmup_ratio\"]:\n",
    "        kwargs[\"warmup_ratio\"] = warmup_ratio\n",
    "    else:\n",
    "        kwargs[\"warmup_steps\"] = warmup_steps\n",
    "    if TRAINER_CAPS[\"fp16\"]:\n",
    "        kwargs[\"fp16\"] = False\n",
    "    if TRAINER_CAPS[\"no_cuda\"]:\n",
    "        kwargs[\"no_cuda\"] = not use_cuda_flag\n",
    "    if TRAINER_CAPS[\"use_mps_device\"]:\n",
    "        kwargs[\"use_mps_device\"] = False\n",
    "    if TRAINER_CAPS[\"report_to\"]:\n",
    "        kwargs[\"report_to\"] = \"none\"\n",
    "    if TRAINER_CAPS[\"grad_accum\"]:\n",
    "        kwargs[\"gradient_accumulation_steps\"] = GRAD_ACCUM\n",
    "    if TRAINER_CAPS[\"eval_accum\"]:\n",
    "        kwargs[\"eval_accumulation_steps\"] = 4\n",
    "    return TrainingArguments(**kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6189e46",
   "metadata": {},
   "source": [
    "## Dataset, metrics, calibration, focal loss, CV helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e738b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) DATASET + METRICS + CALIBRATION HELPERS\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "class TextClsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        labels: Optional[List] = None,\n",
    "        tokenizer=None,\n",
    "        max_len: int = 256,\n",
    "        is_multilabel: bool = False,\n",
    "    ):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.is_multilabel = is_multilabel\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        text = str(self.texts[idx])\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=False,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        if self.labels is not None:\n",
    "            y = self.labels[idx]\n",
    "            item[\"labels\"] = torch.tensor(\n",
    "                y,\n",
    "                dtype=torch.float if self.is_multilabel else torch.long,\n",
    "            )\n",
    "        return item\n",
    "\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "\n",
    "def grid_search_thresholds(y_true, y_prob, label_names=None):\n",
    "    \"\"\"\n",
    "    Per-label threshold search for multi-label tasks.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_prob = np.asarray(y_prob)\n",
    "    C = y_true.shape[1]\n",
    "    grid = np.linspace(0.05, 0.95, 19)\n",
    "    thrs = {}\n",
    "    for c in range(C):\n",
    "        best_t, best_f = 0.5, -1.0\n",
    "        for t in grid:\n",
    "            preds = (y_prob[:, c] >= t).astype(int)\n",
    "            f = f1_score(y_true[:, c], preds, average=\"binary\", zero_division=0)\n",
    "            if f > best_f:\n",
    "                best_f, best_t = f, t\n",
    "        name = label_names[c] if label_names else str(c)\n",
    "        thrs[name] = float(best_t)\n",
    "    return thrs\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Fixed temperature scaling (safe, F1-aware)\n",
    "# ============================================================\n",
    "\n",
    "class TempScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Temperature scaler with log-parameterization:\n",
    "      T = exp(log_T)  -> T > 0\n",
    "    and clamping to [min_T, max_T] for stability.\n",
    "    \"\"\"\n",
    "    def __init__(self, init_T: float = 1.0, min_T: float = 0.05, max_T: float = 10.0):\n",
    "        super().__init__()\n",
    "        self.log_T = nn.Parameter(torch.log(torch.tensor([init_T], dtype=torch.float32)))\n",
    "        self.min_T = min_T\n",
    "        self.max_T = max_T\n",
    "\n",
    "    def get_T(self) -> torch.Tensor:\n",
    "        T = torch.exp(self.log_T)\n",
    "        if self.min_T is not None or self.max_T is not None:\n",
    "            T = torch.clamp(T, self.min_T, self.max_T)\n",
    "        return T\n",
    "\n",
    "    def forward(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        T = self.get_T()\n",
    "        return logits / T\n",
    "\n",
    "\n",
    "def learn_temperature(\n",
    "    dev_logits: torch.Tensor,\n",
    "    dev_labels: torch.Tensor,\n",
    "    is_multilabel: bool,\n",
    "    f1_tolerance: float = 0.01,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Learn a temperature T using LBFGS on CPU, with safety checks:\n",
    "      - T is forced positive via log_T and clamped to [0.05, 10.0].\n",
    "      - Compute macro-F1 before and after scaling (thr=0.5 for multi-label).\n",
    "      - If calibrated F1 << base F1 (by > f1_tolerance), fall back to T=1.0.\n",
    "\n",
    "    Returns:\n",
    "        scalar float T\n",
    "    \"\"\"\n",
    "    device_cpu = torch.device(\"cpu\")\n",
    "\n",
    "    dev_logits = dev_logits.detach().to(device_cpu)\n",
    "    dev_labels = dev_labels.detach().to(device_cpu)\n",
    "\n",
    "    # ---- 1) Baseline F1 at T = 1.0 ----\n",
    "    with torch.no_grad():\n",
    "        if is_multilabel:\n",
    "            probs_base = torch.sigmoid(dev_logits)\n",
    "            preds_base = (probs_base >= 0.5).long().cpu().numpy()\n",
    "            y_true = dev_labels.cpu().numpy()\n",
    "            base_f1 = f1_score(y_true, preds_base, average=\"macro\", zero_division=0)\n",
    "        else:\n",
    "            probs_base = torch.softmax(dev_logits, dim=1)\n",
    "            preds_base = probs_base.argmax(dim=1).cpu().numpy()\n",
    "            y_true = dev_labels.cpu().numpy()\n",
    "            base_f1 = f1_score(y_true, preds_base, average=\"macro\", zero_division=0)\n",
    "\n",
    "    # ---- 2) Optimize log_T ----\n",
    "    scaler = TempScaler(init_T=1.0, min_T=0.05, max_T=10.0).to(device_cpu)\n",
    "    opt = torch.optim.LBFGS([scaler.log_T], lr=0.01, max_iter=50)\n",
    "    criterion = nn.BCEWithLogitsLoss() if is_multilabel else nn.CrossEntropyLoss()\n",
    "\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        z = scaler(dev_logits)\n",
    "        if is_multilabel:\n",
    "            loss = criterion(z, dev_labels.float())\n",
    "        else:\n",
    "            loss = criterion(z, dev_labels.long())\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    try:\n",
    "        opt.step(closure)\n",
    "    except Exception as e:\n",
    "        print(f\"[TempScale] LBFGS failed: {e}. Using T=1.0.\")\n",
    "        return 1.0\n",
    "\n",
    "    # ---- 3) Evaluate calibrated F1 ----\n",
    "    with torch.no_grad():\n",
    "        T_tensor = scaler.get_T()\n",
    "        T_value = float(T_tensor.item())\n",
    "        z_cal = dev_logits / T_tensor\n",
    "\n",
    "        if is_multilabel:\n",
    "            probs_cal = torch.sigmoid(z_cal)\n",
    "            preds_cal = (probs_cal >= 0.5).long().cpu().numpy()\n",
    "            f1_cal = f1_score(y_true, preds_cal, average=\"macro\", zero_division=0)\n",
    "        else:\n",
    "            probs_cal = torch.softmax(z_cal, dim=1)\n",
    "            preds_cal = probs_cal.argmax(dim=1).cpu().numpy()\n",
    "            f1_cal = f1_score(y_true, preds_cal, average=\"macro\", zero_division=0)\n",
    "\n",
    "    print(f\"[TempScale] base_F1={base_f1:.4f}, calibrated_F1={f1_cal:.4f}, T={T_value:.4f}\")\n",
    "\n",
    "    # ---- 4) Safety fallback ----\n",
    "    if f1_cal + 1e-4 < base_f1 - f1_tolerance:\n",
    "        print(\"[TempScale] calibrated F1 is worse than base F1; using T=1.0 instead.\")\n",
    "        return 1.0\n",
    "\n",
    "    return float(T_value)\n",
    "\n",
    "\n",
    "def collect_logits(trainer: Trainer, dataset: Dataset, is_multilabel: bool):\n",
    "    \"\"\"\n",
    "    Use Trainer.predict to collect logits + labels for given dataset.\n",
    "    \"\"\"\n",
    "    preds = trainer.predict(dataset)\n",
    "    raw = preds.predictions\n",
    "    if isinstance(raw, (list, tuple)):\n",
    "        raw = raw[0]\n",
    "    logits = torch.tensor(raw)\n",
    "    labels = torch.tensor(preds.label_ids)\n",
    "    if not is_multilabel and logits.ndim == 1:\n",
    "        logits = logits.unsqueeze(1)\n",
    "    return logits, labels\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1.1 Focal BCE loss (multi-label)\n",
    "# ============================================================\n",
    "\n",
    "class FocalBCEWithLogitsLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal BCE with class-wise alpha (pos_weight) and gamma.\n",
    "    alpha: per-class weights (like pos_weight), shape [C] or None.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha: Optional[torch.Tensor] = None, gamma: float = 2.0, reduction: str = \"mean\"):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits, targets: [B, C]\n",
    "        bce = nn.functional.binary_cross_entropy_with_logits(\n",
    "            logits, targets, reduction=\"none\", pos_weight=self.alpha\n",
    "        )  # [B,C]\n",
    "        p = torch.sigmoid(logits)\n",
    "        pt = p * targets + (1 - p) * (1 - targets)\n",
    "        focal = (1 - pt) ** self.gamma * bce\n",
    "        if self.reduction == \"mean\":\n",
    "            return focal.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal.sum()\n",
    "        return focal\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1.2 Simple K-fold helper\n",
    "# ============================================================\n",
    "\n",
    "def make_stratified_folds(y_for_strat: np.ndarray, n_splits: int, seed: int = 42):\n",
    "    \"\"\"\n",
    "    y_for_strat: 1D stratification labels (e.g., polarization or has_any_label).\n",
    "    Returns list of (train_idx, val_idx).\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    idx = np.arange(len(y_for_strat))\n",
    "    return list(skf.split(idx, y_for_strat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77bcf05",
   "metadata": {},
   "source": [
    "## Translation helpers + ensure_text_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299aac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2) TRANSLATION HELPERS (OPUS-MT) + ensure_text_en\n",
    "# ============================================================\n",
    "\n",
    "def _opus_model_for_lang(lang: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Map language code -> OPUS-MT model name (to EN).\n",
    "    Extend as needed.\n",
    "    \"\"\"\n",
    "    lang = lang.lower()\n",
    "    if lang in {\"bn\", \"ben\"}:\n",
    "        return \"Helsinki-NLP/opus-mt-bn-en\"\n",
    "    if lang in {\"pa\", \"pan\"}:\n",
    "        return \"Helsinki-NLP/opus-mt-pa-en\"\n",
    "    if lang in {\"hi\", \"hin\"}:\n",
    "        return \"Helsinki-NLP/opus-mt-hi-en\"\n",
    "    if lang in {\"ur\"}:\n",
    "        return \"Helsinki-NLP/opus-mt-ur-en\"\n",
    "    # default: no MT model\n",
    "    return None\n",
    "\n",
    "def translate_series_to_en(texts, model_name: Optional[str], batch_size: int = 16, max_len: int = 256):\n",
    "    \"\"\"\n",
    "    Translate a list/Series of sentences to English using OPUS-MT.\n",
    "    Runs on GPU if DEVICE is cuda, else CPU.\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        return [str(t) for t in texts]\n",
    "\n",
    "    device_index = 0 if DEVICE.type == \"cuda\" else -1\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    mt  = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    mt.to(DEVICE if DEVICE.type == \"cuda\" else torch.device(\"cpu\"))\n",
    "\n",
    "    pipe_trans = pipeline(\n",
    "        \"translation\",\n",
    "        model=mt,\n",
    "        tokenizer=tok,\n",
    "        device=device_index,\n",
    "    )\n",
    "\n",
    "    out = []\n",
    "    batch = []\n",
    "    for t in texts:\n",
    "        batch.append(\"\" if not isinstance(t, str) else t)\n",
    "        if len(batch) == batch_size:\n",
    "            res = pipe_trans(batch, max_length=max_len)\n",
    "            out.extend([r[\"translation_text\"] for r in res])\n",
    "            batch = []\n",
    "    if batch:\n",
    "        res = pipe_trans(batch, max_length=max_len)\n",
    "        out.extend([r[\"translation_text\"] for r in res])\n",
    "\n",
    "    # cleanup\n",
    "    del pipe_trans, mt, tok\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    return out\n",
    "\n",
    "def ensure_text_en(df: pd.DataFrame, subtask_tag: str, lang: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds `text_en` to df.\n",
    "    - If lang == \"eng\": copy text -> text_en\n",
    "    - Else: use MT (OPUS) with caching:\n",
    "        cache file: CACHE_ROOT / f\"t{subtask_tag}__{lang}__to_en.csv\"\n",
    "        columns: ['id','text_en'] if 'id' exists, else just 'text_en'\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    lang = lang.lower()\n",
    "\n",
    "    if lang == \"eng\":\n",
    "        df[\"text_en\"] = df[\"text\"].astype(str)\n",
    "        return df\n",
    "\n",
    "    cache_path = CACHE_ROOT / f\"t{subtask_tag}__{lang}__to_en.csv\"\n",
    "    if cache_path.exists():\n",
    "        cache = pd.read_csv(cache_path)\n",
    "        if \"id\" in df.columns and \"id\" in cache.columns:\n",
    "            df = df.merge(cache[[\"id\", \"text_en\"]], on=\"id\", how=\"left\")\n",
    "        else:\n",
    "            df[\"text_en\"] = cache[\"text_en\"]\n",
    "        need = df[\"text_en\"].isna() | (df[\"text_en\"].astype(str).str.len() == 0)\n",
    "        if need.any():\n",
    "            model_name = _opus_model_for_lang(lang)\n",
    "            df.loc[need, \"text_en\"] = translate_series_to_en(\n",
    "                df.loc[need, \"text\"].tolist(),\n",
    "                model_name,\n",
    "            )\n",
    "            # refresh cache\n",
    "            if \"id\" in df.columns:\n",
    "                to_save = df[[\"id\", \"text_en\"]]\n",
    "            else:\n",
    "                to_save = pd.DataFrame({\"text_en\": df[\"text_en\"]})\n",
    "            to_save.to_csv(cache_path, index=False)\n",
    "        return df\n",
    "\n",
    "    # No cache yet → translate all\n",
    "    model_name = _opus_model_for_lang(lang)\n",
    "    df[\"text_en\"] = translate_series_to_en(df[\"text\"], model_name)\n",
    "    if \"id\" in df.columns:\n",
    "        to_save = df[[\"id\", \"text_en\"]]\n",
    "    else:\n",
    "        to_save = pd.DataFrame({\"text_en\": df[\"text_en\"]})\n",
    "    to_save.to_csv(cache_path, index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a6b6f",
   "metadata": {},
   "source": [
    "## Subtask 1 (binary) DeBERTa+MT + K-fold calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3133a12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T1] TRAIN size: 3222\n",
      "[T1] DEV size (unlabeled): 160\n",
      "\n",
      "[T1] Fold 1/3 — train=2148, val=1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='807' max='807' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [807/807 00:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.614700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.525300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.433400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.406100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.318300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.327400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.240700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.322600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.333700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.269600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (argmax): 0.7891223247594739\n",
      "\n",
      "[T1] Fold 2/3 — train=2148, val=1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='807' max='807' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [807/807 00:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.671700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.513100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.479800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.354900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.379200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.380500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.354800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.270900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.266000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (argmax): 0.7737514214872951\n",
      "\n",
      "[T1] Fold 3/3 — train=2148, val=1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='807' max='807' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [807/807 00:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.675400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.585700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.536200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.442300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.433800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.430200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.373100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.321100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.366500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.251800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.321100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.290800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.197400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.146800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.142400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (argmax): 0.8004830020434701\n",
      "[TempScale] base_F1=0.7878, calibrated_F1=0.7878, T=1.3773\n",
      "\n",
      "[T1] Calibration (OOF):\n",
      "  Temperature T=1.3773\n",
      "  Best threshold=0.15\n",
      "  Macro-F1 (OOF, calibrated)=0.7925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[T1] Training FINAL DeBERTa model on full train...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1209' max='1209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1209/1209 01:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.647400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.580800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.488000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.438200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.520300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.439400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.336400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.340400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.373200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.347900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.322100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.341600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.331400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.253900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.199300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.239100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.205900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T1] Macro-F1 (TRAIN, argmax, final model): 0.9613044457770253\n",
      "[T1] Macro-F1 (TRAIN, calibrated T+thr, final model): 0.9576491464436522\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved T1 train/dev probabilities (DeBERTa) for ensembling in: cache/deberta_cv/eng\n",
      "Wrote Subtask 1 submission CSV (DeBERTa): submissions/deberta/subtask_1/pred_eng.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3) SUBTASK 1 — Polarization (binary, translate→EN + DeBERTa)\n",
    "# ============================================================\n",
    "\n",
    "# 3.1 Load TRAIN + DEV, build text_en\n",
    "t1_train_df = pd.read_csv(T1_TRAIN)\n",
    "t1_dev_df   = pd.read_csv(T1_DEV)\n",
    "\n",
    "required_train_cols_t1 = {\"id\", \"text\", \"polarization\"}\n",
    "required_dev_cols_t1   = {\"id\", \"text\"}\n",
    "assert required_train_cols_t1.issubset(t1_train_df.columns), \\\n",
    "    f\"T1 TRAIN missing: {required_train_cols_t1 - set(t1_train_df.columns)}\"\n",
    "assert required_dev_cols_t1.issubset(t1_dev_df.columns), \\\n",
    "    f\"T1 DEV missing: {required_dev_cols_t1 - set(t1_dev_df.columns)}\"\n",
    "\n",
    "t1_train_df[\"polarization\"] = t1_train_df[\"polarization\"].astype(int)\n",
    "\n",
    "t1_train_df = ensure_text_en(t1_train_df, subtask_tag=\"1\", lang=LANG)\n",
    "t1_dev_df   = ensure_text_en(t1_dev_df,   subtask_tag=\"1\", lang=LANG)\n",
    "\n",
    "print(f\"[T1] TRAIN size: {len(t1_train_df)}\")\n",
    "print(f\"[T1] DEV size (unlabeled): {len(t1_dev_df)}\")\n",
    "\n",
    "# 3.2 K-fold OOF logits for calibration\n",
    "y_t1 = t1_train_df[\"polarization\"].to_numpy()\n",
    "folds_t1 = make_stratified_folds(y_t1, n_splits=N_FOLDS, seed=SEED)\n",
    "\n",
    "oof_logits_t1 = np.zeros((len(t1_train_df), 2), dtype=np.float32)\n",
    "oof_labels_t1 = y_t1.copy()\n",
    "\n",
    "tok_t1 = AutoTokenizer.from_pretrained(EN_MODEL, use_fast=True)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(folds_t1):\n",
    "    print(f\"\\n[T1] Fold {fold+1}/{N_FOLDS} — train={len(tr_idx)}, val={len(val_idx)}\")\n",
    "\n",
    "    ds_tr = TextClsDataset(\n",
    "        texts=t1_train_df[\"text_en\"].iloc[tr_idx].tolist(),\n",
    "        labels=t1_train_df[\"polarization\"].iloc[tr_idx].tolist(),\n",
    "        tokenizer=tok_t1,\n",
    "        max_len=MAX_LEN,\n",
    "        is_multilabel=False,\n",
    "    )\n",
    "    ds_val = TextClsDataset(\n",
    "        texts=t1_train_df[\"text_en\"].iloc[val_idx].tolist(),\n",
    "        labels=t1_train_df[\"polarization\"].iloc[val_idx].tolist(),\n",
    "        tokenizer=tok_t1,\n",
    "        max_len=MAX_LEN,\n",
    "        is_multilabel=False,\n",
    "    )\n",
    "\n",
    "    cfg_t1_fold = AutoConfig.from_pretrained(EN_MODEL, num_labels=2)\n",
    "    mdl_t1_fold = AutoModelForSequenceClassification.from_pretrained(EN_MODEL, config=cfg_t1_fold)\n",
    "    mdl_t1_fold.config.use_cache = False\n",
    "    if hasattr(mdl_t1_fold, \"gradient_checkpointing_disable\"):\n",
    "        mdl_t1_fold.gradient_checkpointing_disable()\n",
    "    mdl_t1_fold.to(DEVICE)\n",
    "\n",
    "    args_t1_fold = build_training_args(\n",
    "        output_dir=ART_ROOT / f\"t1_cv_fold{fold+1}\",\n",
    "        per_device_train_batch_size=BATCH_TRAIN,\n",
    "        per_device_eval_batch_size=BATCH_EVAL,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        logging_steps=50,\n",
    "        evaluation=\"epoch\",\n",
    "        save=\"no\",\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "    )\n",
    "\n",
    "    def compute_metrics_t1_fold(eval_pred):\n",
    "        logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, (list, tuple)) else eval_pred.predictions\n",
    "        labels = eval_pred.label_ids\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        return {\"f1_macro\": macro_f1(labels, preds)}\n",
    "\n",
    "    trainer_t1_fold = Trainer(\n",
    "        model=mdl_t1_fold,\n",
    "        args=args_t1_fold,\n",
    "        train_dataset=ds_tr,\n",
    "        eval_dataset=ds_val,\n",
    "        tokenizer=tok_t1,\n",
    "        data_collator=DataCollatorWithPadding(tok_t1),\n",
    "        compute_metrics=compute_metrics_t1_fold,\n",
    "    )\n",
    "    print(\"  Trainer device:\", trainer_t1_fold.args.device)\n",
    "\n",
    "    trainer_t1_fold.train()\n",
    "    eval_fold = trainer_t1_fold.evaluate()\n",
    "    print(\"  Fold Macro-F1 (argmax):\", eval_fold.get(\"eval_f1_macro\"))\n",
    "\n",
    "    logits_val, _ = collect_logits(trainer_t1_fold, ds_val, is_multilabel=False)\n",
    "    oof_logits_t1[val_idx] = logits_val.numpy()\n",
    "\n",
    "    del trainer_t1_fold, mdl_t1_fold\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# 3.3 Calibrate on OOF logits\n",
    "logits_oof_t1 = torch.from_numpy(oof_logits_t1)\n",
    "labels_oof_t1 = torch.from_numpy(oof_labels_t1)\n",
    "\n",
    "T_t1 = learn_temperature(logits_oof_t1, labels_oof_t1, is_multilabel=False)\n",
    "probs_oof_t1 = torch.softmax(logits_oof_t1 / T_t1, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "best_thr_t1, best_f1_t1 = 0.5, -1.0\n",
    "for t in np.linspace(0.05, 0.95, 19):\n",
    "    pred = (probs_oof_t1 >= t).astype(int)\n",
    "    f = macro_f1(labels_oof_t1.numpy(), pred)\n",
    "    if f > best_f1_t1:\n",
    "        best_f1_t1, best_thr_t1 = f, t\n",
    "\n",
    "print(\"\\n[T1] Calibration (OOF):\")\n",
    "print(f\"  Temperature T={T_t1:.4f}\")\n",
    "print(f\"  Best threshold={best_thr_t1:.2f}\")\n",
    "print(f\"  Macro-F1 (OOF, calibrated)={best_f1_t1:.4f}\")\n",
    "\n",
    "# 3.4 Train FINAL model on full TRAIN (text_en)\n",
    "cfg_t1_final = AutoConfig.from_pretrained(EN_MODEL, num_labels=2)\n",
    "mdl_t1_final = AutoModelForSequenceClassification.from_pretrained(EN_MODEL, config=cfg_t1_final)\n",
    "mdl_t1_final.config.use_cache = False\n",
    "if hasattr(mdl_t1_final, \"gradient_checkpointing_disable\"):\n",
    "    mdl_t1_final.gradient_checkpointing_disable()\n",
    "mdl_t1_final.to(DEVICE)\n",
    "\n",
    "ds_t1_train_full = TextClsDataset(\n",
    "    texts=t1_train_df[\"text_en\"].tolist(),\n",
    "    labels=t1_train_df[\"polarization\"].tolist(),\n",
    "    tokenizer=tok_t1,\n",
    "    max_len=MAX_LEN,\n",
    "    is_multilabel=False,\n",
    ")\n",
    "ds_t1_dev_full = TextClsDataset(\n",
    "    texts=t1_dev_df[\"text_en\"].tolist(),\n",
    "    labels=[0] * len(t1_dev_df),\n",
    "    tokenizer=tok_t1,\n",
    "    max_len=MAX_LEN,\n",
    "    is_multilabel=False,\n",
    ")\n",
    "\n",
    "args_t1_final = build_training_args(\n",
    "    output_dir=ART_ROOT / \"t1_final\",\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=50,\n",
    "    evaluation=\"epoch\",\n",
    "    save=\"no\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    ")\n",
    "\n",
    "def compute_metrics_t1_final(eval_pred):\n",
    "    logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, (list, tuple)) else eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\"f1_macro\": macro_f1(labels, preds)}\n",
    "\n",
    "trainer_t1_final = Trainer(\n",
    "    model=mdl_t1_final,\n",
    "    args=args_t1_final,\n",
    "    train_dataset=ds_t1_train_full,\n",
    "    eval_dataset=ds_t1_train_full,\n",
    "    tokenizer=tok_t1,\n",
    "    data_collator=DataCollatorWithPadding(tok_t1),\n",
    "    compute_metrics=compute_metrics_t1_final,\n",
    ")\n",
    "\n",
    "print(\"\\n[T1] Training FINAL DeBERTa model on full train...\")\n",
    "trainer_t1_final.train()\n",
    "eval_t1_train_full = trainer_t1_final.evaluate()\n",
    "print(\"[T1] Macro-F1 (TRAIN, argmax, final model):\", eval_t1_train_full.get(\"eval_f1_macro\"))\n",
    "\n",
    "# Calibrated train F1 for final model\n",
    "logits_t1_train_full, labels_t1_train_full = collect_logits(trainer_t1_final, ds_t1_train_full, is_multilabel=False)\n",
    "probs_t1_train_full = torch.softmax(logits_t1_train_full / T_t1, dim=1)[:, 1].cpu().numpy()\n",
    "pred_t1_train_full  = (probs_t1_train_full >= best_thr_t1).astype(int)\n",
    "print(\"[T1] Macro-F1 (TRAIN, calibrated T+thr, final model):\",\n",
    "      macro_f1(labels_t1_train_full.numpy(), pred_t1_train_full))\n",
    "\n",
    "# 3.5 Inference on DEV\n",
    "preds_dev_t1 = trainer_t1_final.predict(ds_t1_dev_full)\n",
    "logits_t1_dev = torch.tensor(\n",
    "    preds_dev_t1.predictions\n",
    "    if not isinstance(preds_dev_t1.predictions, (list, tuple))\n",
    "    else preds_dev_t1.predictions[0]\n",
    ")\n",
    "probs_t1_dev = torch.softmax(logits_t1_dev / T_t1, dim=1)[:, 1].cpu().numpy()\n",
    "pred_t1_dev = (probs_t1_dev >= best_thr_t1).astype(int)\n",
    "\n",
    "# 3.6 Cache train/dev probs for ensembling\n",
    "cache_t1_train = pd.DataFrame({\n",
    "    \"id\":   t1_train_df[\"id\"].astype(str),\n",
    "    \"prob_pos\": probs_t1_train_full,\n",
    "    \"label\":   t1_train_df[\"polarization\"].astype(int),\n",
    "})\n",
    "cache_t1_train.to_csv(CACHE_ROOT / \"t1_train_probs.csv\", index=False)\n",
    "\n",
    "cache_t1_dev = pd.DataFrame({\n",
    "    \"id\":      t1_dev_df[\"id\"].astype(str),\n",
    "    \"prob_pos\": probs_t1_dev,\n",
    "})\n",
    "cache_t1_dev.to_csv(CACHE_ROOT / \"t1_dev_probs.csv\", index=False)\n",
    "\n",
    "print(\"Saved T1 train/dev probabilities (DeBERTa) for ensembling in:\", CACHE_ROOT)\n",
    "\n",
    "# 3.7 Save model + calibration\n",
    "mdl_t1_final.save_pretrained(ART_ROOT / \"native_t1\")\n",
    "tok_t1.save_pretrained(ART_ROOT / \"native_t1\")\n",
    "with open(ART_ROOT / \"calib_t1_native.json\", \"w\") as f:\n",
    "    json.dump({\"temperature\": float(T_t1), \"threshold\": float(best_thr_t1)}, f, indent=2)\n",
    "\n",
    "# 3.8 Codabench submission CSV\n",
    "sub1 = pd.DataFrame({\n",
    "    \"id\": t1_dev_df[\"id\"].astype(str),\n",
    "    \"polarization\": pred_t1_dev.astype(int),\n",
    "})\n",
    "sub1_path = SUB_ROOT / \"subtask_1\" / f\"pred_{lang_fname}.csv\"\n",
    "sub1.to_csv(sub1_path, index=False)\n",
    "print(\"Wrote Subtask 1 submission CSV (DeBERTa):\", sub1_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ea57e",
   "metadata": {},
   "source": [
    "## Subtask 2 (multi-label 5) DeBERTa+MT + focal + K-fold calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7110dc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T2] TRAIN size: 3222\n",
      "[T2] DEV size (unlabeled): 160\n",
      "\n",
      "[T2] Fold 1/3 — train=2148, val=1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='807' max='807' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [807/807 00:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.440300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.502800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.531200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.565100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.405700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.472500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.478700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.435700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.333900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.334800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.297800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.259500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.233400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (thr=0.5): 0.34754764217307843\n",
      "\n",
      "[T2] Fold 2/3 — train=2148, val=1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='807' max='807' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [807/807 00:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.494500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.463100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.508900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.608500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.395200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.459200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.437400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.373400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.328700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.456200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.278800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.298600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.225700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (thr=0.5): 0.3395533929990189\n",
      "\n",
      "[T2] Fold 3/3 — train=2148, val=1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='807' max='807' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [807/807 00:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.467700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.497300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.579200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.461700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.471100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.540800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.416900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.541600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.523500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.388200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.454300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.440300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.360300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.434300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.354800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (thr=0.5): 0.3010290604212571\n",
      "[TempScale] base_F1=0.3336, calibrated_F1=0.3336, T=0.7884\n",
      "\n",
      "[T2] Calibration (OOF):\n",
      "  Temperature: 0.7884389758110046\n",
      "  Thresholds: {'gender/sexual': 0.25, 'political': 0.44999999999999996, 'religious': 0.75, 'racial/ethnic': 0.35, 'other': 0.5499999999999999}\n",
      "  Macro-F1 (OOF, calibrated): 0.3626535698904044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[T2] Training FINAL DeBERTa model on full train...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1209' max='1209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1209/1209 01:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.436600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.455900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.410700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.516600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.516500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.476900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.545500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.469800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.471500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.420400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.315300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.468200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.438000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.380900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.304400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.261800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.288400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.386400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.285300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.309200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.281200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.317900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.221100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T2] Macro-F1 (TRAIN, thr=0.5, final model): 0.48405134377736986\n",
      "[T2] Macro-F1 (TRAIN, calibrated T+thr, final model): 0.4876222168787071\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved T2 train/dev probabilities (DeBERTa) for ensembling in: cache/deberta_cv/eng\n",
      "Wrote Subtask 2 submission CSV (DeBERTa): submissions/deberta/subtask_2/pred_eng.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4) SUBTASK 2 — Hate type (5 labels, translate→EN + DeBERTa)\n",
    "# ============================================================\n",
    "\n",
    "# 4.1 Load TRAIN + DEV, build text_en\n",
    "t2_train_df = pd.read_csv(T2_TRAIN)\n",
    "t2_dev_df   = pd.read_csv(T2_DEV)\n",
    "\n",
    "required_train_cols_t2 = {\"id\", \"text\", *T2_LABELS}\n",
    "required_dev_cols_t2   = {\"id\", \"text\"}\n",
    "assert required_train_cols_t2.issubset(t2_train_df.columns), \\\n",
    "    f\"T2 TRAIN missing: {required_train_cols_t2 - set(t2_train_df.columns)}\"\n",
    "assert required_dev_cols_t2.issubset(t2_dev_df.columns), \\\n",
    "    f\"T2 DEV missing: {required_dev_cols_t2 - set(t2_dev_df.columns)}\"\n",
    "\n",
    "t2_train_df = ensure_text_en(t2_train_df, subtask_tag=\"2\", lang=LANG)\n",
    "t2_dev_df   = ensure_text_en(t2_dev_df,   subtask_tag=\"2\", lang=LANG)\n",
    "\n",
    "Y2_train = t2_train_df[T2_LABELS].values.astype(int)\n",
    "\n",
    "print(f\"[T2] TRAIN size: {len(t2_train_df)}\")\n",
    "print(f\"[T2] DEV size (unlabeled): {len(t2_dev_df)}\")\n",
    "\n",
    "# For stratification: any positive label vs none\n",
    "y2_strat = (Y2_train.sum(axis=1) > 0).astype(int)\n",
    "folds_t2 = make_stratified_folds(y2_strat, n_splits=N_FOLDS, seed=SEED)\n",
    "\n",
    "tok_t2 = AutoTokenizer.from_pretrained(EN_MODEL, use_fast=True)\n",
    "\n",
    "# pos_weight on full train\n",
    "pos_count_2 = Y2_train.sum(axis=0) + 1e-6\n",
    "neg_count_2 = Y2_train.shape[0] - pos_count_2\n",
    "pos_weight_2 = torch.tensor(neg_count_2 / pos_count_2, dtype=torch.float)\n",
    "\n",
    "oof_logits_t2 = np.zeros((len(t2_train_df), len(T2_LABELS)), dtype=np.float32)\n",
    "oof_labels_t2 = Y2_train.copy()\n",
    "\n",
    "class FocalTrainerT2(Trainer):\n",
    "    def __init__(self, *args, pos_weight=None, gamma=1.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pos_weight = pos_weight\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = FocalBCEWithLogitsLoss(\n",
    "            alpha=self.pos_weight.to(logits.device),\n",
    "            gamma=self.gamma,\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "        loss = loss_fct(logits, labels.to(logits.device).float())\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(folds_t2):\n",
    "    print(f\"\\n[T2] Fold {fold+1}/{N_FOLDS} — train={len(tr_idx)}, val={len(val_idx)}\")\n",
    "\n",
    "    ds_tr = TextClsDataset(\n",
    "        texts=t2_train_df[\"text_en\"].iloc[tr_idx].tolist(),\n",
    "        labels=Y2_train[tr_idx].tolist(),\n",
    "        tokenizer=tok_t2,\n",
    "        max_len=MAX_LEN,\n",
    "        is_multilabel=True,\n",
    "    )\n",
    "    ds_val = TextClsDataset(\n",
    "        texts=t2_train_df[\"text_en\"].iloc[val_idx].tolist(),\n",
    "        labels=Y2_train[val_idx].tolist(),\n",
    "        tokenizer=tok_t2,\n",
    "        max_len=MAX_LEN,\n",
    "        is_multilabel=True,\n",
    "    )\n",
    "\n",
    "    cfg_t2_fold = AutoConfig.from_pretrained(\n",
    "        EN_MODEL,\n",
    "        num_labels=len(T2_LABELS),\n",
    "        problem_type=\"multi_label_classification\",\n",
    "    )\n",
    "    mdl_t2_fold = AutoModelForSequenceClassification.from_pretrained(EN_MODEL, config=cfg_t2_fold)\n",
    "    mdl_t2_fold.config.use_cache = False\n",
    "    if hasattr(mdl_t2_fold, \"gradient_checkpointing_disable\"):\n",
    "        mdl_t2_fold.gradient_checkpointing_disable()\n",
    "    mdl_t2_fold.to(DEVICE)\n",
    "\n",
    "    args_t2_fold = build_training_args(\n",
    "        output_dir=ART_ROOT / f\"t2_cv_fold{fold+1}\",\n",
    "        per_device_train_batch_size=BATCH_TRAIN,\n",
    "        per_device_eval_batch_size=BATCH_EVAL,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        logging_steps=50,\n",
    "        evaluation=\"epoch\",\n",
    "        save=\"no\",\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "    )\n",
    "\n",
    "    def compute_metrics_t2_fold(eval_pred):\n",
    "        logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, (list, tuple)) else eval_pred.predictions\n",
    "        labels = eval_pred.label_ids\n",
    "        probs  = 1.0 / (1.0 + np.exp(-logits))\n",
    "        preds  = (probs >= 0.5).astype(int)\n",
    "        return {\"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0)}\n",
    "\n",
    "    trainer_t2_fold = FocalTrainerT2(\n",
    "        model=mdl_t2_fold,\n",
    "        args=args_t2_fold,\n",
    "        train_dataset=ds_tr,\n",
    "        eval_dataset=ds_val,\n",
    "        tokenizer=tok_t2,\n",
    "        data_collator=DataCollatorWithPadding(tok_t2),\n",
    "        compute_metrics=compute_metrics_t2_fold,\n",
    "        pos_weight=pos_weight_2,\n",
    "        gamma=1.5,\n",
    "    )\n",
    "    print(\"  Trainer device:\", trainer_t2_fold.args.device)\n",
    "\n",
    "    trainer_t2_fold.train()\n",
    "    eval_fold = trainer_t2_fold.evaluate()\n",
    "    print(\"  Fold Macro-F1 (thr=0.5):\", eval_fold.get(\"eval_f1_macro\"))\n",
    "\n",
    "    logits_val, _ = collect_logits(trainer_t2_fold, ds_val, is_multilabel=True)\n",
    "    oof_logits_t2[val_idx] = logits_val.numpy()\n",
    "\n",
    "    del trainer_t2_fold, mdl_t2_fold\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# 4.2 Calibrate on OOF logits\n",
    "logits_oof_t2 = torch.from_numpy(oof_logits_t2)\n",
    "labels_oof_t2 = torch.from_numpy(oof_labels_t2)\n",
    "\n",
    "T_t2 = learn_temperature(logits_oof_t2, labels_oof_t2, is_multilabel=True)\n",
    "probs_oof_t2 = torch.sigmoid(logits_oof_t2 / T_t2).cpu().numpy()\n",
    "thr_map_t2 = grid_search_thresholds(labels_oof_t2.numpy(), probs_oof_t2, T2_LABELS)\n",
    "\n",
    "P2_oof = np.zeros_like(probs_oof_t2, dtype=int)\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    thr = float(thr_map_t2[lab])\n",
    "    P2_oof[:, j] = (probs_oof_t2[:, j] >= thr).astype(int)\n",
    "f1_oof_t2 = f1_score(labels_oof_t2.numpy(), P2_oof, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"\\n[T2] Calibration (OOF):\")\n",
    "print(\"  Temperature:\", T_t2)\n",
    "print(\"  Thresholds:\", thr_map_t2)\n",
    "print(\"  Macro-F1 (OOF, calibrated):\", f1_oof_t2)\n",
    "\n",
    "# 4.3 Train FINAL model on full TRAIN (text_en)\n",
    "cfg_t2_final = AutoConfig.from_pretrained(\n",
    "    EN_MODEL,\n",
    "    num_labels=len(T2_LABELS),\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")\n",
    "mdl_t2_final = AutoModelForSequenceClassification.from_pretrained(EN_MODEL, config=cfg_t2_final)\n",
    "mdl_t2_final.config.use_cache = False\n",
    "if hasattr(mdl_t2_final, \"gradient_checkpointing_disable\"):\n",
    "    mdl_t2_final.gradient_checkpointing_disable()\n",
    "mdl_t2_final.to(DEVICE)\n",
    "\n",
    "ds_t2_train_full = TextClsDataset(\n",
    "    texts=t2_train_df[\"text_en\"].tolist(),\n",
    "    labels=Y2_train.tolist(),\n",
    "    tokenizer=tok_t2,\n",
    "    max_len=MAX_LEN,\n",
    "    is_multilabel=True,\n",
    ")\n",
    "dummy_labels_t2_dev = np.zeros((len(t2_dev_df), len(T2_LABELS)), dtype=int)\n",
    "ds_t2_dev_full = TextClsDataset(\n",
    "    texts=t2_dev_df[\"text_en\"].tolist(),\n",
    "    labels=dummy_labels_t2_dev.tolist(),\n",
    "    tokenizer=tok_t2,\n",
    "    max_len=MAX_LEN,\n",
    "    is_multilabel=True,\n",
    ")\n",
    "\n",
    "args_t2_final = build_training_args(\n",
    "    output_dir=ART_ROOT / \"t2_final\",\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=50,\n",
    "    evaluation=\"epoch\",\n",
    "    save=\"no\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    ")\n",
    "\n",
    "def compute_metrics_t2_final(eval_pred):\n",
    "    logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, (list, tuple)) else eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    probs  = 1.0 / (1.0 + np.exp(-logits))\n",
    "    preds  = (probs >= 0.5).astype(int)\n",
    "    return {\"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0)}\n",
    "\n",
    "trainer_t2_final = FocalTrainerT2(\n",
    "    model=mdl_t2_final,\n",
    "    args=args_t2_final,\n",
    "    train_dataset=ds_t2_train_full,\n",
    "    eval_dataset=ds_t2_train_full,\n",
    "    tokenizer=tok_t2,\n",
    "    data_collator=DataCollatorWithPadding(tok_t2),\n",
    "    compute_metrics=compute_metrics_t2_final,\n",
    "    pos_weight=pos_weight_2,\n",
    "    gamma=1.5,\n",
    ")\n",
    "\n",
    "print(\"\\n[T2] Training FINAL DeBERTa model on full train...\")\n",
    "trainer_t2_final.train()\n",
    "eval_t2_train_full = trainer_t2_final.evaluate()\n",
    "print(\"[T2] Macro-F1 (TRAIN, thr=0.5, final model):\", eval_t2_train_full.get(\"eval_f1_macro\"))\n",
    "\n",
    "# calibrated train F1\n",
    "logits_t2_train_full, labels_t2_train_full = collect_logits(trainer_t2_final, ds_t2_train_full, is_multilabel=True)\n",
    "probs_t2_train_full = torch.sigmoid(logits_t2_train_full / T_t2).cpu().numpy()\n",
    "P2_train_full = np.zeros_like(probs_t2_train_full, dtype=int)\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    thr = float(thr_map_t2[lab])\n",
    "    P2_train_full[:, j] = (probs_t2_train_full[:, j] >= thr).astype(int)\n",
    "train_f1_calib_t2 = f1_score(labels_t2_train_full.numpy(), P2_train_full, average=\"macro\", zero_division=0)\n",
    "print(\"[T2] Macro-F1 (TRAIN, calibrated T+thr, final model):\", train_f1_calib_t2)\n",
    "\n",
    "# 4.4 Inference on DEV\n",
    "preds_dev_t2 = trainer_t2_final.predict(ds_t2_dev_full)\n",
    "logits_t2_dev = torch.tensor(\n",
    "    preds_dev_t2.predictions\n",
    "    if not isinstance(preds_dev_t2.predictions, (list, tuple))\n",
    "    else preds_dev_t2.predictions[0]\n",
    ")\n",
    "probs_t2_dev = torch.sigmoid(logits_t2_dev / T_t2).cpu().numpy()\n",
    "\n",
    "P2_dev = np.zeros_like(probs_t2_dev, dtype=int)\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    thr = float(thr_map_t2[lab])\n",
    "    P2_dev[:, j] = (probs_t2_dev[:, j] >= thr).astype(int)\n",
    "\n",
    "# 4.5 Cache train/dev probs for ensembling\n",
    "cache_cols_train_t2 = {\"id\": t2_train_df[\"id\"].astype(str).values}\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    cache_cols_train_t2[f\"prob_{lab}\"]  = probs_t2_train_full[:, j]\n",
    "    cache_cols_train_t2[f\"label_{lab}\"] = labels_t2_train_full.numpy()[:, j]\n",
    "\n",
    "t2_train_cache = pd.DataFrame(cache_cols_train_t2)\n",
    "t2_train_cache.to_csv(CACHE_ROOT / \"t2_train_probs.csv\", index=False)\n",
    "\n",
    "cache_cols_dev_t2 = {\"id\": t2_dev_df[\"id\"].astype(str).values}\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    cache_cols_dev_t2[f\"prob_{lab}\"] = probs_t2_dev[:, j]\n",
    "t2_dev_cache = pd.DataFrame(cache_cols_dev_t2)\n",
    "t2_dev_cache.to_csv(CACHE_ROOT / \"t2_dev_probs.csv\", index=False)\n",
    "\n",
    "print(\"Saved T2 train/dev probabilities (DeBERTa) for ensembling in:\", CACHE_ROOT)\n",
    "\n",
    "# 4.6 Save model + calibration\n",
    "mdl_t2_final.save_pretrained(ART_ROOT / \"native_t2\")\n",
    "tok_t2.save_pretrained(ART_ROOT / \"native_t2\")\n",
    "with open(ART_ROOT / \"calib_t2_native.json\", \"w\") as f:\n",
    "    json.dump({\"temperature\": float(T_t2), \"thresholds\": thr_map_t2}, f, indent=2)\n",
    "\n",
    "# 4.7 Codabench submission CSV (required header order)\n",
    "idx_gender    = T2_LABELS.index(\"gender/sexual\")\n",
    "idx_political = T2_LABELS.index(\"political\")\n",
    "idx_religious = T2_LABELS.index(\"religious\")\n",
    "idx_racial    = T2_LABELS.index(\"racial/ethnic\")\n",
    "idx_other     = T2_LABELS.index(\"other\")\n",
    "\n",
    "sub2 = pd.DataFrame({\n",
    "    \"id\":            t2_dev_df[\"id\"].astype(str).values,\n",
    "    \"political\":     P2_dev[:, idx_political],\n",
    "    \"racial/ethnic\": P2_dev[:, idx_racial],\n",
    "    \"religious\":     P2_dev[:, idx_religious],\n",
    "    \"gender/sexual\": P2_dev[:, idx_gender],\n",
    "    \"other\":         P2_dev[:, idx_other],\n",
    "})\n",
    "sub2_path = SUB_ROOT / \"subtask_2\" / f\"pred_{lang_fname}.csv\"\n",
    "sub2.to_csv(sub2_path, index=False)\n",
    "print(\"Wrote Subtask 2 submission CSV (DeBERTa):\", sub2_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f22f773",
   "metadata": {},
   "source": [
    "## Subtask 3 (multi-label 6) DeBERTa+MT + focal + K-fold calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2653526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T3] TRAIN size: 3222\n",
      "[T3] DEV size (unlabeled): 160\n",
      "\n",
      "[T3] Fold 1/3 — train=2148, val=1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='807' max='807' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [807/807 00:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.403000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.366200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.313700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.324400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.323100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.285300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.277800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.279900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.288900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.244200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (thr=0.5): 0.494572838367564\n",
      "\n",
      "[T3] Fold 2/3 — train=2148, val=1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='807' max='807' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [807/807 00:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.427500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.362900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.355900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.319400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.309100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.303600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.317300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.278600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.281500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.252100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.268500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.233600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (thr=0.5): 0.48446004150646144\n",
      "\n",
      "[T3] Fold 3/3 — train=2148, val=1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='807' max='807' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [807/807 00:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.413800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.375800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.330100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.315400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.323200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.309600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.326000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.289100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.269300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.273700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.232100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.241300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.230400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (thr=0.5): 0.4882677408629481\n",
      "[TempScale] base_F1=0.4894, calibrated_F1=0.4894, T=0.8889\n",
      "\n",
      "[T3] Calibration (OOF):\n",
      "  Temperature: 0.8888956904411316\n",
      "  Thresholds: {'vilification': 0.44999999999999996, 'extreme_language': 0.44999999999999996, 'stereotype': 0.5499999999999999, 'invalidation': 0.39999999999999997, 'lack_of_empathy': 0.49999999999999994, 'dehumanization': 0.65}\n",
      "  Macro-F1 (OOF, calibrated): 0.49501524716667394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[T3] Training FINAL DeBERTa model on full train...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1209' max='1209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1209/1209 01:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.393700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.410100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.370200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.340700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.367000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.336300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.362600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.364200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.292600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.279100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.294200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.317800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.288000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.303100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.235900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.284400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.229500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.248600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.219400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.314500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.202400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T3] Macro-F1 (TRAIN, thr=0.5, final model): 0.6164551110137816\n",
      "[T3] Macro-F1 (TRAIN, calibrated T+thr, final model): 0.6261228098252071\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved T3 train/dev probabilities (DeBERTa) for ensembling in: cache/deberta_cv/eng\n",
      "Wrote Subtask 3 submission CSV (DeBERTa): submissions/deberta/subtask_3/pred_eng.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5) SUBTASK 3 — Manifestation (6 labels, translate→EN + DeBERTa)\n",
    "# ============================================================\n",
    "\n",
    "# 5.1 Load TRAIN + DEV, build text_en\n",
    "t3_train_df = pd.read_csv(T3_TRAIN)\n",
    "t3_dev_df   = pd.read_csv(T3_DEV)\n",
    "\n",
    "required_train_cols_t3 = {\"id\", \"text\", *T3_LABELS}\n",
    "required_dev_cols_t3   = {\"id\", \"text\"}\n",
    "assert required_train_cols_t3.issubset(t3_train_df.columns), \\\n",
    "    f\"T3 TRAIN missing: {required_train_cols_t3 - set(t3_train_df.columns)}\"\n",
    "assert required_dev_cols_t3.issubset(t3_dev_df.columns), \\\n",
    "    f\"T3 DEV missing: {required_dev_cols_t3 - set(t3_dev_df.columns)}\"\n",
    "\n",
    "t3_train_df = ensure_text_en(t3_train_df, subtask_tag=\"3\", lang=LANG)\n",
    "t3_dev_df   = ensure_text_en(t3_dev_df,   subtask_tag=\"3\", lang=LANG)\n",
    "\n",
    "Y3_train = t3_train_df[T3_LABELS].values.astype(int)\n",
    "\n",
    "print(f\"[T3] TRAIN size: {len(t3_train_df)}\")\n",
    "print(f\"[T3] DEV size (unlabeled): {len(t3_dev_df)}\")\n",
    "\n",
    "# stratification: any manifestation vs none\n",
    "y3_strat = (Y3_train.sum(axis=1) > 0).astype(int)\n",
    "folds_t3 = make_stratified_folds(y3_strat, n_splits=N_FOLDS, seed=SEED)\n",
    "\n",
    "tok_t3 = AutoTokenizer.from_pretrained(EN_MODEL, use_fast=True)\n",
    "\n",
    "pos_count_3 = Y3_train.sum(axis=0) + 1e-6\n",
    "neg_count_3 = Y3_train.shape[0] - pos_count_3\n",
    "pos_weight_3 = torch.tensor(neg_count_3 / pos_count_3, dtype=torch.float)\n",
    "\n",
    "oof_logits_t3 = np.zeros((len(t3_train_df), len(T3_LABELS)), dtype=np.float32)\n",
    "oof_labels_t3 = Y3_train.copy()\n",
    "\n",
    "class FocalTrainerT3(Trainer):\n",
    "    def __init__(self, *args, pos_weight=None, gamma=1.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pos_weight = pos_weight\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = FocalBCEWithLogitsLoss(\n",
    "            alpha=self.pos_weight.to(logits.device),\n",
    "            gamma=self.gamma,\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "        loss = loss_fct(logits, labels.to(logits.device).float())\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(folds_t3):\n",
    "    print(f\"\\n[T3] Fold {fold+1}/{N_FOLDS} — train={len(tr_idx)}, val={len(val_idx)}\")\n",
    "\n",
    "    ds_tr = TextClsDataset(\n",
    "        texts=t3_train_df[\"text_en\"].iloc[tr_idx].tolist(),\n",
    "        labels=Y3_train[tr_idx].tolist(),\n",
    "        tokenizer=tok_t3,\n",
    "        max_len=MAX_LEN,\n",
    "        is_multilabel=True,\n",
    "    )\n",
    "    ds_val = TextClsDataset(\n",
    "        texts=t3_train_df[\"text_en\"].iloc[val_idx].tolist(),\n",
    "        labels=Y3_train[val_idx].tolist(),\n",
    "        tokenizer=tok_t3,\n",
    "        max_len=MAX_LEN,\n",
    "        is_multilabel=True,\n",
    "    )\n",
    "\n",
    "    cfg_t3_fold = AutoConfig.from_pretrained(\n",
    "        EN_MODEL,\n",
    "        num_labels=len(T3_LABELS),\n",
    "        problem_type=\"multi_label_classification\",\n",
    "    )\n",
    "    mdl_t3_fold = AutoModelForSequenceClassification.from_pretrained(EN_MODEL, config=cfg_t3_fold)\n",
    "    mdl_t3_fold.config.use_cache = False\n",
    "    if hasattr(mdl_t3_fold, \"gradient_checkpointing_disable\"):\n",
    "        mdl_t3_fold.gradient_checkpointing_disable()\n",
    "    mdl_t3_fold.to(DEVICE)\n",
    "\n",
    "    args_t3_fold = build_training_args(\n",
    "        output_dir=ART_ROOT / f\"t3_cv_fold{fold+1}\",\n",
    "        per_device_train_batch_size=BATCH_TRAIN,\n",
    "        per_device_eval_batch_size=BATCH_EVAL,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        logging_steps=50,\n",
    "        evaluation=\"epoch\",\n",
    "        save=\"no\",\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "    )\n",
    "\n",
    "    def compute_metrics_t3_fold(eval_pred):\n",
    "        logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, (list, tuple)) else eval_pred.predictions\n",
    "        labels = eval_pred.label_ids\n",
    "        probs  = 1.0 / (1.0 + np.exp(-logits))\n",
    "        preds  = (probs >= 0.5).astype(int)\n",
    "        return {\"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0)}\n",
    "\n",
    "    trainer_t3_fold = FocalTrainerT3(\n",
    "        model=mdl_t3_fold,\n",
    "        args=args_t3_fold,\n",
    "        train_dataset=ds_tr,\n",
    "        eval_dataset=ds_val,\n",
    "        tokenizer=tok_t3,\n",
    "        data_collator=DataCollatorWithPadding(tok_t3),\n",
    "        compute_metrics=compute_metrics_t3_fold,\n",
    "        pos_weight=pos_weight_3,\n",
    "        gamma=1.5,\n",
    "    )\n",
    "    print(\"  Trainer device:\", trainer_t3_fold.args.device)\n",
    "\n",
    "    trainer_t3_fold.train()\n",
    "    eval_fold = trainer_t3_fold.evaluate()\n",
    "    print(\"  Fold Macro-F1 (thr=0.5):\", eval_fold.get(\"eval_f1_macro\"))\n",
    "\n",
    "    logits_val, _ = collect_logits(trainer_t3_fold, ds_val, is_multilabel=True)\n",
    "    oof_logits_t3[val_idx] = logits_val.numpy()\n",
    "\n",
    "    del trainer_t3_fold, mdl_t3_fold\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# 5.2 Calibrate on OOF logits\n",
    "logits_oof_t3 = torch.from_numpy(oof_logits_t3)\n",
    "labels_oof_t3 = torch.from_numpy(oof_labels_t3)\n",
    "\n",
    "T_t3 = learn_temperature(logits_oof_t3, labels_oof_t3, is_multilabel=True)\n",
    "probs_oof_t3 = torch.sigmoid(logits_oof_t3 / T_t3).cpu().numpy()\n",
    "thr_map_t3 = grid_search_thresholds(labels_oof_t3.numpy(), probs_oof_t3, T3_LABELS)\n",
    "\n",
    "P3_oof = np.zeros_like(probs_oof_t3, dtype=int)\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    thr = float(thr_map_t3[lab])\n",
    "    P3_oof[:, j] = (probs_oof_t3[:, j] >= thr).astype(int)\n",
    "f1_oof_t3 = f1_score(labels_oof_t3.numpy(), P3_oof, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"\\n[T3] Calibration (OOF):\")\n",
    "print(\"  Temperature:\", T_t3)\n",
    "print(\"  Thresholds:\", thr_map_t3)\n",
    "print(\"  Macro-F1 (OOF, calibrated):\", f1_oof_t3)\n",
    "\n",
    "# 5.3 Train FINAL model on full TRAIN (text_en)\n",
    "cfg_t3_final = AutoConfig.from_pretrained(\n",
    "    EN_MODEL,\n",
    "    num_labels=len(T3_LABELS),\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")\n",
    "mdl_t3_final = AutoModelForSequenceClassification.from_pretrained(EN_MODEL, config=cfg_t3_final)\n",
    "mdl_t3_final.config.use_cache = False\n",
    "if hasattr(mdl_t3_final, \"gradient_checkpointing_disable\"):\n",
    "    mdl_t3_final.gradient_checkpointing_disable()\n",
    "mdl_t3_final.to(DEVICE)\n",
    "\n",
    "ds_t3_train_full = TextClsDataset(\n",
    "    texts=t3_train_df[\"text_en\"].tolist(),\n",
    "    labels=Y3_train.tolist(),\n",
    "    tokenizer=tok_t3,\n",
    "    max_len=MAX_LEN,\n",
    "    is_multilabel=True,\n",
    ")\n",
    "dummy_labels_t3_dev = np.zeros((len(t3_dev_df), len(T3_LABELS)), dtype=int)\n",
    "ds_t3_dev_full = TextClsDataset(\n",
    "    texts=t3_dev_df[\"text_en\"].tolist(),\n",
    "    labels=dummy_labels_t3_dev.tolist(),\n",
    "    tokenizer=tok_t3,\n",
    "    max_len=MAX_LEN,\n",
    "    is_multilabel=True,\n",
    ")\n",
    "\n",
    "args_t3_final = build_training_args(\n",
    "    output_dir=ART_ROOT / \"t3_final\",\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=50,\n",
    "    evaluation=\"epoch\",\n",
    "    save=\"no\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    ")\n",
    "\n",
    "def compute_metrics_t3_final(eval_pred):\n",
    "    logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, (list, tuple)) else eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    probs  = 1.0 / (1.0 + np.exp(-logits))\n",
    "    preds  = (probs >= 0.5).astype(int)\n",
    "    return {\"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0)}\n",
    "\n",
    "trainer_t3_final = FocalTrainerT3(\n",
    "    model=mdl_t3_final,\n",
    "    args=args_t3_final,\n",
    "    train_dataset=ds_t3_train_full,\n",
    "    eval_dataset=ds_t3_train_full,\n",
    "    tokenizer=tok_t3,\n",
    "    data_collator=DataCollatorWithPadding(tok_t3),\n",
    "    compute_metrics=compute_metrics_t3_final,\n",
    "    pos_weight=pos_weight_3,\n",
    "    gamma=1.5,\n",
    ")\n",
    "\n",
    "print(\"\\n[T3] Training FINAL DeBERTa model on full train...\")\n",
    "trainer_t3_final.train()\n",
    "eval_t3_train_full = trainer_t3_final.evaluate()\n",
    "print(\"[T3] Macro-F1 (TRAIN, thr=0.5, final model):\", eval_t3_train_full.get(\"eval_f1_macro\"))\n",
    "\n",
    "# calibrated train F1\n",
    "logits_t3_train_full, labels_t3_train_full = collect_logits(trainer_t3_final, ds_t3_train_full, is_multilabel=True)\n",
    "probs_t3_train_full = torch.sigmoid(logits_t3_train_full / T_t3).cpu().numpy()\n",
    "P3_train_full = np.zeros_like(probs_t3_train_full, dtype=int)\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    thr = float(thr_map_t3[lab])\n",
    "    P3_train_full[:, j] = (probs_t3_train_full[:, j] >= thr).astype(int)\n",
    "train_f1_calib_t3 = f1_score(labels_t3_train_full.numpy(), P3_train_full, average=\"macro\", zero_division=0)\n",
    "print(\"[T3] Macro-F1 (TRAIN, calibrated T+thr, final model):\", train_f1_calib_t3)\n",
    "\n",
    "# 5.4 Inference on DEV\n",
    "preds_dev_t3 = trainer_t3_final.predict(ds_t3_dev_full)\n",
    "logits_t3_dev = torch.tensor(\n",
    "    preds_dev_t3.predictions\n",
    "    if not isinstance(preds_dev_t3.predictions, (list, tuple))\n",
    "    else preds_dev_t3.predictions[0]\n",
    ")\n",
    "probs_t3_dev = torch.sigmoid(logits_t3_dev / T_t3).cpu().numpy()\n",
    "\n",
    "P3_dev = np.zeros_like(probs_t3_dev, dtype=int)\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    thr = float(thr_map_t3[lab])\n",
    "    P3_dev[:, j] = (probs_t3_dev[:, j] >= thr).astype(int)\n",
    "\n",
    "# 5.5 Cache train/dev probs for ensembling\n",
    "cache_cols_train_t3 = {\"id\": t3_train_df[\"id\"].astype(str).values}\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    cache_cols_train_t3[f\"prob_{lab}\"]  = probs_t3_train_full[:, j]\n",
    "    cache_cols_train_t3[f\"label_{lab}\"] = labels_t3_train_full.numpy()[:, j]\n",
    "\n",
    "t3_train_cache = pd.DataFrame(cache_cols_train_t3)\n",
    "t3_train_cache.to_csv(CACHE_ROOT / \"t3_train_probs.csv\", index=False)\n",
    "\n",
    "cache_cols_dev_t3 = {\"id\": t3_dev_df[\"id\"].astype(str).values}\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    cache_cols_dev_t3[f\"prob_{lab}\"] = probs_t3_dev[:, j]\n",
    "t3_dev_cache = pd.DataFrame(cache_cols_dev_t3)\n",
    "t3_dev_cache.to_csv(CACHE_ROOT / \"t3_dev_probs.csv\", index=False)\n",
    "\n",
    "print(\"Saved T3 train/dev probabilities (DeBERTa) for ensembling in:\", CACHE_ROOT)\n",
    "\n",
    "# 5.6 Save model + calibration\n",
    "mdl_t3_final.save_pretrained(ART_ROOT / \"native_t3\")\n",
    "tok_t3.save_pretrained(ART_ROOT / \"native_t3\")\n",
    "with open(ART_ROOT / \"calib_t3_native.json\", \"w\") as f:\n",
    "    json.dump({\"temperature\": float(T_t3), \"thresholds\": thr_map_t3}, f, indent=2)\n",
    "\n",
    "# 5.7 Codabench submission CSV (required header order)\n",
    "idx_vil      = T3_LABELS.index(\"vilification\")\n",
    "idx_extreme  = T3_LABELS.index(\"extreme_language\")\n",
    "idx_stereo   = T3_LABELS.index(\"stereotype\")\n",
    "idx_invalid  = T3_LABELS.index(\"invalidation\")\n",
    "idx_lackemp  = T3_LABELS.index(\"lack_of_empathy\")\n",
    "idx_dehum    = T3_LABELS.index(\"dehumanization\")\n",
    "\n",
    "sub3 = pd.DataFrame({\n",
    "    \"id\":               t3_dev_df[\"id\"].astype(str).values,\n",
    "    \"stereotype\":       P3_dev[:, idx_stereo],\n",
    "    \"vilification\":     P3_dev[:, idx_vil],\n",
    "    \"dehumanization\":   P3_dev[:, idx_dehum],\n",
    "    \"extreme_language\": P3_dev[:, idx_extreme],\n",
    "    \"lack_of_empathy\":  P3_dev[:, idx_lackemp],\n",
    "    \"invalidation\":     P3_dev[:, idx_invalid],\n",
    "})\n",
    "sub3_path = SUB_ROOT / \"subtask_3\" / f\"pred_{lang_fname}.csv\"\n",
    "sub3.to_csv(sub3_path, index=False)\n",
    "print(\"Wrote Subtask 3 submission CSV (DeBERTa):\", sub3_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
