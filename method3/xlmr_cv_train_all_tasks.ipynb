{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b110ec",
   "metadata": {},
   "source": [
    "## Setup: env, device, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0226e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 16:01:56.981329: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-08 16:01:56.994711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765238517.008019   72287 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765238517.012028   72287 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765238517.024403   72287 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765238517.024417   72287 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765238517.024419   72287 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765238517.024420   72287 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-08 16:01:57.028812: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.0\n",
      "Transformers: 4.57.1\n",
      "Using GPU: NVIDIA H100 80GB HBM3 MIG 2g.20gb\n",
      "LANG=eng, MODEL=xlm-roberta-base, EPOCHS=3, LR=2e-05, BATCH_TRAIN=8, FOLDS=3\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0) SETUP: environment, device toggle, imports, config\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import transformers\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Device selection\n",
    "# ------------------------------------------------------------\n",
    "RUN_DEVICE = \"gpu\"   # \"gpu\" or \"cpu\"\n",
    "\n",
    "if RUN_DEVICE.lower() == \"gpu\" and torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# ------------------------------------------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# High-level config\n",
    "# ------------------------------------------------------------\n",
    "LANG = \"eng\"                       # change per language: e.g., \"eng\", \"ben\", \"hin\"\n",
    "BASE = \"../dev_phase_aug\"              # root of organizer data\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "\n",
    "MAX_LEN = 192\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "\n",
    "BATCH_TRAIN_GPU = 8\n",
    "BATCH_TRAIN_CPU = 4\n",
    "BATCH_EVAL = 8\n",
    "BATCH_TRAIN = BATCH_TRAIN_GPU if DEVICE.type == \"cuda\" else BATCH_TRAIN_CPU\n",
    "\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.1\n",
    "GRAD_ACCUM = 1\n",
    "N_FOLDS = 3\n",
    "\n",
    "print(f\"LANG={LANG}, MODEL={MODEL_NAME}, EPOCHS={EPOCHS}, LR={LR}, BATCH_TRAIN={BATCH_TRAIN}, FOLDS={N_FOLDS}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Paths / dirs\n",
    "# ------------------------------------------------------------\n",
    "lang_fname = LANG\n",
    "\n",
    "# TRAIN + DEV (DEV is unlabeled)\n",
    "T1_TRAIN = f\"{BASE}/subtask1/train/{lang_fname}.csv\"\n",
    "T1_DEV   = f\"{BASE}/subtask1/dev/{lang_fname}.csv\"\n",
    "\n",
    "T2_TRAIN = f\"{BASE}/subtask2/train/{lang_fname}.csv\"\n",
    "T2_DEV   = f\"{BASE}/subtask2/dev/{lang_fname}.csv\"\n",
    "\n",
    "T3_TRAIN = f\"{BASE}/subtask3/train/{lang_fname}.csv\"\n",
    "T3_DEV   = f\"{BASE}/subtask3/dev/{lang_fname}.csv\"\n",
    "\n",
    "# roots for this method\n",
    "ART_ROOT   = Path(\"artifacts\") / \"xlmr_cv\" / LANG\n",
    "CACHE_ROOT = Path(\"cache\")     / \"xlmr_cv\" / LANG\n",
    "OUT_ROOT   = Path(\"outputs\")   / \"xlmr_cv\" / LANG\n",
    "SUB_ROOT   = Path(\"submissions\") / \"xlmr\"\n",
    "\n",
    "for d in [ART_ROOT, CACHE_ROOT, OUT_ROOT, SUB_ROOT]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "(SUB_ROOT / \"subtask_1\").mkdir(parents=True, exist_ok=True)\n",
    "(SUB_ROOT / \"subtask_2\").mkdir(parents=True, exist_ok=True)\n",
    "(SUB_ROOT / \"subtask_3\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "T2_LABELS = [\"gender/sexual\", \"political\", \"religious\", \"racial/ethnic\", \"other\"]\n",
    "T3_LABELS = [\"vilification\", \"extreme_language\", \"stereotype\",\n",
    "             \"invalidation\", \"lack_of_empathy\", \"dehumanization\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TrainingArguments capability detection\n",
    "# ------------------------------------------------------------\n",
    "import inspect\n",
    "_TA_PARAMS = inspect.signature(TrainingArguments.__init__).parameters\n",
    "TRAINER_CAPS = {\n",
    "    \"evaluation_strategy\": \"evaluation_strategy\" in _TA_PARAMS,\n",
    "    \"save_strategy\":       \"save_strategy\" in _TA_PARAMS,\n",
    "    \"warmup_ratio\":        \"warmup_ratio\" in _TA_PARAMS,\n",
    "    \"fp16\":                \"fp16\" in _TA_PARAMS,\n",
    "    \"no_cuda\":             \"no_cuda\" in _TA_PARAMS,\n",
    "    \"use_mps_device\":      \"use_mps_device\" in _TA_PARAMS,\n",
    "    \"report_to\":           \"report_to\" in _TA_PARAMS,\n",
    "    \"grad_accum\":          \"gradient_accumulation_steps\" in _TA_PARAMS,\n",
    "    \"eval_accum\":          \"eval_accumulation_steps\" in _TA_PARAMS,\n",
    "}\n",
    "\n",
    "def build_training_args(\n",
    "    output_dir,\n",
    "    per_device_train_batch_size,\n",
    "    per_device_eval_batch_size,\n",
    "    num_train_epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    logging_steps=50,\n",
    "    evaluation=\"epoch\",\n",
    "    save=\"no\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    warmup_steps=0,\n",
    "):\n",
    "    use_cuda_flag = (DEVICE.type == \"cuda\")\n",
    "    kwargs = dict(\n",
    "        output_dir=str(output_dir),\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        logging_steps=logging_steps,\n",
    "        dataloader_pin_memory=use_cuda_flag,\n",
    "        dataloader_num_workers=0,\n",
    "    )\n",
    "    if TRAINER_CAPS[\"evaluation_strategy\"]:\n",
    "        kwargs[\"evaluation_strategy\"] = evaluation\n",
    "    if TRAINER_CAPS[\"save_strategy\"]:\n",
    "        kwargs[\"save_strategy\"] = save\n",
    "    if TRAINER_CAPS[\"warmup_ratio\"]:\n",
    "        kwargs[\"warmup_ratio\"] = warmup_ratio\n",
    "    else:\n",
    "        kwargs[\"warmup_steps\"] = warmup_steps\n",
    "    if TRAINER_CAPS[\"fp16\"]:\n",
    "        kwargs[\"fp16\"] = False \n",
    "    if TRAINER_CAPS[\"no_cuda\"]:\n",
    "        kwargs[\"no_cuda\"] = not use_cuda_flag\n",
    "    if TRAINER_CAPS[\"use_mps_device\"]:\n",
    "        kwargs[\"use_mps_device\"] = False\n",
    "    if TRAINER_CAPS[\"report_to\"]:\n",
    "        kwargs[\"report_to\"] = \"none\"\n",
    "    if TRAINER_CAPS[\"grad_accum\"]:\n",
    "        kwargs[\"gradient_accumulation_steps\"] = GRAD_ACCUM\n",
    "    if TRAINER_CAPS[\"eval_accum\"]:\n",
    "        kwargs[\"eval_accumulation_steps\"] = 4\n",
    "    return TrainingArguments(**kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf9f2b9",
   "metadata": {},
   "source": [
    "## Dataset, metrics, calibration, focal loss, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a447b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) DATASET + METRICS + CALIBRATION HELPERS\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "class TextClsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        labels: Optional[List] = None,\n",
    "        tokenizer=None,\n",
    "        max_len: int = 256,\n",
    "        is_multilabel: bool = False,\n",
    "    ):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.is_multilabel = is_multilabel\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        text = str(self.texts[idx])\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=False,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        if self.labels is not None:\n",
    "            y = self.labels[idx]\n",
    "            item[\"labels\"] = torch.tensor(\n",
    "                y,\n",
    "                dtype=torch.float if self.is_multilabel else torch.long,\n",
    "            )\n",
    "        return item\n",
    "\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "\n",
    "def grid_search_thresholds(y_true, y_prob, label_names=None):\n",
    "    \"\"\"\n",
    "    Per-label threshold search for multi-label problems.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_prob = np.asarray(y_prob)\n",
    "    C = y_true.shape[1]\n",
    "    grid = np.linspace(0.05, 0.95, 19)\n",
    "    thrs = {}\n",
    "    for c in range(C):\n",
    "        best_t, best_f = 0.5, -1.0\n",
    "        for t in grid:\n",
    "            preds = (y_prob[:, c] >= t).astype(int)\n",
    "            f = f1_score(y_true[:, c], preds, average=\"binary\", zero_division=0)\n",
    "            if f > best_f:\n",
    "                best_f, best_t = f, t\n",
    "        name = label_names[c] if label_names else str(c)\n",
    "        thrs[name] = float(best_t)\n",
    "    return thrs\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Fixed temperature scaling (safe, F1-aware)\n",
    "# ============================================================\n",
    "\n",
    "class TempScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    Temperature scaler with log-parameterization:\n",
    "      T = exp(log_T)  -> T > 0\n",
    "    and clamping to [min_T, max_T] for stability.\n",
    "    \"\"\"\n",
    "    def __init__(self, init_T: float = 1.0, min_T: float = 0.05, max_T: float = 10.0):\n",
    "        super().__init__()\n",
    "        self.log_T = nn.Parameter(torch.log(torch.tensor([init_T], dtype=torch.float32)))\n",
    "        self.min_T = min_T\n",
    "        self.max_T = max_T\n",
    "\n",
    "    def get_T(self) -> torch.Tensor:\n",
    "        T = torch.exp(self.log_T)\n",
    "        if self.min_T is not None or self.max_T is not None:\n",
    "            T = torch.clamp(T, self.min_T, self.max_T)\n",
    "        return T\n",
    "\n",
    "    def forward(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        T = self.get_T()\n",
    "        return logits / T\n",
    "\n",
    "\n",
    "def learn_temperature(\n",
    "    dev_logits: torch.Tensor,\n",
    "    dev_labels: torch.Tensor,\n",
    "    is_multilabel: bool,\n",
    "    f1_tolerance: float = 0.01,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Learn a temperature T using LBFGS on CPU, with safety checks:\n",
    "      - T is forced positive via log_T and clamped to [0.05, 10.0].\n",
    "      - Compute macro-F1 before and after scaling (thr=0.5 for multi-label).\n",
    "      - If calibrated F1 << base F1 (by > f1_tolerance), fall back to T=1.0.\n",
    "\n",
    "    Returns:\n",
    "        scalar float T\n",
    "    \"\"\"\n",
    "    device_cpu = torch.device(\"cpu\")\n",
    "\n",
    "    dev_logits = dev_logits.detach().to(device_cpu)\n",
    "    dev_labels = dev_labels.detach().to(device_cpu)\n",
    "\n",
    "    # ---- 1) Baseline F1 at T = 1.0 ----\n",
    "    with torch.no_grad():\n",
    "        if is_multilabel:\n",
    "            probs_base = torch.sigmoid(dev_logits)\n",
    "            preds_base = (probs_base >= 0.5).long().cpu().numpy()\n",
    "            y_true = dev_labels.cpu().numpy()\n",
    "            base_f1 = f1_score(y_true, preds_base, average=\"macro\", zero_division=0)\n",
    "        else:\n",
    "            probs_base = torch.softmax(dev_logits, dim=1)\n",
    "            preds_base = probs_base.argmax(dim=1).cpu().numpy()\n",
    "            y_true = dev_labels.cpu().numpy()\n",
    "            base_f1 = f1_score(y_true, preds_base, average=\"macro\", zero_division=0)\n",
    "\n",
    "    # ---- 2) Optimize log_T ----\n",
    "    scaler = TempScaler(init_T=1.0, min_T=0.05, max_T=10.0).to(device_cpu)\n",
    "    opt = torch.optim.LBFGS([scaler.log_T], lr=0.01, max_iter=50)\n",
    "    criterion = nn.BCEWithLogitsLoss() if is_multilabel else nn.CrossEntropyLoss()\n",
    "\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        z = scaler(dev_logits)\n",
    "        if is_multilabel:\n",
    "            loss = criterion(z, dev_labels.float())\n",
    "        else:\n",
    "            loss = criterion(z, dev_labels.long())\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    try:\n",
    "        opt.step(closure)\n",
    "    except Exception as e:\n",
    "        print(f\"[TempScale] LBFGS failed: {e}. Using T=1.0.\")\n",
    "        return 1.0\n",
    "\n",
    "    # ---- 3) Evaluate calibrated F1 ----\n",
    "    with torch.no_grad():\n",
    "        T_tensor = scaler.get_T()\n",
    "        T_value = float(T_tensor.item())\n",
    "        z_cal = dev_logits / T_tensor\n",
    "\n",
    "        if is_multilabel:\n",
    "            probs_cal = torch.sigmoid(z_cal)\n",
    "            preds_cal = (probs_cal >= 0.5).long().cpu().numpy()\n",
    "            f1_cal = f1_score(y_true, preds_cal, average=\"macro\", zero_division=0)\n",
    "        else:\n",
    "            probs_cal = torch.softmax(z_cal, dim=1)\n",
    "            preds_cal = probs_cal.argmax(dim=1).cpu().numpy()\n",
    "            f1_cal = f1_score(y_true, preds_cal, average=\"macro\", zero_division=0)\n",
    "\n",
    "    print(f\"[TempScale] base_F1={base_f1:.4f}, calibrated_F1={f1_cal:.4f}, T={T_value:.4f}\")\n",
    "\n",
    "    # ---- 4) Safety fallback ----\n",
    "    if f1_cal + 1e-4 < base_f1 - f1_tolerance:\n",
    "        print(\"[TempScale] calibrated F1 is worse than base F1; using T=1.0 instead.\")\n",
    "        return 1.0\n",
    "\n",
    "    return float(T_value)\n",
    "\n",
    "\n",
    "def collect_logits(trainer: Trainer, dataset: Dataset, is_multilabel: bool):\n",
    "    \"\"\"\n",
    "    Use trainer.predict to collect logits + labels for a dataset.\n",
    "    \"\"\"\n",
    "    preds = trainer.predict(dataset)\n",
    "    raw = preds.predictions\n",
    "    if isinstance(raw, (list, tuple)):\n",
    "        raw = raw[0]\n",
    "    logits = torch.tensor(raw)\n",
    "    labels = torch.tensor(preds.label_ids)\n",
    "    if not is_multilabel and logits.ndim == 1:\n",
    "        logits = logits.unsqueeze(1)\n",
    "    return logits, labels\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1.1 Focal BCE loss for multi-label\n",
    "# ============================================================\n",
    "\n",
    "class FocalBCEWithLogitsLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal BCE with optional per-class alpha (pos_weight).\n",
    "    alpha: tensor [C] (like your pos_weight), gamma ~ 1.0-2.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha: Optional[torch.Tensor] = None, gamma: float = 2.0, reduction: str = \"mean\"):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits, targets: [B, C]\n",
    "        bce = nn.functional.binary_cross_entropy_with_logits(\n",
    "            logits, targets, reduction=\"none\", pos_weight=self.alpha\n",
    "        )  # [B,C]\n",
    "        p = torch.sigmoid(logits)\n",
    "        pt = p * targets + (1 - p) * (1 - targets)   # [B,C]\n",
    "        focal = (1 - pt) ** self.gamma * bce\n",
    "        if self.reduction == \"mean\":\n",
    "            return focal.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal.sum()\n",
    "        return focal\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1.2 Simple K-fold helper\n",
    "# ============================================================\n",
    "\n",
    "def make_stratified_folds(y_for_strat: np.ndarray, n_splits: int, seed: int = 42):\n",
    "    \"\"\"\n",
    "    y_for_strat: 1D array of labels for stratification (e.g., polarization or has_any_label).\n",
    "    Returns list of (train_idx, val_idx).\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    idx = np.arange(len(y_for_strat))\n",
    "    folds = list(skf.split(idx, y_for_strat))\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8dddc",
   "metadata": {},
   "source": [
    "## Subtask 1 (binary) with K-fold calibration + dev submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e806617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T1] TRAIN size: 5572\n",
      "[T1] DEV size (unlabeled): 160\n",
      "\n",
      "[T1] Fold 1/3 — train=3714, val=1858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1395' max='1395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1395/1395 01:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.649500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.601500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.655900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.658500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.515300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.570400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.529800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.524900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.487200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.493500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.414900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.462900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.460300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.398400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.380100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.466100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.337300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.358800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.358900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.383400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.266800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.351300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.278700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (argmax): 0.8133783210543509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[T1] Fold 2/3 — train=3715, val=1857\n",
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1395' max='1395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1395/1395 01:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.634800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.642100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.583700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.561100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.578700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.520200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.542800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.428400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.480200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.463100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.363000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.371900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.349600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.349600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.341700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.272600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.322200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.320800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.292900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.383700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (argmax): 0.8109016321899276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[T1] Fold 3/3 — train=3715, val=1857\n",
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1395' max='1395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1395/1395 01:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.660100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.622300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.561400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.643100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.591900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.578800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.615600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.541000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.491700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.472300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.487300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.504900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.497200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.421200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.436900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.403400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.361800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.322600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.427400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.398900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.405500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.404000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.380400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (argmax): 0.8170634184785128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TempScale] base_F1=0.8139, calibrated_F1=0.8139, T=1.3003\n",
      "\n",
      "[T1] Calibration (OOF):\n",
      "  Temperature T=1.3003\n",
      "  Best threshold=0.80\n",
      "  Macro-F1 (OOF, calibrated)=0.8214\n",
      "\n",
      "[T1] Training FINAL model on full train...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2091' max='2091' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2091/2091 01:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.690600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.619500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.604500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.631100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.615400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.549900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.528800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.512100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.509100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.480900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.479100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.406000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.421700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.409600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.390500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.408400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.346100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.449300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.351200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.430100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.350700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.368800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.294400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.364200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.351200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.295600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.261500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.348400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.284900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.351500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.300100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T1] Macro-F1 (TRAIN, argmax, final model): 0.9258120841623141\n",
      "[T1] Macro-F1 (TRAIN, calibrated T+thr, final model): 0.9321058071750241\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved T1 train/dev probabilities for ensembling in: cache/xlmr_cv/eng\n",
      "Wrote Subtask 1 submission CSV: submissions/xlmr/subtask_1/pred_eng.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2) SUBTASK 1 — Polarization (binary, 2 classes)\n",
    "#    K-fold OOF calibration + final model + dev submission\n",
    "# ============================================================\n",
    "\n",
    "# 2.1 Load TRAIN + DEV\n",
    "t1_train_df = pd.read_csv(T1_TRAIN)\n",
    "t1_dev_df   = pd.read_csv(T1_DEV)\n",
    "\n",
    "required_train_cols_t1 = {\"id\", \"text\", \"polarization\"}\n",
    "required_dev_cols_t1   = {\"id\", \"text\"}\n",
    "assert required_train_cols_t1.issubset(t1_train_df.columns), \\\n",
    "    f\"T1 TRAIN missing: {required_train_cols_t1 - set(t1_train_df.columns)}\"\n",
    "assert required_dev_cols_t1.issubset(t1_dev_df.columns), \\\n",
    "    f\"T1 DEV missing: {required_dev_cols_t1 - set(t1_dev_df.columns)}\"\n",
    "\n",
    "t1_train_df[\"polarization\"] = t1_train_df[\"polarization\"].astype(int)\n",
    "\n",
    "print(f\"[T1] TRAIN size: {len(t1_train_df)}\")\n",
    "print(f\"[T1] DEV size (unlabeled): {len(t1_dev_df)}\")\n",
    "\n",
    "# 2.2 K-fold out-of-fold logits for calibration\n",
    "y_t1 = t1_train_df[\"polarization\"].to_numpy()\n",
    "folds_t1 = make_stratified_folds(y_t1, n_splits=N_FOLDS, seed=SEED)\n",
    "\n",
    "oof_logits_t1 = np.zeros((len(t1_train_df), 2), dtype=np.float32)\n",
    "oof_labels_t1 = y_t1.copy()\n",
    "\n",
    "tok_t1 = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(folds_t1):\n",
    "    print(f\"\\n[T1] Fold {fold+1}/{N_FOLDS} — train={len(tr_idx)}, val={len(val_idx)}\")\n",
    "\n",
    "    # datasets for this fold\n",
    "    ds_tr = TextClsDataset(\n",
    "        texts=t1_train_df[\"text\"].iloc[tr_idx].tolist(),\n",
    "        labels=t1_train_df[\"polarization\"].iloc[tr_idx].tolist(),\n",
    "        tokenizer=tok_t1,\n",
    "        max_len=MAX_LEN,\n",
    "        is_multilabel=False,\n",
    "    )\n",
    "    ds_val = TextClsDataset(\n",
    "        texts=t1_train_df[\"text\"].iloc[val_idx].tolist(),\n",
    "        labels=t1_train_df[\"polarization\"].iloc[val_idx].tolist(),\n",
    "        tokenizer=tok_t1,\n",
    "        max_len=MAX_LEN,\n",
    "        is_multilabel=False,\n",
    "    )\n",
    "\n",
    "    cfg_t1_fold = AutoConfig.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "    mdl_t1_fold = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=cfg_t1_fold)\n",
    "    mdl_t1_fold.config.use_cache = False\n",
    "    if hasattr(mdl_t1_fold, \"gradient_checkpointing_enable\"):\n",
    "        mdl_t1_fold.gradient_checkpointing_enable()\n",
    "    mdl_t1_fold.to(DEVICE)\n",
    "\n",
    "    args_t1_fold = build_training_args(\n",
    "        output_dir=ART_ROOT / f\"t1_cv_fold{fold+1}\",\n",
    "        per_device_train_batch_size=BATCH_TRAIN,\n",
    "        per_device_eval_batch_size=BATCH_EVAL,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        logging_steps=50,\n",
    "        evaluation=\"epoch\",\n",
    "        save=\"no\",\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "    )\n",
    "\n",
    "    def compute_metrics_t1_fold(eval_pred):\n",
    "        logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, (list, tuple)) else eval_pred.predictions\n",
    "        labels = eval_pred.label_ids\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        return {\"f1_macro\": macro_f1(labels, preds)}\n",
    "\n",
    "    trainer_t1_fold = Trainer(\n",
    "        model=mdl_t1_fold,\n",
    "        args=args_t1_fold,\n",
    "        train_dataset=ds_tr,\n",
    "        eval_dataset=ds_val,\n",
    "        tokenizer=tok_t1,\n",
    "        data_collator=DataCollatorWithPadding(tok_t1),\n",
    "        compute_metrics=compute_metrics_t1_fold,\n",
    "    )\n",
    "    print(\"  Trainer device:\", trainer_t1_fold.args.device)\n",
    "\n",
    "    trainer_t1_fold.train()\n",
    "    eval_fold = trainer_t1_fold.evaluate()\n",
    "    print(\"  Fold Macro-F1 (argmax):\", eval_fold.get(\"eval_f1_macro\"))\n",
    "\n",
    "    # collect logits for this fold's val set\n",
    "    logits_val, labels_val = collect_logits(trainer_t1_fold, ds_val, is_multilabel=False)\n",
    "    oof_logits_t1[val_idx] = logits_val.numpy()\n",
    "\n",
    "    # cleanup\n",
    "    del trainer_t1_fold, mdl_t1_fold\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# 2.3 Calibrate on OOF logits\n",
    "logits_oof_t1 = torch.from_numpy(oof_logits_t1)\n",
    "labels_oof_t1 = torch.from_numpy(oof_labels_t1)\n",
    "\n",
    "T_t1 = learn_temperature(logits_oof_t1, labels_oof_t1, is_multilabel=False)\n",
    "probs_oof_t1 = torch.softmax(logits_oof_t1 / T_t1, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "best_thr_t1, best_f1_t1 = 0.5, -1.0\n",
    "for t in np.linspace(0.05, 0.95, 19):\n",
    "    pred = (probs_oof_t1 >= t).astype(int)\n",
    "    f = macro_f1(labels_oof_t1.numpy(), pred)\n",
    "    if f > best_f1_t1:\n",
    "        best_f1_t1, best_thr_t1 = f, t\n",
    "\n",
    "print(\"\\n[T1] Calibration (OOF):\")\n",
    "print(f\"  Temperature T={T_t1:.4f}\")\n",
    "print(f\"  Best threshold={best_thr_t1:.2f}\")\n",
    "print(f\"  Macro-F1 (OOF, calibrated)={best_f1_t1:.4f}\")\n",
    "\n",
    "# 2.4 Train FINAL model on full TRAIN\n",
    "cfg_t1_final = AutoConfig.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "mdl_t1_final = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=cfg_t1_final)\n",
    "mdl_t1_final.config.use_cache = False\n",
    "if hasattr(mdl_t1_final, \"gradient_checkpointing_enable\"):\n",
    "    mdl_t1_final.gradient_checkpointing_enable()\n",
    "mdl_t1_final.to(DEVICE)\n",
    "\n",
    "ds_t1_train_full = TextClsDataset(\n",
    "    texts=t1_train_df[\"text\"].tolist(),\n",
    "    labels=t1_train_df[\"polarization\"].tolist(),\n",
    "    tokenizer=tok_t1,\n",
    "    max_len=MAX_LEN,\n",
    "    is_multilabel=False,\n",
    ")\n",
    "ds_t1_dev_full = TextClsDataset(\n",
    "    texts=t1_dev_df[\"text\"].tolist(),\n",
    "    labels=[0] * len(t1_dev_df),\n",
    "    tokenizer=tok_t1,\n",
    "    max_len=MAX_LEN,\n",
    "    is_multilabel=False,\n",
    ")\n",
    "\n",
    "args_t1_final = build_training_args(\n",
    "    output_dir=ART_ROOT / \"t1_final\",\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=50,\n",
    "    evaluation=\"epoch\",\n",
    "    save=\"no\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    ")\n",
    "\n",
    "def compute_metrics_t1_final(eval_pred):\n",
    "    logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, (list, tuple)) else eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\"f1_macro\": macro_f1(labels, preds)}\n",
    "\n",
    "trainer_t1_final = Trainer(\n",
    "    model=mdl_t1_final,\n",
    "    args=args_t1_final,\n",
    "    train_dataset=ds_t1_train_full,\n",
    "    eval_dataset=ds_t1_train_full,  # report train F1\n",
    "    tokenizer=tok_t1,\n",
    "    data_collator=DataCollatorWithPadding(tok_t1),\n",
    "    compute_metrics=compute_metrics_t1_final,\n",
    ")\n",
    "\n",
    "print(\"\\n[T1] Training FINAL model on full train...\")\n",
    "trainer_t1_final.train()\n",
    "eval_t1_train_full = trainer_t1_final.evaluate()\n",
    "print(\"[T1] Macro-F1 (TRAIN, argmax, final model):\", eval_t1_train_full.get(\"eval_f1_macro\"))\n",
    "\n",
    "# 2.5 Train-set metrics with calibrated T + threshold\n",
    "logits_t1_train_full, labels_t1_train_full = collect_logits(trainer_t1_final, ds_t1_train_full, is_multilabel=False)\n",
    "probs_t1_train_full = torch.softmax(logits_t1_train_full / T_t1, dim=1)[:, 1].cpu().numpy()\n",
    "pred_t1_train_full  = (probs_t1_train_full >= best_thr_t1).astype(int)\n",
    "print(\"[T1] Macro-F1 (TRAIN, calibrated T+thr, final model):\",\n",
    "      macro_f1(labels_t1_train_full.numpy(), pred_t1_train_full))\n",
    "\n",
    "# 2.6 Inference on DEV\n",
    "preds_dev_t1 = trainer_t1_final.predict(ds_t1_dev_full)\n",
    "logits_t1_dev = torch.tensor(preds_dev_t1.predictions if not isinstance(preds_dev_t1.predictions,(list,tuple)) else preds_dev_t1.predictions[0])\n",
    "probs_t1_dev = torch.softmax(logits_t1_dev / T_t1, dim=1)[:, 1].cpu().numpy()\n",
    "pred_t1_dev = (probs_t1_dev >= best_thr_t1).astype(int)\n",
    "\n",
    "# 2.7 Cache train/dev probs for later ensembling\n",
    "cache_t1_train = pd.DataFrame({\n",
    "    \"id\": t1_train_df[\"id\"].astype(str),\n",
    "    \"prob_pos\": probs_t1_train_full,\n",
    "    \"label\": t1_train_df[\"polarization\"].astype(int),\n",
    "})\n",
    "cache_t1_train.to_csv(CACHE_ROOT / \"t1_train_probs.csv\", index=False)\n",
    "\n",
    "cache_t1_dev = pd.DataFrame({\n",
    "    \"id\": t1_dev_df[\"id\"].astype(str),\n",
    "    \"prob_pos\": probs_t1_dev,\n",
    "})\n",
    "cache_t1_dev.to_csv(CACHE_ROOT / \"t1_dev_probs.csv\", index=False)\n",
    "\n",
    "print(\"Saved T1 train/dev probabilities for ensembling in:\", CACHE_ROOT)\n",
    "\n",
    "# 2.8 Save model + calibration\n",
    "mdl_t1_final.save_pretrained(ART_ROOT / \"native_t1\")\n",
    "tok_t1.save_pretrained(ART_ROOT / \"native_t1\")\n",
    "with open(ART_ROOT / \"calib_t1_native.json\", \"w\") as f:\n",
    "    json.dump({\"temperature\": float(T_t1), \"threshold\": float(best_thr_t1)}, f, indent=2)\n",
    "\n",
    "# 2.9 Codabench submission CSV\n",
    "sub1 = pd.DataFrame({\n",
    "    \"id\": t1_dev_df[\"id\"].astype(str),\n",
    "    \"polarization\": pred_t1_dev.astype(int),\n",
    "})\n",
    "sub1_path = SUB_ROOT / \"subtask_1\" / f\"pred_{lang_fname}.csv\"\n",
    "sub1.to_csv(sub1_path, index=False)\n",
    "print(\"Wrote Subtask 1 submission CSV:\", sub1_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafc007a",
   "metadata": {},
   "source": [
    "## Subtask 2 (multi-label, 5) with focal loss + K-fold calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd7db31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T2] TRAIN size: 5572\n",
      "[T2] DEV size (unlabeled): 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[T2] Fold 1/3 — train=3714, val=1858\n",
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1395' max='1395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1395/1395 01:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.395900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.455400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.427100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.371800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.376500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.359700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.385900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.379600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.305800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.310200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.296500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.358600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.264600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.295800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.264200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.237900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.241900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.248600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.308900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.188500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (thr=0.5): 0.5614513400904735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[T2] Fold 2/3 — train=3715, val=1857\n",
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1395' max='1395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1395/1395 01:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.377000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.447200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.459100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.389100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.402500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.396100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.361100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.399200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.324800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.315900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.449300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.283500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.315800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.254900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.302900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (thr=0.5): 0.5663994550280234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[T2] Fold 3/3 — train=3715, val=1857\n",
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1395' max='1395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1395/1395 01:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.413900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.440200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.389600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.360300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.367600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.327300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.273500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.344800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.306500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.247900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.351600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.306100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.322400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.326500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.293600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.177800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (thr=0.5): 0.5386081482615483\n",
      "[TempScale] base_F1=0.5564, calibrated_F1=0.5564, T=0.7672\n",
      "\n",
      "[T2] Calibration (OOF):\n",
      "  Temperature: 0.76723712682724\n",
      "  Thresholds: {'gender/sexual': 0.7999999999999999, 'political': 0.39999999999999997, 'religious': 0.85, 'racial/ethnic': 0.65, 'other': 0.65}\n",
      "  Macro-F1 (OOF, calibrated): 0.6022014934791591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[T2] Training FINAL model on full train...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2091' max='2091' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2091/2091 01:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.362400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.384600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.432500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.393600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.410700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.374700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.399600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.283000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.336700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.359900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.389100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.256400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.343600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.366900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.268100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.350800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.309300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.292700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.378300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.328200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.309600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.167600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.253200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.284900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.249600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.226900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.271100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.232900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T2] Macro-F1 (TRAIN, thr=0.5, final model): 0.7048350257707434\n",
      "[T2] Macro-F1 (TRAIN, calibrated T+thr, final model): 0.7591862707286214\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved T2 train/dev probabilities for ensembling in: cache/xlmr_cv/eng\n",
      "Wrote Subtask 2 submission CSV: submissions/xlmr/subtask_2/pred_eng.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3) SUBTASK 2 — Hate type (multi-label, 5 labels)\n",
    "#    Focal BCE + K-fold OOF calibration + dev submission\n",
    "# ============================================================\n",
    "\n",
    "# 3.1 Load TRAIN + DEV\n",
    "t2_train_df = pd.read_csv(T2_TRAIN)\n",
    "t2_dev_df   = pd.read_csv(T2_DEV)\n",
    "\n",
    "required_train_cols_t2 = {\"id\", \"text\", *T2_LABELS}\n",
    "required_dev_cols_t2   = {\"id\", \"text\"}\n",
    "assert required_train_cols_t2.issubset(t2_train_df.columns), \\\n",
    "    f\"T2 TRAIN missing: {required_train_cols_t2 - set(t2_train_df.columns)}\"\n",
    "assert required_dev_cols_t2.issubset(t2_dev_df.columns), \\\n",
    "    f\"T2 DEV missing: {required_dev_cols_t2 - set(t2_dev_df.columns)}\"\n",
    "\n",
    "Y2_train = t2_train_df[T2_LABELS].values.astype(int)\n",
    "print(f\"[T2] TRAIN size: {len(t2_train_df)}\")\n",
    "print(f\"[T2] DEV size (unlabeled): {len(t2_dev_df)}\")\n",
    "\n",
    "# For stratification: does a sample have ANY positive label?\n",
    "y2_strat = (Y2_train.sum(axis=1) > 0).astype(int)\n",
    "folds_t2 = make_stratified_folds(y2_strat, n_splits=N_FOLDS, seed=SEED)\n",
    "\n",
    "tok_t2 = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "# pos_weight from full train (can also recompute per fold if you want)\n",
    "pos_count_2 = Y2_train.sum(axis=0) + 1e-6\n",
    "neg_count_2 = Y2_train.shape[0] - pos_count_2\n",
    "pos_weight_2 = torch.tensor(neg_count_2 / pos_count_2, dtype=torch.float)\n",
    "\n",
    "oof_logits_t2 = np.zeros((len(t2_train_df), len(T2_LABELS)), dtype=np.float32)\n",
    "oof_labels_t2 = Y2_train.copy()\n",
    "\n",
    "class FocalTrainerT2(Trainer):\n",
    "    def __init__(self, *args, pos_weight=None, gamma=1.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pos_weight = pos_weight\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = FocalBCEWithLogitsLoss(\n",
    "            alpha=self.pos_weight.to(logits.device),\n",
    "            gamma=self.gamma,\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "        loss = loss_fct(logits, labels.to(logits.device).float())\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(folds_t2):\n",
    "    print(f\"\\n[T2] Fold {fold+1}/{N_FOLDS} — train={len(tr_idx)}, val={len(val_idx)}\")\n",
    "\n",
    "    ds_tr = TextClsDataset(\n",
    "        texts=t2_train_df[\"text\"].iloc[tr_idx].tolist(),\n",
    "        labels=Y2_train[tr_idx].tolist(),\n",
    "        tokenizer=tok_t2,\n",
    "        max_len=MAX_LEN,\n",
    "        is_multilabel=True,\n",
    "    )\n",
    "    ds_val = TextClsDataset(\n",
    "        texts=t2_train_df[\"text\"].iloc[val_idx].tolist(),\n",
    "        labels=Y2_train[val_idx].tolist(),\n",
    "        tokenizer=tok_t2,\n",
    "        max_len=MAX_LEN,\n",
    "        is_multilabel=True,\n",
    "    )\n",
    "\n",
    "    cfg_t2_fold = AutoConfig.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(T2_LABELS),\n",
    "        problem_type=\"multi_label_classification\",\n",
    "    )\n",
    "    mdl_t2_fold = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=cfg_t2_fold)\n",
    "    mdl_t2_fold.config.use_cache = False\n",
    "    if hasattr(mdl_t2_fold, \"gradient_checkpointing_enable\"):\n",
    "        mdl_t2_fold.gradient_checkpointing_enable()\n",
    "    mdl_t2_fold.to(DEVICE)\n",
    "\n",
    "    args_t2_fold = build_training_args(\n",
    "        output_dir=ART_ROOT / f\"t2_cv_fold{fold+1}\",\n",
    "        per_device_train_batch_size=BATCH_TRAIN,\n",
    "        per_device_eval_batch_size=BATCH_EVAL,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        logging_steps=50,\n",
    "        evaluation=\"epoch\",\n",
    "        save=\"no\",\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "    )\n",
    "\n",
    "    def compute_metrics_t2_fold(eval_pred):\n",
    "        logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, (list, tuple)) else eval_pred.predictions\n",
    "        labels = eval_pred.label_ids\n",
    "        probs  = 1.0 / (1.0 + np.exp(-logits))\n",
    "        preds  = (probs >= 0.5).astype(int)\n",
    "        return {\"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0)}\n",
    "\n",
    "    trainer_t2_fold = FocalTrainerT2(\n",
    "        model=mdl_t2_fold,\n",
    "        args=args_t2_fold,\n",
    "        train_dataset=ds_tr,\n",
    "        eval_dataset=ds_val,\n",
    "        tokenizer=tok_t2,\n",
    "        data_collator=DataCollatorWithPadding(tok_t2),\n",
    "        compute_metrics=compute_metrics_t2_fold,\n",
    "        pos_weight=pos_weight_2,\n",
    "        gamma=1.5,\n",
    "    )\n",
    "    print(\"  Trainer device:\", trainer_t2_fold.args.device)\n",
    "\n",
    "    trainer_t2_fold.train()\n",
    "    eval_fold = trainer_t2_fold.evaluate()\n",
    "    print(\"  Fold Macro-F1 (thr=0.5):\", eval_fold.get(\"eval_f1_macro\"))\n",
    "\n",
    "    logits_val, labels_val = collect_logits(trainer_t2_fold, ds_val, is_multilabel=True)\n",
    "    oof_logits_t2[val_idx] = logits_val.numpy()\n",
    "\n",
    "    del trainer_t2_fold, mdl_t2_fold\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# 3.2 Calibrate on OOF logits\n",
    "logits_oof_t2 = torch.from_numpy(oof_logits_t2)\n",
    "labels_oof_t2 = torch.from_numpy(oof_labels_t2)\n",
    "\n",
    "T_t2 = learn_temperature(logits_oof_t2, labels_oof_t2, is_multilabel=True)\n",
    "probs_oof_t2 = torch.sigmoid(logits_oof_t2 / T_t2).cpu().numpy()\n",
    "thr_map_t2 = grid_search_thresholds(labels_oof_t2.numpy(), probs_oof_t2, T2_LABELS)\n",
    "\n",
    "# Macro-F1 with calibrated thresholds on OOF\n",
    "P2_oof = np.zeros_like(probs_oof_t2, dtype=int)\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    thr = float(thr_map_t2[lab])\n",
    "    P2_oof[:, j] = (probs_oof_t2[:, j] >= thr).astype(int)\n",
    "f1_oof_t2 = f1_score(labels_oof_t2.numpy(), P2_oof, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"\\n[T2] Calibration (OOF):\")\n",
    "print(\"  Temperature:\", T_t2)\n",
    "print(\"  Thresholds:\", thr_map_t2)\n",
    "print(\"  Macro-F1 (OOF, calibrated):\", f1_oof_t2)\n",
    "\n",
    "# 3.3 Train FINAL model on full TRAIN\n",
    "cfg_t2_final = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(T2_LABELS),\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")\n",
    "mdl_t2_final = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=cfg_t2_final)\n",
    "mdl_t2_final.config.use_cache = False\n",
    "if hasattr(mdl_t2_final, \"gradient_checkpointing_enable\"):\n",
    "    mdl_t2_final.gradient_checkpointing_enable()\n",
    "mdl_t2_final.to(DEVICE)\n",
    "\n",
    "ds_t2_train_full = TextClsDataset(\n",
    "    texts=t2_train_df[\"text\"].tolist(),\n",
    "    labels=Y2_train.tolist(),\n",
    "    tokenizer=tok_t2,\n",
    "    max_len=MAX_LEN,\n",
    "    is_multilabel=True,\n",
    ")\n",
    "dummy_labels_t2_dev = np.zeros((len(t2_dev_df), len(T2_LABELS)), dtype=int)\n",
    "ds_t2_dev_full = TextClsDataset(\n",
    "    texts=t2_dev_df[\"text\"].tolist(),\n",
    "    labels=dummy_labels_t2_dev.tolist(),\n",
    "    tokenizer=tok_t2,\n",
    "    max_len=MAX_LEN,\n",
    "    is_multilabel=True,\n",
    ")\n",
    "\n",
    "args_t2_final = build_training_args(\n",
    "    output_dir=ART_ROOT / \"t2_final\",\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=50,\n",
    "    evaluation=\"epoch\",\n",
    "    save=\"no\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    ")\n",
    "\n",
    "def compute_metrics_t2_final(eval_pred):\n",
    "    logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, (list, tuple)) else eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    probs  = 1.0 / (1.0 + np.exp(-logits))\n",
    "    preds  = (probs >= 0.5).astype(int)\n",
    "    return {\"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0)}\n",
    "\n",
    "trainer_t2_final = FocalTrainerT2(\n",
    "    model=mdl_t2_final,\n",
    "    args=args_t2_final,\n",
    "    train_dataset=ds_t2_train_full,\n",
    "    eval_dataset=ds_t2_train_full,  # report train F1\n",
    "    tokenizer=tok_t2,\n",
    "    data_collator=DataCollatorWithPadding(tok_t2),\n",
    "    compute_metrics=compute_metrics_t2_final,\n",
    "    pos_weight=pos_weight_2,\n",
    "    gamma=1.5,\n",
    ")\n",
    "\n",
    "print(\"\\n[T2] Training FINAL model on full train...\")\n",
    "trainer_t2_final.train()\n",
    "eval_t2_train_full = trainer_t2_final.evaluate()\n",
    "print(\"[T2] Macro-F1 (TRAIN, thr=0.5, final model):\", eval_t2_train_full.get(\"eval_f1_macro\"))\n",
    "\n",
    "# Train-set calibrated metrics\n",
    "logits_t2_train_full, labels_t2_train_full = collect_logits(trainer_t2_final, ds_t2_train_full, is_multilabel=True)\n",
    "probs_t2_train_full = torch.sigmoid(logits_t2_train_full / T_t2).cpu().numpy()\n",
    "P2_train_full = np.zeros_like(probs_t2_train_full, dtype=int)\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    thr = float(thr_map_t2[lab])\n",
    "    P2_train_full[:, j] = (probs_t2_train_full[:, j] >= thr).astype(int)\n",
    "train_f1_calib_t2 = f1_score(labels_t2_train_full.numpy(), P2_train_full, average=\"macro\", zero_division=0)\n",
    "print(\"[T2] Macro-F1 (TRAIN, calibrated T+thr, final model):\", train_f1_calib_t2)\n",
    "\n",
    "# 3.4 Inference on DEV\n",
    "preds_dev_t2 = trainer_t2_final.predict(ds_t2_dev_full)\n",
    "logits_t2_dev = torch.tensor(preds_dev_t2.predictions if not isinstance(preds_dev_t2.predictions,(list,tuple)) else preds_dev_t2.predictions[0])\n",
    "probs_t2_dev = torch.sigmoid(logits_t2_dev / T_t2).cpu().numpy()\n",
    "\n",
    "P2_dev = np.zeros_like(probs_t2_dev, dtype=int)\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    thr = float(thr_map_t2[lab])\n",
    "    P2_dev[:, j] = (probs_t2_dev[:, j] >= thr).astype(int)\n",
    "\n",
    "# 3.5 Cache train/dev probs for ensembling\n",
    "cache_cols_train_t2 = {\"id\": t2_train_df[\"id\"].astype(str).values}\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    cache_cols_train_t2[f\"prob_{lab}\"] = probs_t2_train_full[:, j]\n",
    "    cache_cols_train_t2[f\"label_{lab}\"] = labels_t2_train_full.numpy()[:, j]\n",
    "\n",
    "t2_train_cache = pd.DataFrame(cache_cols_train_t2)\n",
    "t2_train_cache.to_csv(CACHE_ROOT / \"t2_train_probs.csv\", index=False)\n",
    "\n",
    "cache_cols_dev_t2 = {\"id\": t2_dev_df[\"id\"].astype(str).values}\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    cache_cols_dev_t2[f\"prob_{lab}\"] = probs_t2_dev[:, j]\n",
    "t2_dev_cache = pd.DataFrame(cache_cols_dev_t2)\n",
    "t2_dev_cache.to_csv(CACHE_ROOT / \"t2_dev_probs.csv\", index=False)\n",
    "\n",
    "print(\"Saved T2 train/dev probabilities for ensembling in:\", CACHE_ROOT)\n",
    "\n",
    "# 3.6 Save model + calibration\n",
    "mdl_t2_final.save_pretrained(ART_ROOT / \"native_t2\")\n",
    "tok_t2.save_pretrained(ART_ROOT / \"native_t2\")\n",
    "with open(ART_ROOT / \"calib_t2_native.json\", \"w\") as f:\n",
    "    json.dump({\"temperature\": float(T_t2), \"thresholds\": thr_map_t2}, f, indent=2)\n",
    "\n",
    "# 3.7 Codabench submission CSV (required header order)\n",
    "idx_gender    = T2_LABELS.index(\"gender/sexual\")\n",
    "idx_political = T2_LABELS.index(\"political\")\n",
    "idx_religious = T2_LABELS.index(\"religious\")\n",
    "idx_racial    = T2_LABELS.index(\"racial/ethnic\")\n",
    "idx_other     = T2_LABELS.index(\"other\")\n",
    "\n",
    "sub2 = pd.DataFrame({\n",
    "    \"id\":            t2_dev_df[\"id\"].astype(str).values,\n",
    "    \"political\":     P2_dev[:, idx_political],\n",
    "    \"racial/ethnic\": P2_dev[:, idx_racial],\n",
    "    \"religious\":     P2_dev[:, idx_religious],\n",
    "    \"gender/sexual\": P2_dev[:, idx_gender],\n",
    "    \"other\":         P2_dev[:, idx_other],\n",
    "})\n",
    "sub2_path = SUB_ROOT / \"subtask_2\" / f\"pred_{lang_fname}.csv\"\n",
    "sub2.to_csv(sub2_path, index=False)\n",
    "print(\"Wrote Subtask 2 submission CSV:\", sub2_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1a0469",
   "metadata": {},
   "source": [
    "## Subtask 3 (multi-label, 6) with focal loss + K-fold calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d1cc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T3] TRAIN size: 5572\n",
      "[T3] DEV size (unlabeled): 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[T3] Fold 1/3 — train=3714, val=1858\n",
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1395' max='1395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1395/1395 01:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.339200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.350700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.337600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.333400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.323900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.323700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.329700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.322100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.300300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.327400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.279600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.277300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.292800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.304300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.262400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.259100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.279100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.265200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.267300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.262300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (thr=0.5): 0.5797554239566807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[T3] Fold 2/3 — train=3715, val=1857\n",
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1395' max='1395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1395/1395 01:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.333400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.341700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.346800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.343700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.341300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.328400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.327900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.325500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.314900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.308200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.295900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.315800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.296700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.296600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.300400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.292900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.293900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.287400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.261400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.263500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.273500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.260800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (thr=0.5): 0.5836855865536099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[T3] Fold 3/3 — train=3715, val=1857\n",
      "  Trainer device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1395' max='1395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1395/1395 01:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.345600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.335300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.336700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.342200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.328000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.308500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.321300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.308500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.311000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.301600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.296200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.293100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.283600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.285100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.292300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.256700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.270100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.260800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.254600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.256400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.257300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold Macro-F1 (thr=0.5): 0.5945626736203286\n",
      "[TempScale] base_F1=0.5856, calibrated_F1=0.5856, T=0.8326\n",
      "\n",
      "[T3] Calibration (OOF):\n",
      "  Temperature: 0.832567036151886\n",
      "  Thresholds: {'vilification': 0.49999999999999994, 'extreme_language': 0.44999999999999996, 'stereotype': 0.6, 'invalidation': 0.49999999999999994, 'lack_of_empathy': 0.5499999999999999, 'dehumanization': 0.5499999999999999}\n",
      "  Macro-F1 (OOF, calibrated): 0.5930747268985176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[T3] Training FINAL model on full train...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2091' max='2091' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2091/2091 01:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.338800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.324800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.325100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.317900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.313800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.319600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.309500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.301500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.280200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.282700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.285600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.268300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.281100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.252800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.258900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.253600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.250300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.251200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.261600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.273200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T3] Macro-F1 (TRAIN, thr=0.5, final model): 0.6572399484627239\n",
      "[T3] Macro-F1 (TRAIN, calibrated T+thr, final model): 0.6740981764510829\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved T3 train/dev probabilities for ensembling in: cache/xlmr_cv/eng\n",
      "Wrote Subtask 3 submission CSV: submissions/xlmr/subtask_3/pred_eng.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4) SUBTASK 3 — Manifestation (multi-label, 6 labels)\n",
    "#    Focal BCE + K-fold OOF calibration + dev submission\n",
    "# ============================================================\n",
    "\n",
    "# 4.1 Load TRAIN + DEV\n",
    "t3_train_df = pd.read_csv(T3_TRAIN)\n",
    "t3_dev_df   = pd.read_csv(T3_DEV)\n",
    "\n",
    "required_train_cols_t3 = {\"id\", \"text\", *T3_LABELS}\n",
    "required_dev_cols_t3   = {\"id\", \"text\"}\n",
    "assert required_train_cols_t3.issubset(t3_train_df.columns), \\\n",
    "    f\"T3 TRAIN missing: {required_train_cols_t3 - set(t3_train_df.columns)}\"\n",
    "assert required_dev_cols_t3.issubset(t3_dev_df.columns), \\\n",
    "    f\"T3 DEV missing: {required_dev_cols_t3 - set(t3_dev_df.columns)}\"\n",
    "\n",
    "Y3_train = t3_train_df[T3_LABELS].values.astype(int)\n",
    "print(f\"[T3] TRAIN size: {len(t3_train_df)}\")\n",
    "print(f\"[T3] DEV size (unlabeled): {len(t3_dev_df)}\")\n",
    "\n",
    "# stratification: any manifestation vs none\n",
    "y3_strat = (Y3_train.sum(axis=1) > 0).astype(int)\n",
    "folds_t3 = make_stratified_folds(y3_strat, n_splits=N_FOLDS, seed=SEED)\n",
    "\n",
    "tok_t3 = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "pos_count_3 = Y3_train.sum(axis=0) + 1e-6\n",
    "neg_count_3 = Y3_train.shape[0] - pos_count_3\n",
    "pos_weight_3 = torch.tensor(neg_count_3 / pos_count_3, dtype=torch.float)\n",
    "\n",
    "oof_logits_t3 = np.zeros((len(t3_train_df), len(T3_LABELS)), dtype=np.float32)\n",
    "oof_labels_t3 = Y3_train.copy()\n",
    "\n",
    "class FocalTrainerT3(Trainer):\n",
    "    def __init__(self, *args, pos_weight=None, gamma=1.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pos_weight = pos_weight\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = FocalBCEWithLogitsLoss(\n",
    "            alpha=self.pos_weight.to(logits.device),\n",
    "            gamma=self.gamma,\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "        loss = loss_fct(logits, labels.to(logits.device).float())\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(folds_t3):\n",
    "    print(f\"\\n[T3] Fold {fold+1}/{N_FOLDS} — train={len(tr_idx)}, val={len(val_idx)}\")\n",
    "\n",
    "    ds_tr = TextClsDataset(\n",
    "        texts=t3_train_df[\"text\"].iloc[tr_idx].tolist(),\n",
    "        labels=Y3_train[tr_idx].tolist(),\n",
    "        tokenizer=tok_t3,\n",
    "        max_len=MAX_LEN,\n",
    "        is_multilabel=True,\n",
    "    )\n",
    "    ds_val = TextClsDataset(\n",
    "        texts=t3_train_df[\"text\"].iloc[val_idx].tolist(),\n",
    "        labels=Y3_train[val_idx].tolist(),\n",
    "        tokenizer=tok_t3,\n",
    "        max_len=MAX_LEN,\n",
    "        is_multilabel=True,\n",
    "    )\n",
    "\n",
    "    cfg_t3_fold = AutoConfig.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(T3_LABELS),\n",
    "        problem_type=\"multi_label_classification\",\n",
    "    )\n",
    "    mdl_t3_fold = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=cfg_t3_fold)\n",
    "    mdl_t3_fold.config.use_cache = False\n",
    "    if hasattr(mdl_t3_fold, \"gradient_checkpointing_enable\"):\n",
    "        mdl_t3_fold.gradient_checkpointing_enable()\n",
    "    mdl_t3_fold.to(DEVICE)\n",
    "\n",
    "    args_t3_fold = build_training_args(\n",
    "        output_dir=ART_ROOT / f\"t3_cv_fold{fold+1}\",\n",
    "        per_device_train_batch_size=BATCH_TRAIN,\n",
    "        per_device_eval_batch_size=BATCH_EVAL,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        logging_steps=50,\n",
    "        evaluation=\"epoch\",\n",
    "        save=\"no\",\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "    )\n",
    "\n",
    "    def compute_metrics_t3_fold(eval_pred):\n",
    "        logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, (list, tuple)) else eval_pred.predictions\n",
    "        labels = eval_pred.label_ids\n",
    "        probs  = 1.0 / (1.0 + np.exp(-logits))\n",
    "        preds  = (probs >= 0.5).astype(int)\n",
    "        return {\"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0)}\n",
    "\n",
    "    trainer_t3_fold = FocalTrainerT3(\n",
    "        model=mdl_t3_fold,\n",
    "        args=args_t3_fold,\n",
    "        train_dataset=ds_tr,\n",
    "        eval_dataset=ds_val,\n",
    "        tokenizer=tok_t3,\n",
    "        data_collator=DataCollatorWithPadding(tok_t3),\n",
    "        compute_metrics=compute_metrics_t3_fold,\n",
    "        pos_weight=pos_weight_3,\n",
    "        gamma=1.5,\n",
    "    )\n",
    "    print(\"  Trainer device:\", trainer_t3_fold.args.device)\n",
    "\n",
    "    trainer_t3_fold.train()\n",
    "    eval_fold = trainer_t3_fold.evaluate()\n",
    "    print(\"  Fold Macro-F1 (thr=0.5):\", eval_fold.get(\"eval_f1_macro\"))\n",
    "\n",
    "    logits_val, labels_val = collect_logits(trainer_t3_fold, ds_val, is_multilabel=True)\n",
    "    oof_logits_t3[val_idx] = logits_val.numpy()\n",
    "\n",
    "    del trainer_t3_fold, mdl_t3_fold\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# 4.2 Calibrate on OOF logits\n",
    "logits_oof_t3 = torch.from_numpy(oof_logits_t3)\n",
    "labels_oof_t3 = torch.from_numpy(oof_labels_t3)\n",
    "\n",
    "T_t3 = learn_temperature(logits_oof_t3, labels_oof_t3, is_multilabel=True)\n",
    "probs_oof_t3 = torch.sigmoid(logits_oof_t3 / T_t3).cpu().numpy()\n",
    "thr_map_t3 = grid_search_thresholds(labels_oof_t3.numpy(), probs_oof_t3, T3_LABELS)\n",
    "\n",
    "P3_oof = np.zeros_like(probs_oof_t3, dtype=int)\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    thr = float(thr_map_t3[lab])\n",
    "    P3_oof[:, j] = (probs_oof_t3[:, j] >= thr).astype(int)\n",
    "f1_oof_t3 = f1_score(labels_oof_t3.numpy(), P3_oof, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"\\n[T3] Calibration (OOF):\")\n",
    "print(\"  Temperature:\", T_t3)\n",
    "print(\"  Thresholds:\", thr_map_t3)\n",
    "print(\"  Macro-F1 (OOF, calibrated):\", f1_oof_t3)\n",
    "\n",
    "# 4.3 Train FINAL model on full TRAIN\n",
    "cfg_t3_final = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(T3_LABELS),\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")\n",
    "mdl_t3_final = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=cfg_t3_final)\n",
    "mdl_t3_final.config.use_cache = False\n",
    "if hasattr(mdl_t3_final, \"gradient_checkpointing_enable\"):\n",
    "    mdl_t3_final.gradient_checkpointing_enable()\n",
    "mdl_t3_final.to(DEVICE)\n",
    "\n",
    "ds_t3_train_full = TextClsDataset(\n",
    "    texts=t3_train_df[\"text\"].tolist(),\n",
    "    labels=Y3_train.tolist(),\n",
    "    tokenizer=tok_t3,\n",
    "    max_len=MAX_LEN,\n",
    "    is_multilabel=True,\n",
    ")\n",
    "dummy_labels_t3_dev = np.zeros((len(t3_dev_df), len(T3_LABELS)), dtype=int)\n",
    "ds_t3_dev_full = TextClsDataset(\n",
    "    texts=t3_dev_df[\"text\"].tolist(),\n",
    "    labels=dummy_labels_t3_dev.tolist(),\n",
    "    tokenizer=tok_t3,\n",
    "    max_len=MAX_LEN,\n",
    "    is_multilabel=True,\n",
    ")\n",
    "\n",
    "args_t3_final = build_training_args(\n",
    "    output_dir=ART_ROOT / \"t3_final\",\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=50,\n",
    "    evaluation=\"epoch\",\n",
    "    save=\"no\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    ")\n",
    "\n",
    "def compute_metrics_t3_final(eval_pred):\n",
    "    logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, (list, tuple)) else eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    probs  = 1.0 / (1.0 + np.exp(-logits))\n",
    "    preds  = (probs >= 0.5).astype(int)\n",
    "    return {\"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0)}\n",
    "\n",
    "trainer_t3_final = FocalTrainerT3(\n",
    "    model=mdl_t3_final,\n",
    "    args=args_t3_final,\n",
    "    train_dataset=ds_t3_train_full,\n",
    "    eval_dataset=ds_t3_train_full,\n",
    "    tokenizer=tok_t3,\n",
    "    data_collator=DataCollatorWithPadding(tok_t3),\n",
    "    compute_metrics=compute_metrics_t3_final,\n",
    "    pos_weight=pos_weight_3,\n",
    "    gamma=1.5,\n",
    ")\n",
    "\n",
    "print(\"\\n[T3] Training FINAL model on full train...\")\n",
    "trainer_t3_final.train()\n",
    "eval_t3_train_full = trainer_t3_final.evaluate()\n",
    "print(\"[T3] Macro-F1 (TRAIN, thr=0.5, final model):\", eval_t3_train_full.get(\"eval_f1_macro\"))\n",
    "\n",
    "# Train-set calibrated metrics\n",
    "logits_t3_train_full, labels_t3_train_full = collect_logits(trainer_t3_final, ds_t3_train_full, is_multilabel=True)\n",
    "probs_t3_train_full = torch.sigmoid(logits_t3_train_full / T_t3).cpu().numpy()\n",
    "P3_train_full = np.zeros_like(probs_t3_train_full, dtype=int)\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    thr = float(thr_map_t3[lab])\n",
    "    P3_train_full[:, j] = (probs_t3_train_full[:, j] >= thr).astype(int)\n",
    "train_f1_calib_t3 = f1_score(labels_t3_train_full.numpy(), P3_train_full, average=\"macro\", zero_division=0)\n",
    "print(\"[T3] Macro-F1 (TRAIN, calibrated T+thr, final model):\", train_f1_calib_t3)\n",
    "\n",
    "# 4.4 Inference on DEV\n",
    "preds_dev_t3 = trainer_t3_final.predict(ds_t3_dev_full)\n",
    "logits_t3_dev = torch.tensor(preds_dev_t3.predictions if not isinstance(preds_dev_t3.predictions,(list,tuple)) else preds_dev_t3.predictions[0])\n",
    "probs_t3_dev = torch.sigmoid(logits_t3_dev / T_t3).cpu().numpy()\n",
    "\n",
    "P3_dev = np.zeros_like(probs_t3_dev, dtype=int)\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    thr = float(thr_map_t3[lab])\n",
    "    P3_dev[:, j] = (probs_t3_dev[:, j] >= thr).astype(int)\n",
    "\n",
    "# 4.5 Cache train/dev probs for ensembling\n",
    "cache_cols_train_t3 = {\"id\": t3_train_df[\"id\"].astype(str).values}\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    cache_cols_train_t3[f\"prob_{lab}\"] = probs_t3_train_full[:, j]\n",
    "    cache_cols_train_t3[f\"label_{lab}\"] = labels_t3_train_full.numpy()[:, j]\n",
    "\n",
    "t3_train_cache = pd.DataFrame(cache_cols_train_t3)\n",
    "t3_train_cache.to_csv(CACHE_ROOT / \"t3_train_probs.csv\", index=False)\n",
    "\n",
    "cache_cols_dev_t3 = {\"id\": t3_dev_df[\"id\"].astype(str).values}\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    cache_cols_dev_t3[f\"prob_{lab}\"] = probs_t3_dev[:, j]\n",
    "t3_dev_cache = pd.DataFrame(cache_cols_dev_t3)\n",
    "t3_dev_cache.to_csv(CACHE_ROOT / \"t3_dev_probs.csv\", index=False)\n",
    "\n",
    "print(\"Saved T3 train/dev probabilities for ensembling in:\", CACHE_ROOT)\n",
    "\n",
    "# 4.6 Save model + calibration\n",
    "mdl_t3_final.save_pretrained(ART_ROOT / \"native_t3\")\n",
    "tok_t3.save_pretrained(ART_ROOT / \"native_t3\")\n",
    "with open(ART_ROOT / \"calib_t3_native.json\", \"w\") as f:\n",
    "    json.dump({\"temperature\": float(T_t3), \"thresholds\": thr_map_t3}, f, indent=2)\n",
    "\n",
    "# 4.7 Codabench submission CSV (required header order)\n",
    "idx_vil      = T3_LABELS.index(\"vilification\")\n",
    "idx_extreme  = T3_LABELS.index(\"extreme_language\")\n",
    "idx_stereo   = T3_LABELS.index(\"stereotype\")\n",
    "idx_invalid  = T3_LABELS.index(\"invalidation\")\n",
    "idx_lackemp  = T3_LABELS.index(\"lack_of_empathy\")\n",
    "idx_dehum    = T3_LABELS.index(\"dehumanization\")\n",
    "\n",
    "sub3 = pd.DataFrame({\n",
    "    \"id\":               t3_dev_df[\"id\"].astype(str).values,\n",
    "    \"stereotype\":       P3_dev[:, idx_stereo],\n",
    "    \"vilification\":     P3_dev[:, idx_vil],\n",
    "    \"dehumanization\":   P3_dev[:, idx_dehum],\n",
    "    \"extreme_language\": P3_dev[:, idx_extreme],\n",
    "    \"lack_of_empathy\":  P3_dev[:, idx_lackemp],\n",
    "    \"invalidation\":     P3_dev[:, idx_invalid],\n",
    "})\n",
    "sub3_path = SUB_ROOT / \"subtask_3\" / f\"pred_{lang_fname}.csv\"\n",
    "sub3.to_csv(sub3_path, index=False)\n",
    "print(\"Wrote Subtask 3 submission CSV:\", sub3_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
