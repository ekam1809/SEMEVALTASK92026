{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a29c90b4",
   "metadata": {},
   "source": [
    "## Setup, paths, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c1b960b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANG=eng\n",
      "XLM-R cache root   : cache/xlmr_cv/eng\n",
      "DeBERTa cache root : cache/deberta_cv/eng\n",
      "Outputs root       : outputs/ensemble_cv/eng\n",
      "Submissions root   : submissions/ensemble_cv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0) SETUP: imports, config, paths\n",
    "#    Ensembles XLM-R (native) + DeBERTa (MT→EN)\n",
    "#    using calibrated probabilities from CV notebooks\n",
    "# ============================================================\n",
    "\n",
    "import os, json, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Language + data root\n",
    "# ------------------------------------------------------------\n",
    "LANG = \"eng\"                # e.g. \"eng\", \"ben\", \"hin\"\n",
    "BASE = \"../dev_phase_aug\"       # organizer data root\n",
    "\n",
    "# Train/Dev CSVs (for id order + labels)\n",
    "T1_TRAIN = f\"{BASE}/subtask1/train/{LANG}.csv\"\n",
    "T1_DEV   = f\"{BASE}/subtask1/dev/{LANG}.csv\"\n",
    "\n",
    "T2_TRAIN = f\"{BASE}/subtask2/train/{LANG}.csv\"\n",
    "T2_DEV   = f\"{BASE}/subtask2/dev/{LANG}.csv\"\n",
    "\n",
    "T3_TRAIN = f\"{BASE}/subtask3/train/{LANG}.csv\"\n",
    "T3_DEV   = f\"{BASE}/subtask3/dev/{LANG}.csv\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Paths for method2 CV runs\n",
    "# ------------------------------------------------------------\n",
    "XLMR_CACHE_ROOT   = Path(\"cache\")     / \"xlmr_cv\"     / LANG\n",
    "DEBERTA_CACHE_ROOT = Path(\"cache\")    / \"deberta_cv\"  / LANG\n",
    "\n",
    "XLMR_ART_ROOT     = Path(\"artifacts\") / \"xlmr_cv\"     / LANG\n",
    "DEBERTA_ART_ROOT  = Path(\"artifacts\") / \"deberta_cv\"  / LANG\n",
    "\n",
    "OUT_ROOT          = Path(\"outputs\")   / \"ensemble_cv\" / LANG\n",
    "SUB_ROOT          = Path(\"submissions\") / \"ensemble_cv\"\n",
    "\n",
    "for d in [OUT_ROOT, SUB_ROOT]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Submission subfolders\n",
    "(SUB_ROOT / \"subtask_1\").mkdir(parents=True, exist_ok=True)\n",
    "(SUB_ROOT / \"subtask_2\").mkdir(parents=True, exist_ok=True)\n",
    "(SUB_ROOT / \"subtask_3\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Label orders (same as training notebooks)\n",
    "# ------------------------------------------------------------\n",
    "T2_LABELS = [\"gender/sexual\", \"political\", \"religious\", \"racial/ethnic\", \"other\"]\n",
    "T3_LABELS = [\"vilification\", \"extreme_language\", \"stereotype\",\n",
    "             \"invalidation\", \"lack_of_empathy\", \"dehumanization\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helpers\n",
    "# ------------------------------------------------------------\n",
    "def macro_f1(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "def grid_search_thresholds(y_true, y_prob, label_names=None):\n",
    "    \"\"\"\n",
    "    For multi-label: per-label threshold search.\n",
    "    y_true: [N, C], y_prob: [N, C]\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_prob = np.asarray(y_prob)\n",
    "    C = y_true.shape[1]\n",
    "    grid = np.linspace(0.05, 0.95, 19)\n",
    "    thrs = {}\n",
    "    for c in range(C):\n",
    "        best_t, best_f = 0.5, -1.0\n",
    "        for t in grid:\n",
    "            preds = (y_prob[:, c] >= t).astype(int)\n",
    "            f = f1_score(y_true[:, c], preds, average=\"binary\", zero_division=0)\n",
    "            if f > best_f:\n",
    "                best_f, best_t = f, t\n",
    "        name = label_names[c] if label_names else str(c)\n",
    "        thrs[name] = float(best_t)\n",
    "    return thrs\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "print(f\"LANG={LANG}\")\n",
    "print(\"XLM-R cache root   :\", XLMR_CACHE_ROOT)\n",
    "print(\"DeBERTa cache root :\", DEBERTA_CACHE_ROOT)\n",
    "print(\"Outputs root       :\", OUT_ROOT)\n",
    "print(\"Submissions root   :\", SUB_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1961648a",
   "metadata": {},
   "source": [
    "## Subtask 1 ensemble (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f92bd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T1] TRAIN size: 5572\n",
      "[T1] DEV size  : 160\n",
      "\n",
      "[T1] TRAIN macro-F1 comparison:\n",
      "  XLM-R    @thr_native : 0.9321058071750241\n",
      "  DeBERTa  @thr_native : 0.9833770214149999\n",
      "  Ensemble @thr_ens=0.50 : 0.975692726750826\n",
      "Saved T1 debug ensemble file: outputs/ensemble_cv/eng/t1_train_ensemble_debug.xlsx\n",
      "Saved Subtask 1 ensemble submission: submissions/ensemble_cv/subtask_1/pred_eng.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1) SUBTASK 1 — Polarization (binary) ENSEMBLE\n",
    "#    Uses calibrated probs from:\n",
    "#      - cache/xlmr_cv/{LANG}/t1_train_probs.csv\n",
    "#      - cache/deberta_cv/{LANG}/t1_train_probs.csv\n",
    "# ============================================================\n",
    "\n",
    "# 1.1 Load train CSV for labels + id order\n",
    "t1_train_df = pd.read_csv(T1_TRAIN)\n",
    "t1_dev_df   = pd.read_csv(T1_DEV)\n",
    "\n",
    "required_train_cols_t1 = {\"id\", \"text\", \"polarization\"}\n",
    "required_dev_cols_t1   = {\"id\", \"text\"}\n",
    "assert required_train_cols_t1.issubset(t1_train_df.columns), \\\n",
    "    f\"T1 TRAIN missing: {required_train_cols_t1 - set(t1_train_df.columns)}\"\n",
    "assert required_dev_cols_t1.issubset(t1_dev_df.columns), \\\n",
    "    f\"T1 DEV missing: {required_dev_cols_t1 - set(t1_dev_df.columns)}\"\n",
    "\n",
    "t1_train_df[\"id\"] = t1_train_df[\"id\"].astype(str)\n",
    "t1_dev_df[\"id\"]   = t1_dev_df[\"id\"].astype(str)\n",
    "t1_train_df[\"polarization\"] = t1_train_df[\"polarization\"].astype(int)\n",
    "y_true_t1 = t1_train_df[\"polarization\"].values\n",
    "\n",
    "print(\"[T1] TRAIN size:\", len(t1_train_df))\n",
    "print(\"[T1] DEV size  :\", len(t1_dev_df))\n",
    "\n",
    "# 1.2 Load prob caches\n",
    "t1_train_x = pd.read_csv(XLMR_CACHE_ROOT   / \"t1_train_probs.csv\")\n",
    "t1_train_d = pd.read_csv(DEBERTA_CACHE_ROOT / \"t1_train_probs.csv\")\n",
    "\n",
    "t1_train_x[\"id\"] = t1_train_x[\"id\"].astype(str)\n",
    "t1_train_d[\"id\"] = t1_train_d[\"id\"].astype(str)\n",
    "\n",
    "t1_train_merged = (\n",
    "    t1_train_df[[\"id\", \"polarization\"]]\n",
    "    .merge(t1_train_x, on=\"id\", how=\"left\", suffixes=(\"\", \"_xlmr\"))\n",
    "    .merge(t1_train_d, on=\"id\", how=\"left\", suffixes=(\"_xlmr\", \"_deberta\"))\n",
    ")\n",
    "\n",
    "assert t1_train_merged[\"prob_pos_xlmr\"].notna().all(), \"Missing XLM-R probs for some train rows\"\n",
    "assert t1_train_merged[\"prob_pos_deberta\"].notna().all(), \"Missing DeBERTa probs for some train rows\"\n",
    "\n",
    "# 1.3 Load calibration thresholds for individual models (optional, for metrics)\n",
    "cal_t1_x = load_json(XLMR_ART_ROOT    / \"calib_t1_native.json\")\n",
    "cal_t1_d = load_json(DEBERTA_ART_ROOT / \"calib_t1_native.json\")\n",
    "\n",
    "thr_t1_x = float(cal_t1_x[\"threshold\"])\n",
    "thr_t1_d = float(cal_t1_d[\"threshold\"])\n",
    "\n",
    "p_x = t1_train_merged[\"prob_pos_xlmr\"].values\n",
    "p_d = t1_train_merged[\"prob_pos_deberta\"].values\n",
    "p_ens = 0.5 * (p_x + p_d)\n",
    "\n",
    "# 1.4 Metrics: XLM-R vs DeBERTa vs Ensemble\n",
    "pred_x = (p_x >= thr_t1_x).astype(int)\n",
    "pred_d = (p_d >= thr_t1_d).astype(int)\n",
    "\n",
    "best_thr_ens, best_f1_ens = 0.5, -1.0\n",
    "for t in np.linspace(0.05, 0.95, 19):\n",
    "    pred_e = (p_ens >= t).astype(int)\n",
    "    f = macro_f1(y_true_t1, pred_e)\n",
    "    if f > best_f1_ens:\n",
    "        best_f1_ens, best_thr_ens = f, t\n",
    "\n",
    "print(\"\\n[T1] TRAIN macro-F1 comparison:\")\n",
    "print(\"  XLM-R    @thr_native :\", macro_f1(y_true_t1, pred_x))\n",
    "print(\"  DeBERTa  @thr_native :\", macro_f1(y_true_t1, pred_d))\n",
    "print(f\"  Ensemble @thr_ens={best_thr_ens:.2f} :\", best_f1_ens)\n",
    "\n",
    "# 1.5 Save ensemble calibration info\n",
    "calib_t1_ens = {\n",
    "    \"threshold\": float(best_thr_ens),\n",
    "    \"note\": \"Threshold chosen on train using averaged calibrated probs (XLM-R + DeBERTa).\",\n",
    "}\n",
    "with open(OUT_ROOT / \"calib_t1_ensemble.json\", \"w\") as f:\n",
    "    json.dump(calib_t1_ens, f, indent=2)\n",
    "\n",
    "# 1.6 Debug dump (optional)\n",
    "t1_debug = t1_train_merged[[\"id\", \"polarization\", \"prob_pos_xlmr\", \"prob_pos_deberta\"]].copy()\n",
    "t1_debug[\"prob_pos_ens\"] = p_ens\n",
    "t1_debug.to_excel(OUT_ROOT / \"t1_train_ensemble_debug.xlsx\", index=False)\n",
    "print(\"Saved T1 debug ensemble file:\", OUT_ROOT / \"t1_train_ensemble_debug.xlsx\")\n",
    "\n",
    "# 1.7 Ensemble predictions for DEV\n",
    "t1_dev_x = pd.read_csv(XLMR_CACHE_ROOT / \"t1_dev_probs.csv\")\n",
    "t1_dev_d = pd.read_csv(DEBERTA_CACHE_ROOT / \"t1_dev_probs.csv\")\n",
    "\n",
    "t1_dev_x[\"id\"] = t1_dev_x[\"id\"].astype(str)\n",
    "t1_dev_d[\"id\"] = t1_dev_d[\"id\"].astype(str)\n",
    "\n",
    "t1_dev_merged = (\n",
    "    t1_dev_df[[\"id\"]]\n",
    "    .merge(t1_dev_x, on=\"id\", how=\"left\", suffixes=(\"\", \"_xlmr\"))\n",
    "    .merge(t1_dev_d, on=\"id\", how=\"left\", suffixes=(\"_xlmr\", \"_deberta\"))\n",
    ")\n",
    "\n",
    "assert t1_dev_merged[\"prob_pos_xlmr\"].notna().all(), \"Missing XLM-R probs for some dev rows\"\n",
    "assert t1_dev_merged[\"prob_pos_deberta\"].notna().all(), \"Missing DeBERTa probs for some dev rows\"\n",
    "\n",
    "p_x_dev = t1_dev_merged[\"prob_pos_xlmr\"].values\n",
    "p_d_dev = t1_dev_merged[\"prob_pos_deberta\"].values\n",
    "p_ens_dev = 0.5 * (p_x_dev + p_d_dev)\n",
    "pred_ens_dev = (p_ens_dev >= best_thr_ens).astype(int)\n",
    "\n",
    "# 1.8 Codabench submission CSV\n",
    "sub1 = pd.DataFrame({\n",
    "    \"id\": t1_dev_merged[\"id\"].astype(str),\n",
    "    \"polarization\": pred_ens_dev.astype(int),\n",
    "})\n",
    "sub1_path = SUB_ROOT / \"subtask_1\" / f\"pred_{LANG}.csv\"\n",
    "sub1.to_csv(sub1_path, index=False)\n",
    "print(\"Saved Subtask 1 ensemble submission:\", sub1_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed4f99b",
   "metadata": {},
   "source": [
    "## Subtask 2 ensemble (multi-label 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9355a98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T2] TRAIN size: 5572\n",
      "[T2] DEV size  : 160\n",
      "\n",
      "[T2] TRAIN macro-F1 comparison:\n",
      "  XLM-R    (calibrated) : 0.7591862707286214\n",
      "  DeBERTa  (calibrated) : 0.7879986535787913\n",
      "  Ensemble (calibrated) : 0.8157219982846303\n",
      "  Ensemble thresholds   : {'gender/sexual': 0.75, 'political': 0.44999999999999996, 'religious': 0.75, 'racial/ethnic': 0.7, 'other': 0.75}\n",
      "Saved T2 debug ensemble file: outputs/ensemble_cv/eng/t2_train_ensemble_debug.xlsx\n",
      "Saved Subtask 2 ensemble submission: submissions/ensemble_cv/subtask_2/pred_eng.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2) SUBTASK 2 — Hate type (5 labels) ENSEMBLE\n",
    "# ============================================================\n",
    "\n",
    "# 2.1 Load train + dev IDs\n",
    "t2_train_df = pd.read_csv(T2_TRAIN)\n",
    "t2_dev_df   = pd.read_csv(T2_DEV)\n",
    "\n",
    "required_train_cols_t2 = {\"id\", \"text\", *T2_LABELS}\n",
    "required_dev_cols_t2   = {\"id\", \"text\"}\n",
    "assert required_train_cols_t2.issubset(t2_train_df.columns), \\\n",
    "    f\"T2 TRAIN missing: {required_train_cols_t2 - set(t2_train_df.columns)}\"\n",
    "assert required_dev_cols_t2.issubset(t2_dev_df.columns), \\\n",
    "    f\"T2 DEV missing: {required_dev_cols_t2 - set(t2_dev_df.columns)}\"\n",
    "\n",
    "t2_train_df[\"id\"] = t2_train_df[\"id\"].astype(str)\n",
    "t2_dev_df[\"id\"]   = t2_dev_df[\"id\"].astype(str)\n",
    "\n",
    "Y2_true = t2_train_df[T2_LABELS].values.astype(int)\n",
    "\n",
    "print(\"[T2] TRAIN size:\", len(t2_train_df))\n",
    "print(\"[T2] DEV size  :\", len(t2_dev_df))\n",
    "\n",
    "# 2.2 Load prob caches\n",
    "t2_train_x = pd.read_csv(XLMR_CACHE_ROOT   / \"t2_train_probs.csv\")\n",
    "t2_train_d = pd.read_csv(DEBERTA_CACHE_ROOT / \"t2_train_probs.csv\")\n",
    "\n",
    "t2_train_x[\"id\"] = t2_train_x[\"id\"].astype(str)\n",
    "t2_train_d[\"id\"] = t2_train_d[\"id\"].astype(str)\n",
    "\n",
    "t2_train_merged = (\n",
    "    t2_train_df[[\"id\"]]\n",
    "    .merge(t2_train_x, on=\"id\", how=\"left\", suffixes=(\"\", \"_xlmr\"))\n",
    "    .merge(t2_train_d, on=\"id\", how=\"left\", suffixes=(\"_xlmr\", \"_deberta\"))\n",
    ")\n",
    "\n",
    "# sanity check presence\n",
    "for lab in T2_LABELS:\n",
    "    assert t2_train_merged[f\"prob_{lab}_xlmr\"].notna().all(), f\"Missing XLM-R prob_{lab}\"\n",
    "    assert t2_train_merged[f\"prob_{lab}_deberta\"].notna().all(), f\"Missing DeBERTa prob_{lab}\"\n",
    "\n",
    "# 2.3 Load individual model calibration thresholds\n",
    "cal_t2_x = load_json(XLMR_ART_ROOT    / \"calib_t2_native.json\")\n",
    "cal_t2_d = load_json(DEBERTA_ART_ROOT / \"calib_t2_native.json\")\n",
    "thr_map_t2_x = cal_t2_x[\"thresholds\"]\n",
    "thr_map_t2_d = cal_t2_d[\"thresholds\"]\n",
    "\n",
    "# 2.4 Build prob matrices\n",
    "N = len(t2_train_merged)\n",
    "C = len(T2_LABELS)\n",
    "P_x_train   = np.zeros((N, C), dtype=np.float32)\n",
    "P_d_train   = np.zeros((N, C), dtype=np.float32)\n",
    "P_ens_train = np.zeros((N, C), dtype=np.float32)\n",
    "\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    P_x_train[:, j]   = t2_train_merged[f\"prob_{lab}_xlmr\"].values\n",
    "    P_d_train[:, j]   = t2_train_merged[f\"prob_{lab}_deberta\"].values\n",
    "    P_ens_train[:, j] = 0.5 * (P_x_train[:, j] + P_d_train[:, j])\n",
    "\n",
    "# 2.5 XLM-R, DeBERTa, Ensemble F1\n",
    "P2_x = np.zeros_like(P_x_train, dtype=int)\n",
    "P2_d = np.zeros_like(P_d_train, dtype=int)\n",
    "\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    thr_x = float(thr_map_t2_x[lab])\n",
    "    thr_d = float(thr_map_t2_d[lab])\n",
    "    P2_x[:, j] = (P_x_train[:, j] >= thr_x).astype(int)\n",
    "    P2_d[:, j] = (P_d_train[:, j] >= thr_d).astype(int)\n",
    "\n",
    "f1_x = f1_score(Y2_true, P2_x, average=\"macro\", zero_division=0)\n",
    "f1_d = f1_score(Y2_true, P2_d, average=\"macro\", zero_division=0)\n",
    "\n",
    "# ensemble thresholds via grid search on P_ens_train\n",
    "thr_map_t2_ens = grid_search_thresholds(Y2_true, P_ens_train, T2_LABELS)\n",
    "\n",
    "P2_ens = np.zeros_like(P_ens_train, dtype=int)\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    thr_e = float(thr_map_t2_ens[lab])\n",
    "    P2_ens[:, j] = (P_ens_train[:, j] >= thr_e).astype(int)\n",
    "\n",
    "f1_ens = f1_score(Y2_true, P2_ens, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"\\n[T2] TRAIN macro-F1 comparison:\")\n",
    "print(\"  XLM-R    (calibrated) :\", f1_x)\n",
    "print(\"  DeBERTa  (calibrated) :\", f1_d)\n",
    "print(\"  Ensemble (calibrated) :\", f1_ens)\n",
    "print(\"  Ensemble thresholds   :\", thr_map_t2_ens)\n",
    "\n",
    "# 2.6 Save ensemble calibration info\n",
    "with open(OUT_ROOT / \"calib_t2_ensemble.json\", \"w\") as f:\n",
    "    json.dump({\"thresholds\": thr_map_t2_ens}, f, indent=2)\n",
    "\n",
    "# 2.7 Debug Excel\n",
    "t2_debug_cols = {\"id\": t2_train_merged[\"id\"].astype(str).values}\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    t2_debug_cols[f\"prob_{lab}_xlmr\"]   = P_x_train[:, j]\n",
    "    t2_debug_cols[f\"prob_{lab}_deberta\"] = P_d_train[:, j]\n",
    "    t2_debug_cols[f\"prob_{lab}_ens\"]    = P_ens_train[:, j]\n",
    "t2_debug = pd.DataFrame(t2_debug_cols)\n",
    "t2_debug.to_excel(OUT_ROOT / \"t2_train_ensemble_debug.xlsx\", index=False)\n",
    "print(\"Saved T2 debug ensemble file:\", OUT_ROOT / \"t2_train_ensemble_debug.xlsx\")\n",
    "\n",
    "# 2.8 Ensemble predictions for DEV\n",
    "t2_dev_x = pd.read_csv(XLMR_CACHE_ROOT   / \"t2_dev_probs.csv\")\n",
    "t2_dev_d = pd.read_csv(DEBERTA_CACHE_ROOT / \"t2_dev_probs.csv\")\n",
    "\n",
    "t2_dev_x[\"id\"] = t2_dev_x[\"id\"].astype(str)\n",
    "t2_dev_d[\"id\"] = t2_dev_d[\"id\"].astype(str)\n",
    "\n",
    "t2_dev_merged = (\n",
    "    t2_dev_df[[\"id\"]]\n",
    "    .merge(t2_dev_x, on=\"id\", how=\"left\", suffixes=(\"\", \"_xlmr\"))\n",
    "    .merge(t2_dev_d, on=\"id\", how=\"left\", suffixes=(\"_xlmr\", \"_deberta\"))\n",
    ")\n",
    "\n",
    "N_dev = len(t2_dev_merged)\n",
    "P_ens_dev = np.zeros((N_dev, C), dtype=np.float32)\n",
    "\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    px = t2_dev_merged[f\"prob_{lab}_xlmr\"].values\n",
    "    pd_ = t2_dev_merged[f\"prob_{lab}_deberta\"].values\n",
    "    assert np.isfinite(px).all(), f\"Missing XLM-R dev probs for label {lab}\"\n",
    "    assert np.isfinite(pd_).all(), f\"Missing DeBERTa dev probs for label {lab}\"\n",
    "    P_ens_dev[:, j] = 0.5 * (px + pd_)\n",
    "\n",
    "P2_dev = np.zeros_like(P_ens_dev, dtype=int)\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    thr_e = float(thr_map_t2_ens[lab])\n",
    "    P2_dev[:, j] = (P_ens_dev[:, j] >= thr_e).astype(int)\n",
    "\n",
    "# 2.9 Codabench submission CSV (required header order)\n",
    "idx_gender    = T2_LABELS.index(\"gender/sexual\")\n",
    "idx_political = T2_LABELS.index(\"political\")\n",
    "idx_religious = T2_LABELS.index(\"religious\")\n",
    "idx_racial    = T2_LABELS.index(\"racial/ethnic\")\n",
    "idx_other     = T2_LABELS.index(\"other\")\n",
    "\n",
    "sub2 = pd.DataFrame({\n",
    "    \"id\":            t2_dev_merged[\"id\"].astype(str).values,\n",
    "    \"political\":     P2_dev[:, idx_political],\n",
    "    \"racial/ethnic\": P2_dev[:, idx_racial],\n",
    "    \"religious\":     P2_dev[:, idx_religious],\n",
    "    \"gender/sexual\": P2_dev[:, idx_gender],\n",
    "    \"other\":         P2_dev[:, idx_other],\n",
    "})\n",
    "sub2_path = SUB_ROOT / \"subtask_2\" / f\"pred_{LANG}.csv\"\n",
    "sub2.to_csv(sub2_path, index=False)\n",
    "print(\"Saved Subtask 2 ensemble submission:\", sub2_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a719d8b5",
   "metadata": {},
   "source": [
    "## Subtask 3 ensemble (multi-label 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c315ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T3] TRAIN size: 5572\n",
      "[T3] DEV size  : 160\n",
      "\n",
      "[T3] TRAIN macro-F1 comparison:\n",
      "  XLM-R    (calibrated) : 0.6740981764510829\n",
      "  DeBERTa  (calibrated) : 0.7587176504651337\n",
      "  Ensemble (calibrated) : 0.7640762401668121\n",
      "  Ensemble thresholds   : {'vilification': 0.49999999999999994, 'extreme_language': 0.5499999999999999, 'stereotype': 0.6, 'invalidation': 0.5499999999999999, 'lack_of_empathy': 0.65, 'dehumanization': 0.65}\n",
      "Saved T3 debug ensemble file: outputs/ensemble_cv/eng/t3_train_ensemble_debug.xlsx\n",
      "Saved Subtask 3 ensemble submission: submissions/ensemble_cv/subtask_3/pred_eng.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3) SUBTASK 3 — Manifestation (6 labels) ENSEMBLE\n",
    "# ============================================================\n",
    "\n",
    "# 3.1 Load train + dev IDs\n",
    "t3_train_df = pd.read_csv(T3_TRAIN)\n",
    "t3_dev_df   = pd.read_csv(T3_DEV)\n",
    "\n",
    "required_train_cols_t3 = {\"id\", \"text\", *T3_LABELS}\n",
    "required_dev_cols_t3   = {\"id\", \"text\"}\n",
    "assert required_train_cols_t3.issubset(t3_train_df.columns), \\\n",
    "    f\"T3 TRAIN missing: {required_train_cols_t3 - set(t3_train_df.columns)}\"\n",
    "assert required_dev_cols_t3.issubset(t3_dev_df.columns), \\\n",
    "    f\"T3 DEV missing: {required_dev_cols_t3 - set(t3_dev_df.columns)}\"\n",
    "\n",
    "t3_train_df[\"id\"] = t3_train_df[\"id\"].astype(str)\n",
    "t3_dev_df[\"id\"]   = t3_dev_df[\"id\"].astype(str)\n",
    "\n",
    "Y3_true = t3_train_df[T3_LABELS].values.astype(int)\n",
    "\n",
    "print(\"[T3] TRAIN size:\", len(t3_train_df))\n",
    "print(\"[T3] DEV size  :\", len(t3_dev_df))\n",
    "\n",
    "# 3.2 Load prob caches\n",
    "t3_train_x = pd.read_csv(XLMR_CACHE_ROOT   / \"t3_train_probs.csv\")\n",
    "t3_train_d = pd.read_csv(DEBERTA_CACHE_ROOT / \"t3_train_probs.csv\")\n",
    "\n",
    "t3_train_x[\"id\"] = t3_train_x[\"id\"].astype(str)\n",
    "t3_train_d[\"id\"] = t3_train_d[\"id\"].astype(str)\n",
    "\n",
    "t3_train_merged = (\n",
    "    t3_train_df[[\"id\"]]\n",
    "    .merge(t3_train_x, on=\"id\", how=\"left\", suffixes=(\"\", \"_xlmr\"))\n",
    "    .merge(t3_train_d, on=\"id\", how=\"left\", suffixes=(\"_xlmr\", \"_deberta\"))\n",
    ")\n",
    "\n",
    "for lab in T3_LABELS:\n",
    "    assert t3_train_merged[f\"prob_{lab}_xlmr\"].notna().all(), f\"Missing XLM-R prob_{lab}\"\n",
    "    assert t3_train_merged[f\"prob_{lab}_deberta\"].notna().all(), f\"Missing DeBERTa prob_{lab}\"\n",
    "\n",
    "# 3.3 Load individual model calibration thresholds\n",
    "cal_t3_x = load_json(XLMR_ART_ROOT    / \"calib_t3_native.json\")\n",
    "cal_t3_d = load_json(DEBERTA_ART_ROOT / \"calib_t3_native.json\")\n",
    "thr_map_t3_x = cal_t3_x[\"thresholds\"]\n",
    "thr_map_t3_d = cal_t3_d[\"thresholds\"]\n",
    "\n",
    "# 3.4 Build prob matrices\n",
    "N = len(t3_train_merged)\n",
    "C = len(T3_LABELS)\n",
    "P_x_train   = np.zeros((N, C), dtype=np.float32)\n",
    "P_d_train   = np.zeros((N, C), dtype=np.float32)\n",
    "P_ens_train = np.zeros((N, C), dtype=np.float32)\n",
    "\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    P_x_train[:, j]   = t3_train_merged[f\"prob_{lab}_xlmr\"].values\n",
    "    P_d_train[:, j]   = t3_train_merged[f\"prob_{lab}_deberta\"].values\n",
    "    P_ens_train[:, j] = 0.5 * (P_x_train[:, j] + P_d_train[:, j])\n",
    "\n",
    "# 3.5 XLM-R, DeBERTa, Ensemble F1\n",
    "P3_x = np.zeros_like(P_x_train, dtype=int)\n",
    "P3_d = np.zeros_like(P_d_train, dtype=int)\n",
    "\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    thr_x = float(thr_map_t3_x[lab])\n",
    "    thr_d = float(thr_map_t3_d[lab])\n",
    "    P3_x[:, j] = (P_x_train[:, j] >= thr_x).astype(int)\n",
    "    P3_d[:, j] = (P_d_train[:, j] >= thr_d).astype(int)\n",
    "\n",
    "f1_x = f1_score(Y3_true, P3_x, average=\"macro\", zero_division=0)\n",
    "f1_d = f1_score(Y3_true, P3_d, average=\"macro\", zero_division=0)\n",
    "\n",
    "# ensemble thresholds via grid search\n",
    "thr_map_t3_ens = grid_search_thresholds(Y3_true, P_ens_train, T3_LABELS)\n",
    "\n",
    "P3_ens = np.zeros_like(P_ens_train, dtype=int)\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    thr_e = float(thr_map_t3_ens[lab])\n",
    "    P3_ens[:, j] = (P_ens_train[:, j] >= thr_e).astype(int)\n",
    "\n",
    "f1_ens = f1_score(Y3_true, P3_ens, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"\\n[T3] TRAIN macro-F1 comparison:\")\n",
    "print(\"  XLM-R    (calibrated) :\", f1_x)\n",
    "print(\"  DeBERTa  (calibrated) :\", f1_d)\n",
    "print(\"  Ensemble (calibrated) :\", f1_ens)\n",
    "print(\"  Ensemble thresholds   :\", thr_map_t3_ens)\n",
    "\n",
    "# 3.6 Save ensemble calibration info\n",
    "with open(OUT_ROOT / \"calib_t3_ensemble.json\", \"w\") as f:\n",
    "    json.dump({\"thresholds\": thr_map_t3_ens}, f, indent=2)\n",
    "\n",
    "# 3.7 Debug Excel\n",
    "t3_debug_cols = {\"id\": t3_train_merged[\"id\"].astype(str).values}\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    t3_debug_cols[f\"prob_{lab}_xlmr\"]    = P_x_train[:, j]\n",
    "    t3_debug_cols[f\"prob_{lab}_deberta\"] = P_d_train[:, j]\n",
    "    t3_debug_cols[f\"prob_{lab}_ens\"]     = P_ens_train[:, j]\n",
    "t3_debug = pd.DataFrame(t3_debug_cols)\n",
    "t3_debug.to_excel(OUT_ROOT / \"t3_train_ensemble_debug.xlsx\", index=False)\n",
    "print(\"Saved T3 debug ensemble file:\", OUT_ROOT / \"t3_train_ensemble_debug.xlsx\")\n",
    "\n",
    "# 3.8 Ensemble predictions for DEV\n",
    "t3_dev_x = pd.read_csv(XLMR_CACHE_ROOT   / \"t3_dev_probs.csv\")\n",
    "t3_dev_d = pd.read_csv(DEBERTA_CACHE_ROOT / \"t3_dev_probs.csv\")\n",
    "\n",
    "t3_dev_x[\"id\"] = t3_dev_x[\"id\"].astype(str)\n",
    "t3_dev_d[\"id\"] = t3_dev_d[\"id\"].astype(str)\n",
    "\n",
    "t3_dev_merged = (\n",
    "    t3_dev_df[[\"id\"]]\n",
    "    .merge(t3_dev_x, on=\"id\", how=\"left\", suffixes=(\"\", \"_xlmr\"))\n",
    "    .merge(t3_dev_d, on=\"id\", how=\"left\", suffixes=(\"_xlmr\", \"_deberta\"))\n",
    ")\n",
    "\n",
    "N_dev = len(t3_dev_merged)\n",
    "P_ens_dev = np.zeros((N_dev, C), dtype=np.float32)\n",
    "\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    px = t3_dev_merged[f\"prob_{lab}_xlmr\"].values\n",
    "    pd_ = t3_dev_merged[f\"prob_{lab}_deberta\"].values\n",
    "    assert np.isfinite(px).all(), f\"Missing XLM-R dev probs for label {lab}\"\n",
    "    assert np.isfinite(pd_).all(), f\"Missing DeBERTa dev probs for label {lab}\"\n",
    "    P_ens_dev[:, j] = 0.5 * (px + pd_)\n",
    "\n",
    "P3_dev = np.zeros_like(P_ens_dev, dtype=int)\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    thr_e = float(thr_map_t3_ens[lab])\n",
    "    P3_dev[:, j] = (P_ens_dev[:, j] >= thr_e).astype(int)\n",
    "\n",
    "# 3.9 Codabench submission CSV (required header)\n",
    "#   id,stereotype,vilification,dehumanization,\n",
    "#   extreme_language,lack_of_empathy,invalidation\n",
    "\n",
    "idx_vil      = T3_LABELS.index(\"vilification\")\n",
    "idx_extreme  = T3_LABELS.index(\"extreme_language\")\n",
    "idx_stereo   = T3_LABELS.index(\"stereotype\")\n",
    "idx_invalid  = T3_LABELS.index(\"invalidation\")\n",
    "idx_lackemp  = T3_LABELS.index(\"lack_of_empathy\")\n",
    "idx_dehum    = T3_LABELS.index(\"dehumanization\")\n",
    "\n",
    "sub3 = pd.DataFrame({\n",
    "    \"id\":               t3_dev_merged[\"id\"].astype(str).values,\n",
    "    \"stereotype\":       P3_dev[:, idx_stereo],\n",
    "    \"vilification\":     P3_dev[:, idx_vil],\n",
    "    \"dehumanization\":   P3_dev[:, idx_dehum],\n",
    "    \"extreme_language\": P3_dev[:, idx_extreme],\n",
    "    \"lack_of_empathy\":  P3_dev[:, idx_lackemp],\n",
    "    \"invalidation\":     P3_dev[:, idx_invalid],\n",
    "})\n",
    "sub3_path = SUB_ROOT / \"subtask_3\" / f\"pred_{LANG}.csv\"\n",
    "sub3.to_csv(sub3_path, index=False)\n",
    "print(\"Saved Subtask 3 ensemble submission:\", sub3_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
