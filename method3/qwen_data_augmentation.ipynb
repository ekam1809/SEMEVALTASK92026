{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3514d92b",
   "metadata": {},
   "source": [
    "## Setup, config, paths, and model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32eafe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.0\n",
      "Using GPU: NVIDIA H100 80GB HBM3 MIG 2g.20gb\n",
      "LANG = eng\n",
      "Original base: ../dev_phase\n",
      "Augmented base: ../dev_phase_aug\n",
      "Loading Qwen model: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "2025-12-08 15:52:59.479302: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-08 15:52:59.492994: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765237979.502724   68492 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765237979.505387   68492 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765237979.514708   68492 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765237979.514718   68492 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765237979.514720   68492 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765237979.514722   68492 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-08 15:52:59.518008: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen loaded and ready.\n"
     ]
    }
   ],
   "source": [
    "# # Qwen Data Augmentation (with Resume, Subtask1-only paraphrasing)\n",
    "#\n",
    "# - Uses Qwen2.5-7B-Instruct to paraphrase Subtask 1 training data.\n",
    "# - Writes augmented CSVs into `../dev_phase_aug`.\n",
    "# - Can resume per row using:\n",
    "#   - `cache/qwen_aug_progress/{LANG}/subtask1/paraphrases.csv`\n",
    "#   - `cache/qwen_aug_progress/{LANG}/subtask1/progress.json`\n",
    "# - For Subtask 2 and 3, we do not paraphrase again:\n",
    "#   - All three subtasks share the same rows / texts.\n",
    "#   - We:\n",
    "#       1. Read the augmented Subtask 1 train CSV (original + Qwen paraphrases).\n",
    "#       2. Strip `_augX` to get `base_id`.\n",
    "#       3. Copy multi-labels from original Subtask 2/3 train files based on `base_id`.\n",
    "# - Final structure:\n",
    "#   - `../dev_phase_aug/subtask1/train/{LANG}.csv`   (original + Qwen paraphrases)\n",
    "#   - `../dev_phase_aug/subtask2/train/{LANG}.csv`   (labels propagated from original T2)\n",
    "#   - `../dev_phase_aug/subtask3/train/{LANG}.csv`   (labels propagated from original T3)\n",
    "# - After running this notebook:\n",
    "#   - In your XLM-R / DeBERTa / ensemble notebooks, set:\n",
    "#       `BASE = \"../dev_phase_aug\"`\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x, **kwargs: x  # fallback: no progress bar\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Device selection (Qwen generation)\n",
    "# ------------------------------------------------------------\n",
    "RUN_DEVICE = \"gpu\"  # \"gpu\" or \"cpu\"\n",
    "\n",
    "if RUN_DEVICE.lower() == \"gpu\" and torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# ------------------------------------------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Dataset + language config\n",
    "# ------------------------------------------------------------\n",
    "# Set this to the language you want to augment: \"eng\", \"ben\", \"hin\"\n",
    "LANG = \"eng\"\n",
    "lang_fname = LANG\n",
    "\n",
    "# Original data root (same as method1/method2 notebooks)\n",
    "BASE = Path(\"../dev_phase\")\n",
    "\n",
    "# New augmented data root\n",
    "AUG_BASE = Path(\"../dev_phase_aug\")\n",
    "\n",
    "# Mirror of original structure:\n",
    "T1_TRAIN_IN  = BASE     / \"subtask1\" / \"train\" / f\"{lang_fname}.csv\"\n",
    "T2_TRAIN_IN  = BASE     / \"subtask2\" / \"train\" / f\"{lang_fname}.csv\"\n",
    "T3_TRAIN_IN  = BASE     / \"subtask3\" / \"train\" / f\"{lang_fname}.csv\"\n",
    "\n",
    "T1_TRAIN_OUT = AUG_BASE / \"subtask1\" / \"train\" / f\"{lang_fname}.csv\"\n",
    "T2_TRAIN_OUT = AUG_BASE / \"subtask2\" / \"train\" / f\"{lang_fname}.csv\"\n",
    "T3_TRAIN_OUT = AUG_BASE / \"subtask3\" / \"train\" / f\"{lang_fname}.csv\"\n",
    "\n",
    "for p in [T1_TRAIN_OUT, T2_TRAIN_OUT, T3_TRAIN_OUT]:\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Label orders (must match your other notebooks)\n",
    "T2_LABELS = [\"gender/sexual\", \"political\", \"religious\", \"racial/ethnic\", \"other\"]\n",
    "T3_LABELS = [\"vilification\", \"extreme_language\", \"stereotype\",\n",
    "             \"invalidation\", \"lack_of_empathy\", \"dehumanization\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Augmentation hyperparameters\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# How many paraphrases per selected example? (used for Subtask 1 only)\n",
    "NUM_PARAPHRASES_T1 = 2\n",
    "\n",
    "# Maximum number of original examples to augment in Subtask 1 (None = no limit)\n",
    "MAX_SAMPLES_T1 = None\n",
    "\n",
    "print(\"LANG =\", LANG)\n",
    "print(\"Original base:\", BASE)\n",
    "print(\"Augmented base:\", AUG_BASE)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load Qwen model & tokenizer\n",
    "# ------------------------------------------------------------\n",
    "QWEN_MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(f\"Loading Qwen model: {QWEN_MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_NAME)\n",
    "\n",
    "dtype = torch.bfloat16 if DEVICE.type == \"cuda\" else torch.float32\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    QWEN_MODEL_NAME,\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(\"Qwen loaded and ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931bb69d",
   "metadata": {},
   "source": [
    "## Qwen paraphrasing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47eaf3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Paraphrasing helpers using Qwen (sentence-by-sentence, with quality checks)\n",
    "#\n",
    "# Key ideas:\n",
    "# - Split each comment into sentences.\n",
    "# - Paraphrase one sentence at a time with Qwen.\n",
    "# - For each example, build N paraphrases by paraphrasing each sentence N times and concatenating.\n",
    "# - Use relative length heuristics so short sentences are allowed, but we avoid tiny fragments.\n",
    "\n",
    "MAX_QWEN_RETRIES = 3          # retries per sentence\n",
    "MIN_ABS_CHARS     = 5         # absolute minimum for very short sentences\n",
    "MIN_REL_CHARS     = 0.4       # paraphrase must be at least 40% of original length (chars)\n",
    "\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Very simple multilingual-ish sentence splitter:\n",
    "    - Splits on ., !, ?, or Devanagari danda '।'\n",
    "    - Keeps the punctuation with the sentence if present.\n",
    "    Works well enough for short social media comments.\n",
    "    \"\"\"\n",
    "    text = str(text).strip()\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    # Pattern: one or more non-terminal chars followed by optional punctuation\n",
    "    pattern = r'[^.!?।]+[.!?।]*'\n",
    "    sentences = [m.group().strip() for m in re.finditer(pattern, text) if m.group().strip()]\n",
    "\n",
    "    # Fallback: if the regex somehow fails, just return the whole text as one sentence\n",
    "    if not sentences:\n",
    "        return [text]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def qwen_generate(prompt: str, max_new_tokens: int = 64) -> str:\n",
    "    \"\"\"\n",
    "    Run Qwen in chat mode and return the raw assistant text.\n",
    "    Assumes `tokenizer`, `model`, and `DEVICE` are already defined.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a helpful assistant that paraphrases short social media sentences \"\n",
    "                \"for a hate-speech and toxicity detection dataset. \"\n",
    "                \"Always answer with a single paraphrased sentence, not explanations.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    chat_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(chat_text, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Slice off the prompt tokens\n",
    "    generated_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    out_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return out_text.strip()\n",
    "\n",
    "\n",
    "def build_single_sentence_prompt(\n",
    "    sentence: str,\n",
    "    lang: str,\n",
    "    labels_description: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Prompt for a single sentence.\n",
    "    We explicitly require one paraphrased sentence in the same language.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are rewriting a single short social media sentence for a toxicity and hate-speech detection dataset.\n",
    "\n",
    "Language: {lang}\n",
    "Original sentence:\n",
    "{sentence}\n",
    "\n",
    "Labels for this example:\n",
    "{labels_description}\n",
    "\n",
    "Task:\n",
    "- Rewrite this ONE sentence in the SAME LANGUAGE ({lang}).\n",
    "- Keep exactly the same meaning and labels (do NOT change whether this is hate speech or which categories apply).\n",
    "- Make it sound natural for social media.\n",
    "- Do NOT add explanations.\n",
    "- Do NOT translate to any other language.\n",
    "\n",
    "Output format:\n",
    "- Write exactly ONE paraphrased sentence.\n",
    "- Do NOT add numbering, bullet points, or any extra text.\n",
    "\"\"\".strip()\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def _clean_single_line(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Take Qwen's raw output and:\n",
    "    - Keep only the first non-empty line.\n",
    "    - Strip simple numbering like '1)' or '1.' at the start.\n",
    "    \"\"\"\n",
    "    lines = [l.strip() for l in text.split(\"\\n\") if l.strip()]\n",
    "    if not lines:\n",
    "        return \"\"\n",
    "\n",
    "    line = lines[0]\n",
    "    line = re.sub(r\"^[0-9]+[\\).\\-\\:]\\s*\", \"\", line).strip()\n",
    "    return line\n",
    "\n",
    "\n",
    "def _is_good_paraphrase(candidate: str, original: str) -> bool:\n",
    "    \"\"\"\n",
    "    Relative length-based filter:\n",
    "    - candidate must not be empty.\n",
    "    - candidate length >= max(MIN_ABS_CHARS, MIN_REL_CHARS * len(original)).\n",
    "    This allows short sentences but rejects tiny fragments like 'टीवी से फ'.\n",
    "    \"\"\"\n",
    "    if not isinstance(candidate, str):\n",
    "        return False\n",
    "    candidate = candidate.strip()\n",
    "    if not candidate:\n",
    "        return False\n",
    "\n",
    "    orig = str(original).strip()\n",
    "    if not orig:\n",
    "        return False\n",
    "\n",
    "    need_len = max(MIN_ABS_CHARS, int(len(orig) * MIN_REL_CHARS))\n",
    "    if len(candidate) < need_len:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def paraphrase_single_sentence(\n",
    "    sentence: str,\n",
    "    lang: str,\n",
    "    labels_description: str,\n",
    "    max_new_tokens: int = 64,\n",
    "    max_retries: int = MAX_QWEN_RETRIES,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Paraphrase ONE sentence with retries and relative-length filtering.\n",
    "    If we consistently fail, we fall back to the original sentence.\n",
    "    \"\"\"\n",
    "    sentence = str(sentence).strip()\n",
    "    if not sentence:\n",
    "        return sentence\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            prompt = build_single_sentence_prompt(\n",
    "                sentence=sentence,\n",
    "                lang=lang,\n",
    "                labels_description=labels_description,\n",
    "            )\n",
    "            raw = qwen_generate(prompt, max_new_tokens=max_new_tokens)\n",
    "            candidate = _clean_single_line(raw)\n",
    "            if _is_good_paraphrase(candidate, sentence):\n",
    "                return candidate\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Qwen single-sentence generation failed (attempt {attempt+1}): {e}\")\n",
    "\n",
    "    # Fallback: keep original sentence\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def generate_paraphrases_for_example(\n",
    "    text: str,\n",
    "    lang: str,\n",
    "    labels_description: str,\n",
    "    num_paraphrases: int,\n",
    "    max_new_tokens: int = 64,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Main API used by augmentation functions.\n",
    "\n",
    "    Steps:\n",
    "    - Split comment into sentences.\n",
    "    - For each of N paraphrases:\n",
    "        - Paraphrase each sentence individually.\n",
    "        - Join paraphrased sentences back into a full comment.\n",
    "    - Return list of N paraphrased comments (may be fewer if something goes wrong).\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    if not sentences:\n",
    "        return []\n",
    "\n",
    "    all_paras: List[str] = []\n",
    "\n",
    "    for _ in range(num_paraphrases):\n",
    "        new_sentences = []\n",
    "        for s in sentences:\n",
    "            para_s = paraphrase_single_sentence(\n",
    "                sentence=s,\n",
    "                lang=lang,\n",
    "                labels_description=labels_description,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "            new_sentences.append(para_s)\n",
    "\n",
    "        paraphrased_comment = \" \".join(new_sentences).strip()\n",
    "        all_paras.append(paraphrased_comment)\n",
    "\n",
    "    return all_paras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f721818",
   "metadata": {},
   "source": [
    "## Resume-able augmentation logic for all 3 subtasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c9c4513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Augment Subtask 1 with Qwen (row-level resume)\n",
    "#\n",
    "# - Reads original Subtask 1 train CSV from `../dev_phase/subtask1/train/{LANG}.csv`.\n",
    "# - Writes augmented train CSV to `../dev_phase_aug/subtask1/train/{LANG}.csv`.\n",
    "# - Keeps a per-language, per-subtask progress cache in:\n",
    "#   - `cache/qwen_aug_progress/{LANG}/subtask1/paraphrases.csv`\n",
    "#   - `cache/qwen_aug_progress/{LANG}/subtask1/progress.json`\n",
    "# - If interrupted, just rerun: it resumes from where it left off (per original id).\n",
    "# - It then rebuilds the final augmented train CSV from:\n",
    "#     original + all paraphrases cached so far.\n",
    "\n",
    "# Root for per-language augmentation progress\n",
    "PROGRESS_ROOT = Path(\"cache\") / \"qwen_aug_progress\" / LANG\n",
    "PROGRESS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FLUSH_EVERY = 20  # flush paraphrases + progress every N source rows\n",
    "\n",
    "\n",
    "# ------------------------ progress & cache helpers ------------------------ #\n",
    "\n",
    "def _progress_file(subtask_id: int) -> Path:\n",
    "    return PROGRESS_ROOT / f\"subtask{subtask_id}\" / \"progress.json\"\n",
    "\n",
    "\n",
    "def _paraphrase_cache_file(subtask_id: int) -> Path:\n",
    "    return PROGRESS_ROOT / f\"subtask{subtask_id}\" / \"paraphrases.csv\"\n",
    "\n",
    "\n",
    "def _read_progress(subtask_id: int) -> set:\n",
    "    \"\"\"\n",
    "    Return set of original ids already processed for this subtask.\n",
    "    \"\"\"\n",
    "    p = _progress_file(subtask_id)\n",
    "    if not p.exists():\n",
    "        return set()\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    # store done_ids as list of strings\n",
    "    return set(data.get(\"done_ids\", []))\n",
    "\n",
    "\n",
    "def _write_progress(subtask_id: int, done_ids: set):\n",
    "    \"\"\"\n",
    "    Save set of original ids already processed.\n",
    "    \"\"\"\n",
    "    p = _progress_file(subtask_id)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"done_ids\": sorted(list(done_ids))}, f, indent=2)\n",
    "\n",
    "\n",
    "def _append_paraphrases(subtask_id: int, df_rows: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Append newly generated paraphrases to a cache CSV:\n",
    "      cache/qwen_aug_progress/LANG/subtaskX/paraphrases.csv\n",
    "    \"\"\"\n",
    "    if df_rows is None or len(df_rows) == 0:\n",
    "        return\n",
    "    cache_path = _paraphrase_cache_file(subtask_id)\n",
    "    cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    header = not cache_path.exists()\n",
    "    df_rows.to_csv(cache_path, mode=\"a\", index=False, header=header)\n",
    "\n",
    "\n",
    "def _load_paraphrases(subtask_id: int) -> Optional[pd.DataFrame]:\n",
    "    cache_path = _paraphrase_cache_file(subtask_id)\n",
    "    if cache_path.exists():\n",
    "        return pd.read_csv(cache_path)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _rebuild_augmented_train(\n",
    "    subtask_id: int,\n",
    "    orig_df: pd.DataFrame,\n",
    "    out_path: Path,\n",
    "    keep_cols: list,\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine original train df with all cached paraphrases and write final CSV.\n",
    "    \"\"\"\n",
    "    df_par = _load_paraphrases(subtask_id)\n",
    "    if df_par is not None and len(df_par) > 0:\n",
    "        df_aug = pd.concat(\n",
    "            [orig_df, df_par[keep_cols]],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "    else:\n",
    "        df_aug = orig_df.copy()\n",
    "\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_aug.to_csv(out_path, index=False)\n",
    "    print(\n",
    "        f\"[Subtask {subtask_id}] Rebuilt augmented TRAIN at {out_path} \"\n",
    "        f\"(size={len(df_aug)})\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------ Subtask 1: polarization ------------------------ #\n",
    "\n",
    "def augment_subtask1_for_lang(\n",
    "    lang: str,\n",
    "    num_paraphrases: int = 2,\n",
    "    max_samples: Optional[int] = None,\n",
    ") -> None:\n",
    "    subtask_id = 1\n",
    "    in_path = T1_TRAIN_IN\n",
    "    out_path = T1_TRAIN_OUT\n",
    "\n",
    "    print(f\"[T1] Reading TRAIN from: {in_path}\")\n",
    "    df = pd.read_csv(in_path)\n",
    "    assert {\"id\", \"text\", \"polarization\"}.issubset(df.columns)\n",
    "\n",
    "    print(f\"[T1] Original TRAIN size: {len(df)}\")\n",
    "    pos_mask = df[\"polarization\"].astype(int) == 1\n",
    "    df_pos = df[pos_mask].reset_index(drop=True)\n",
    "\n",
    "    # optional cap on number of positives we augment\n",
    "    if max_samples is not None and max_samples > 0 and len(df_pos) > max_samples:\n",
    "        rs = np.random.RandomState(42)\n",
    "        idx = rs.choice(len(df_pos), size=max_samples, replace=False)\n",
    "        df_pos = df_pos.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "    print(\n",
    "        f\"[T1] Will augment {len(df_pos)} positive examples \"\n",
    "        f\"with {num_paraphrases} paraphrases each.\"\n",
    "    )\n",
    "\n",
    "    # progress: which original ids are already done\n",
    "    done_ids = _read_progress(subtask_id)\n",
    "    print(f\"[T1] Already completed examples (from cache): {len(done_ids)}\")\n",
    "\n",
    "    new_rows_buf = []\n",
    "\n",
    "    for i in tqdm(range(len(df_pos)), desc=\"[T1] Augmenting\", total=len(df_pos)):\n",
    "        row = df_pos.iloc[i]\n",
    "        orig_id = str(row[\"id\"])\n",
    "        if orig_id in done_ids:\n",
    "            continue  # skip already processed example\n",
    "\n",
    "        orig_text = str(row[\"text\"])\n",
    "        label = int(row[\"polarization\"])\n",
    "        label_name = \"polarized\" if label == 1 else \"not polarized\"\n",
    "        labels_desc = f\"polarization = {label} ({label_name})\"\n",
    "\n",
    "        paras = generate_paraphrases_for_example(\n",
    "            text=orig_text,\n",
    "            lang=lang,\n",
    "            labels_description=labels_desc,\n",
    "            num_paraphrases=num_paraphrases,\n",
    "        )\n",
    "\n",
    "        for k, p in enumerate(paras):\n",
    "            new_rows_buf.append({\n",
    "                \"id\": f\"{orig_id}_aug{k+1}\",\n",
    "                \"text\": p,\n",
    "                \"polarization\": label,\n",
    "                \"src_id\": orig_id,\n",
    "                \"is_aug\": 1,\n",
    "            })\n",
    "\n",
    "        done_ids.add(orig_id)\n",
    "\n",
    "        # Periodically flush to disk so we don't lose progress on interrupt\n",
    "        is_last = (i == len(df_pos) - 1)\n",
    "        if len(new_rows_buf) >= FLUSH_EVERY or is_last:\n",
    "            if new_rows_buf:\n",
    "                df_flush = pd.DataFrame(new_rows_buf)\n",
    "                _append_paraphrases(subtask_id, df_flush)\n",
    "                new_rows_buf = []\n",
    "            _write_progress(subtask_id, done_ids)\n",
    "\n",
    "    # finally rebuild augmented train CSV (original + all paraphrases so far)\n",
    "    _rebuild_augmented_train(\n",
    "        subtask_id=subtask_id,\n",
    "        orig_df=df,\n",
    "        out_path=out_path,\n",
    "        keep_cols=[\"id\", \"text\", \"polarization\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8c709",
   "metadata": {},
   "source": [
    "## Run augmentation for the chosen language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "556ca955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Qwen data augmentation for LANG = eng ===\n",
      "\n",
      "[T1] Reading TRAIN from: ../dev_phase/subtask1/train/eng.csv\n",
      "[T1] Original TRAIN size: 3222\n",
      "[T1] Will augment 1175 positive examples with 2 paraphrases each.\n",
      "[T1] Already completed examples (from cache): 1002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[T1] Augmenting: 100%|██████████| 1175/1175 [02:26<00:00,  8.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Subtask 1] Rebuilt augmented TRAIN at ../dev_phase_aug/subtask1/train/eng.csv (size=5572)\n",
      "\n",
      "=== Building augmented Subtask 2 from Subtask 1 paraphrases ===\n",
      "[T2] Building augmented train from T1_aug.\n",
      "     T1_aug: ../dev_phase_aug/subtask1/train/eng.csv\n",
      "     T2_orig: ../dev_phase/subtask2/train/eng.csv\n",
      "[T2] Saved augmented TRAIN to: ../dev_phase_aug/subtask2/train/eng.csv  (rows=5572)\n",
      "\n",
      "=== Building augmented Subtask 3 from Subtask 1 paraphrases ===\n",
      "[T3] Building augmented train from T1_aug.\n",
      "     T1_aug: ../dev_phase_aug/subtask1/train/eng.csv\n",
      "     T3_orig: ../dev_phase/subtask3/train/eng.csv\n",
      "[T3] Saved augmented TRAIN to: ../dev_phase_aug/subtask3/train/eng.csv  (rows=5572)\n",
      "\n",
      "=== Done. Augmented TRAIN CSVs are in: ../dev_phase_aug ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ## Build augmented Subtask 2 & 3 from augmented Subtask 1\n",
    "#\n",
    "# Assumption: for a given `LANG`:\n",
    "# - `subtask1/train/LANG.csv`, `subtask2/train/LANG.csv`, `subtask3/train/LANG.csv`\n",
    "#   all have the same rows and the same `id`/`text` pairs in the original data.\n",
    "# - We first run Qwen to paraphrase Subtask 1 and save:\n",
    "#     `../dev_phase_aug/subtask1/train/LANG.csv`\n",
    "#   which contains both original rows and rows with IDs like `<orig_id>_aug1`.\n",
    "#\n",
    "# Then:\n",
    "# - For Subtask 2 and 3, we DO NOT paraphrase again.\n",
    "# - Instead, we:\n",
    "#   * Read the augmented Subtask 1 CSV (T1_TRAIN_OUT).\n",
    "#   * Derive `base_id` by stripping `_augX` suffix.\n",
    "#   * Join labels from the original Subtask 2/3 train CSV by `base_id`.\n",
    "#   * Save new augmented Subtask 2/3 train CSVs into `../dev_phase_aug`.\n",
    "\n",
    "def _add_base_id_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a 'base_id' column:\n",
    "    - For original rows: base_id == id\n",
    "    - For augmented rows like 'abc123_aug1': base_id == 'abc123'\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"id\"] = df[\"id\"].astype(str)\n",
    "    df[\"base_id\"] = df[\"id\"].str.replace(r\"_aug\\d+$\", \"\", regex=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_augmented_subtask2_from_t1(lang: str) -> None:\n",
    "    \"\"\"\n",
    "    Use augmented Subtask 1 CSV + original Subtask 2 labels\n",
    "    to build augmented Subtask 2 train CSV.\n",
    "    \"\"\"\n",
    "    # T1 augmented (already contains paraphrased text)\n",
    "    t1_aug_path = T1_TRAIN_OUT\n",
    "    if not t1_aug_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"[T2] Expected augmented Subtask 1 file not found: {t1_aug_path}\\n\"\n",
    "            \"Run augment_subtask1_for_lang(...) first.\"\n",
    "        )\n",
    "\n",
    "    # Original Subtask 2 train (labels source)\n",
    "    t2_orig_path = T2_TRAIN_IN\n",
    "    if not t2_orig_path.exists():\n",
    "        raise FileNotFoundError(f\"[T2] Original train file not found: {t2_orig_path}\")\n",
    "\n",
    "    print(f\"[T2] Building augmented train from T1_aug.\")\n",
    "    print(f\"     T1_aug: {t1_aug_path}\")\n",
    "    print(f\"     T2_orig: {t2_orig_path}\")\n",
    "\n",
    "    t1_aug = pd.read_csv(t1_aug_path)\n",
    "    t2_orig = pd.read_csv(t2_orig_path)\n",
    "\n",
    "    assert {\"id\", \"text\"}.issubset(t1_aug.columns), \"[T2] T1_aug must have 'id' and 'text'.\"\n",
    "    assert {\"id\", \"text\", *T2_LABELS}.issubset(t2_orig.columns), \\\n",
    "        \"[T2] original T2 train must have 'id', 'text' and all T2_LABELS.\"\n",
    "\n",
    "    # Add base_id to T1_aug (original + augmented rows)\n",
    "    t1_aug = _add_base_id_column(t1_aug)\n",
    "\n",
    "    # Map base_id -> labels for T2\n",
    "    t2_labels = t2_orig[[\"id\"] + T2_LABELS].rename(columns={\"id\": \"base_id\"})\n",
    "\n",
    "    # Join labels\n",
    "    merged = t1_aug.merge(t2_labels, on=\"base_id\", how=\"left\", validate=\"m:1\")\n",
    "\n",
    "    # Check for any rows that didn't find labels (shouldn't happen if IDs match)\n",
    "    missing = merged[T2_LABELS].isna().any(axis=1)\n",
    "    if missing.any():\n",
    "        n_miss = int(missing.sum())\n",
    "        print(f\"[T2] WARNING: {n_miss} rows in T1_aug had no matching labels in T2_orig. Dropping them.\")\n",
    "        merged = merged[~missing].copy()\n",
    "\n",
    "    # Ensure integer label types\n",
    "    for lab in T2_LABELS:\n",
    "        merged[lab] = merged[lab].astype(int)\n",
    "\n",
    "    # We can drop 'base_id' if you don't need it\n",
    "    merged = merged.drop(columns=[\"base_id\"])\n",
    "\n",
    "    # Reorder columns: id, text, labels...\n",
    "    cols = [\"id\", \"text\"] + T2_LABELS\n",
    "    merged = merged[cols]\n",
    "\n",
    "    # Save to augmented T2 path\n",
    "    out_path = T2_TRAIN_OUT\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    merged.to_csv(out_path, index=False)\n",
    "    print(f\"[T2] Saved augmented TRAIN to: {out_path}  (rows={len(merged)})\")\n",
    "\n",
    "\n",
    "def build_augmented_subtask3_from_t1(lang: str) -> None:\n",
    "    \"\"\"\n",
    "    Use augmented Subtask 1 CSV + original Subtask 3 labels\n",
    "    to build augmented Subtask 3 train CSV.\n",
    "    \"\"\"\n",
    "    # T1 augmented\n",
    "    t1_aug_path = T1_TRAIN_OUT\n",
    "    if not t1_aug_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"[T3] Expected augmented Subtask 1 file not found: {t1_aug_path}\\n\"\n",
    "            \"Run augment_subtask1_for_lang(...) first.\"\n",
    "        )\n",
    "\n",
    "    # Original Subtask 3\n",
    "    t3_orig_path = T3_TRAIN_IN\n",
    "    if not t3_orig_path.exists():\n",
    "        raise FileNotFoundError(f\"[T3] Original train file not found: {t3_orig_path}\")\n",
    "\n",
    "    print(f\"[T3] Building augmented train from T1_aug.\")\n",
    "    print(f\"     T1_aug: {t1_aug_path}\")\n",
    "    print(f\"     T3_orig: {t3_orig_path}\")\n",
    "\n",
    "    t1_aug = pd.read_csv(t1_aug_path)\n",
    "    t3_orig = pd.read_csv(t3_orig_path)\n",
    "\n",
    "    assert {\"id\", \"text\"}.issubset(t1_aug.columns), \"[T3] T1_aug must have 'id' and 'text'.\"\n",
    "    assert {\"id\", \"text\", *T3_LABELS}.issubset(t3_orig.columns), \\\n",
    "        \"[T3] original T3 train must have 'id', 'text' and all T3_LABELS.\"\n",
    "\n",
    "    # Add base_id\n",
    "    t1_aug = _add_base_id_column(t1_aug)\n",
    "\n",
    "    # Map base_id -> labels for T3\n",
    "    t3_labels = t3_orig[[\"id\"] + T3_LABELS].rename(columns={\"id\": \"base_id\"})\n",
    "\n",
    "    # Join labels\n",
    "    merged = t1_aug.merge(t3_labels, on=\"base_id\", how=\"left\", validate=\"m:1\")\n",
    "\n",
    "    missing = merged[T3_LABELS].isna().any(axis=1)\n",
    "    if missing.any():\n",
    "        n_miss = int(missing.sum())\n",
    "        print(f\"[T3] WARNING: {n_miss} rows in T1_aug had no matching labels in T3_orig. Dropping them.\")\n",
    "        merged = merged[~missing].copy()\n",
    "\n",
    "    for lab in T3_LABELS:\n",
    "        merged[lab] = merged[lab].astype(int)\n",
    "\n",
    "    merged = merged.drop(columns=[\"base_id\"])\n",
    "    cols = [\"id\", \"text\"] + T3_LABELS\n",
    "    merged = merged[cols]\n",
    "\n",
    "    out_path = T3_TRAIN_OUT\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    merged.to_csv(out_path, index=False)\n",
    "    print(f\"[T3] Saved augmented TRAIN to: {out_path}  (rows={len(merged)})\")\n",
    "# ## Run augmentation for this language (resume-aware + label propagation)\n",
    "#\n",
    "# Workflow:\n",
    "# 1. Set `LANG` at the top (e.g. \"eng\", \"ben\", \"hin\").\n",
    "# 2. Run all cells.\n",
    "# 3. This cell:\n",
    "#    - Runs Qwen paraphrasing only for Subtask 1, using resume/cache:\n",
    "#         - `cache/qwen_aug_progress/{LANG}/subtask1/paraphrases.csv`\n",
    "#         - `cache/qwen_aug_progress/{LANG}/subtask1/progress.json`\n",
    "#    - Then builds Subtask 2 & 3 augmented train CSVs by:\n",
    "#         * Reading augmented Subtask 1 train CSV.\n",
    "#         * Copying labels from original Subtask 2/3 train CSVs\n",
    "#           based on `base_id`.\n",
    "# 4. If you interrupt Subtask 1 augmentation, rerun this cell:\n",
    "#    - Subtask 1 will resume from cache.\n",
    "#    - Subtask 2 & 3 will be rebuilt from the updated T1_aug file.\n",
    "# 5. Then in XLM-R / DeBERTa / ensemble notebooks, set:\n",
    "#       `BASE = \"../dev_phase_aug\"`\n",
    "#    so they train on augmented data.\n",
    "\n",
    "print(\"=== Qwen data augmentation for LANG =\", LANG, \"===\\n\")\n",
    "\n",
    "# 1) Qwen paraphrasing for Subtask 1 (resume-aware inside this function)\n",
    "augment_subtask1_for_lang(\n",
    "    lang=LANG,\n",
    "    num_paraphrases=NUM_PARAPHRASES_T1,\n",
    "    max_samples=MAX_SAMPLES_T1,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Building augmented Subtask 2 from Subtask 1 paraphrases ===\")\n",
    "build_augmented_subtask2_from_t1(lang=LANG)\n",
    "\n",
    "print(\"\\n=== Building augmented Subtask 3 from Subtask 1 paraphrases ===\")\n",
    "build_augmented_subtask3_from_t1(lang=LANG)\n",
    "\n",
    "print(\"\\n=== Done. Augmented TRAIN CSVs are in:\", AUG_BASE, \"===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcbccb4",
   "metadata": {},
   "source": [
    "## Copy dev set from dev_phase to dev_phase_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a116c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied ../dev_phase/subtask1/dev/eng.csv -> ../dev_phase_aug/subtask1/dev/eng.csv\n",
      "Copied ../dev_phase/subtask2/dev/eng.csv -> ../dev_phase_aug/subtask2/dev/eng.csv\n",
      "Copied ../dev_phase/subtask3/dev/eng.csv -> ../dev_phase_aug/subtask3/dev/eng.csv\n"
     ]
    }
   ],
   "source": [
    "# ## Copy DEV splits to augmented folder\n",
    "#\n",
    "# We do NOT change the dev/validation data content, but we mirror the folder\n",
    "# structure under `../dev_phase_aug` so that all downstream notebooks can use:\n",
    "#\n",
    "#     BASE = \"../dev_phase_aug\"\n",
    "#\n",
    "# for both train and dev.\n",
    "\n",
    "import shutil\n",
    "\n",
    "base_dir = Path(\".\")  # directory where the notebook lives\n",
    "\n",
    "tasks = [\"subtask1\", \"subtask2\", \"subtask3\"]\n",
    "\n",
    "for task in tasks:\n",
    "    src = base_dir / \"..\" / \"dev_phase\"     / task / \"dev\" / f\"{LANG}.csv\"\n",
    "    dst = base_dir / \"..\" / \"dev_phase_aug\" / task / \"dev\" / f\"{LANG}.csv\"\n",
    "\n",
    "    # Create target directory if needed\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Copy file (including metadata)\n",
    "    shutil.copy2(src, dst)\n",
    "    print(f\"Copied {src} -> {dst}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
