{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936ef1bd",
   "metadata": {},
   "source": [
    "## Setup, config, paths, model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a84f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 14:16:48.044941: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-08 14:16:48.058057: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765232208.071634   11074 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765232208.075709   11074 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765232208.088426   11074 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765232208.088442   11074 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765232208.088444   11074 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765232208.088445   11074 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-08 14:16:48.092677: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.0\n",
      "Transformers: 4.57.1\n",
      "Using GPU: NVIDIA H100 80GB HBM3 MIG 2g.20gb\n",
      "LANG=eng\n",
      "T2_LABELS: ['gender/sexual', 'political', 'religious', 'racial/ethnic', 'other']\n",
      "T3_LABELS: ['vilification', 'extreme_language', 'stereotype', 'invalidation', 'lack_of_empathy', 'dehumanization']\n",
      "\n",
      "Loading Qwen2.5-7B-Instruct... (this may take a moment)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.48s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen pipeline ready on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# # method3 — Qwen2.5-7B LLM predictions (all subtasks)\n",
    "# \n",
    "# - Uses Qwen/Qwen2.5-7B-Instruct as a multilingual classifier (no fine-tuning).\n",
    "# - Works directly on the original language (no translation to English).\n",
    "# - Outputs per-example predictions for TRAIN and DEV for all 3 subtasks.\n",
    "# - Caches go into: cache/qwen/<LANG>/\n",
    "# - Optional Qwen-only submissions go into: submissions/qwen/subtask_X/\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "import transformers\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Device selection: GPU strongly recommended for Qwen\n",
    "# ------------------------------------------------------------\n",
    "RUN_DEVICE = \"gpu\"  # \"gpu\" or \"cpu\"\n",
    "\n",
    "if RUN_DEVICE.lower() == \"gpu\" and torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# ------------------------------------------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# High-level config\n",
    "# ------------------------------------------------------------\n",
    "# Change LANG when you switch language (eng, ben, hin, etc.)\n",
    "LANG = \"eng\"\n",
    "\n",
    "BASE = \"../dev_phase\"    # root of organizer data\n",
    "\n",
    "# Qwen model:\n",
    "QWEN_MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# Paths for data (TRAIN has labels, DEV is unlabeled for Codabench)\n",
    "lang_fname = LANG\n",
    "\n",
    "T1_TRAIN = f\"{BASE}/subtask1/train/{lang_fname}.csv\"\n",
    "T1_DEV   = f\"{BASE}/subtask1/dev/{lang_fname}.csv\"\n",
    "\n",
    "T2_TRAIN = f\"{BASE}/subtask2/train/{lang_fname}.csv\"\n",
    "T2_DEV   = f\"{BASE}/subtask2/dev/{lang_fname}.csv\"\n",
    "\n",
    "T3_TRAIN = f\"{BASE}/subtask3/train/{lang_fname}.csv\"\n",
    "T3_DEV   = f\"{BASE}/subtask3/dev/{lang_fname}.csv\"\n",
    "\n",
    "# Caches + outputs for Qwen\n",
    "CACHE_ROOT = Path(\"cache\") / \"qwen\" / LANG\n",
    "OUT_ROOT   = Path(\"outputs\") / \"qwen\" / LANG\n",
    "SUB_ROOT   = Path(\"submissions\") / \"qwen\"\n",
    "\n",
    "for d in [CACHE_ROOT, OUT_ROOT, SUB_ROOT]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Submission subfolders (optional, just to inspect Qwen-only performance)\n",
    "(SUB_ROOT / \"subtask_1\").mkdir(parents=True, exist_ok=True)\n",
    "(SUB_ROOT / \"subtask_2\").mkdir(parents=True, exist_ok=True)\n",
    "(SUB_ROOT / \"subtask_3\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Label order for multi-label tasks (same as other methods)\n",
    "T2_LABELS = [\"gender/sexual\", \"political\", \"religious\", \"racial/ethnic\", \"other\"]\n",
    "T3_LABELS = [\"vilification\", \"extreme_language\", \"stereotype\",\n",
    "             \"invalidation\", \"lack_of_empathy\", \"dehumanization\"]\n",
    "\n",
    "print(f\"LANG={LANG}\")\n",
    "print(\"T2_LABELS:\", T2_LABELS)\n",
    "print(\"T3_LABELS:\", T3_LABELS)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load Qwen2.5-7B-Instruct via pipeline\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nLoading Qwen2.5-7B-Instruct... (this may take a moment)\")\n",
    "\n",
    "# Use bfloat16 on GPU if available, else float32 on CPU\n",
    "dtype = torch.bfloat16 if DEVICE.type == \"cuda\" else torch.float32\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_NAME, use_fast=True)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    QWEN_MODEL_NAME,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\" if DEVICE.type == \"cuda\" else None,\n",
    ")\n",
    "llm_model.eval()\n",
    "\n",
    "llm_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llm_model,\n",
    "    tokenizer=llm_tokenizer,\n",
    "    # pipeline will handle device via model.device\n",
    ")\n",
    "\n",
    "print(\"Qwen pipeline ready on device:\", llm_model.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff35d53",
   "metadata": {},
   "source": [
    "## Helpers: prompts, parsers, batch runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "905e9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Helpers: prompts, parsers, batch inference\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Prompt builders\n",
    "# ------------------------------------------------------------\n",
    "def build_prompt_t1(text: str, lang: str) -> str:\n",
    "    \"\"\"\n",
    "    Subtask 1: binary polarization.\n",
    "    Output format: single digit 0 or 1.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "You are an expert annotator for online hate and polarization detection.\n",
    "\n",
    "Language code of the text: {lang}\n",
    "\n",
    "Task:\n",
    "Given the following social media post, decide whether it contains hateful, abusive, or strongly polarizing content.\n",
    "\n",
    "Classes:\n",
    "0 = NON_HATE / NOT_POLARIZING (neutral, non-hateful content)\n",
    "1 = HATE / POLARIZING (hateful, abusive, or strongly polarizing content)\n",
    "\n",
    "Instructions:\n",
    "- Read the post carefully.\n",
    "- Decide which class is more appropriate.\n",
    "- Answer with ONLY ONE DIGIT: 0 or 1.\n",
    "- Do not include any additional words or explanation.\n",
    "\n",
    "Post:\n",
    "{text}\n",
    "\n",
    "Answer (0 or 1 only):\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_prompt_t2(text: str, lang: str) -> str:\n",
    "    \"\"\"\n",
    "    Subtask 2: multi-label 5-way.\n",
    "    Output format: 5 digits (0/1) in fixed order.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "You are an expert annotator for hate type classification in online text.\n",
    "\n",
    "Language code of the text: {lang}\n",
    "\n",
    "Task:\n",
    "Given the social media post, decide which hate types are present.\n",
    "There can be multiple hate types at the same time, or none.\n",
    "\n",
    "Label order (5 labels):\n",
    "1) gender/sexual\n",
    "2) political\n",
    "3) religious\n",
    "4) racial/ethnic\n",
    "5) other\n",
    "\n",
    "Output format:\n",
    "Return EXACTLY 5 digits, each 0 or 1, separated by spaces.\n",
    "- 1 means the hate type is present.\n",
    "- 0 means the hate type is not present.\n",
    "For example: \"0 1 0 0 1\"\n",
    "\n",
    "Important:\n",
    "- Return ONLY the 5 digits, nothing else.\n",
    "- Do NOT write label names or explanations.\n",
    "\n",
    "Post:\n",
    "{text}\n",
    "\n",
    "Answer (5 digits for the 5 labels, in order):\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_prompt_t3(text: str, lang: str) -> str:\n",
    "    \"\"\"\n",
    "    Subtask 3: multi-label 6-way.\n",
    "    Output format: 6 digits (0/1) in fixed order.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "You are an expert annotator for manifestations of hate in online text.\n",
    "\n",
    "Language code of the text: {lang}\n",
    "\n",
    "Task:\n",
    "Given the social media post, decide which manifestations of hate are present.\n",
    "There can be multiple manifestations at the same time, or none.\n",
    "\n",
    "Label order (6 labels):\n",
    "1) vilification\n",
    "2) extreme_language\n",
    "3) stereotype\n",
    "4) invalidation\n",
    "5) lack_of_empathy\n",
    "6) dehumanization\n",
    "\n",
    "Output format:\n",
    "Return EXACTLY 6 digits, each 0 or 1, separated by spaces.\n",
    "- 1 means the manifestation is present.\n",
    "- 0 means it is not present.\n",
    "For example: \"0 1 0 0 1 0\"\n",
    "\n",
    "Important:\n",
    "- Return ONLY the 6 digits, nothing else.\n",
    "- Do NOT write label names or explanations.\n",
    "\n",
    "Post:\n",
    "{text}\n",
    "\n",
    "Answer (6 digits for the 6 labels, in order):\n",
    "\"\"\".strip()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Output parsers\n",
    "# ------------------------------------------------------------\n",
    "def parse_t1_output(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Extract first occurrence of 0 or 1 from the model output.\n",
    "    Default to 0 if nothing found.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return 0\n",
    "    match = re.search(r\"[01]\", str(text))\n",
    "    if match:\n",
    "        return int(match.group(0))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def parse_digit_vector(text: str, n_labels: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract first n_labels digits (0/1) from the model output.\n",
    "    Pad with zeros if fewer digits, truncate if more.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return np.zeros(n_labels, dtype=int)\n",
    "    digits = re.findall(r\"[01]\", str(text))\n",
    "    if len(digits) < n_labels:\n",
    "        digits = digits + [\"0\"] * (n_labels - len(digits))\n",
    "    elif len(digits) > n_labels:\n",
    "        digits = digits[:n_labels]\n",
    "    return np.array([int(d) for d in digits], dtype=int)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Batch inference helper\n",
    "# ------------------------------------------------------------\n",
    "def qwen_generate_batch(prompts: List[str],\n",
    "                        max_new_tokens: int = 32,\n",
    "                        batch_size: int = 4) -> List[str]:\n",
    "    \"\"\"\n",
    "    Run Qwen on a list of prompts using the chat-style pipeline.\n",
    "    Returns list of raw assistant outputs (strings).\n",
    "    \"\"\"\n",
    "    all_outputs = []\n",
    "    for start in range(0, len(prompts), batch_size):\n",
    "        batch_prompts = prompts[start:start + batch_size]\n",
    "        messages_batch = [\n",
    "            [{\"role\": \"user\", \"content\": p}] for p in batch_prompts\n",
    "        ]\n",
    "        outputs = llm_pipe(\n",
    "            messages_batch,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        for out in outputs:\n",
    "            try:\n",
    "                msg = out[\"generated_text\"][-1][\"content\"]\n",
    "            except Exception:\n",
    "                msg = str(out)\n",
    "            all_outputs.append(msg)\n",
    "    assert len(all_outputs) == len(prompts)\n",
    "    return all_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e15f0",
   "metadata": {},
   "source": [
    "## Subtask 1 (binary) with Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7376870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T1] TRAIN size: 3222\n",
      "[T1] DEV size  : 160\n",
      "\n",
      "[T1] Running Qwen on TRAIN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T1] Qwen Macro-F1 on TRAIN (hard labels): 0.38849876636933006\n",
      "\n",
      "[T1] Running Qwen on DEV...\n",
      "Saved T1 TRAIN cache: cache/qwen/eng/t1_train_probs.csv\n",
      "Saved T1 DEV cache  : cache/qwen/eng/t1_dev_probs.csv\n",
      "Saved Qwen-only Subtask 1 submission: submissions/qwen/subtask_1/pred_eng.csv\n"
     ]
    }
   ],
   "source": [
    "# ## Subtask 1 — Polarization (binary, Qwen)\n",
    "\n",
    "# 1. Load TRAIN + DEV\n",
    "t1_train_df = pd.read_csv(T1_TRAIN)\n",
    "t1_dev_df   = pd.read_csv(T1_DEV)\n",
    "\n",
    "required_train_cols_t1 = {\"id\", \"text\", \"polarization\"}\n",
    "required_dev_cols_t1   = {\"id\", \"text\"}\n",
    "assert required_train_cols_t1.issubset(t1_train_df.columns), \\\n",
    "    f\"T1 TRAIN missing: {required_train_cols_t1 - set(t1_train_df.columns)}\"\n",
    "assert required_dev_cols_t1.issubset(t1_dev_df.columns), \\\n",
    "    f\"T1 DEV missing: {required_dev_cols_t1 - set(t1_dev_df.columns)}\"\n",
    "\n",
    "t1_train_df[\"polarization\"] = t1_train_df[\"polarization\"].astype(int)\n",
    "\n",
    "print(f\"[T1] TRAIN size: {len(t1_train_df)}\")\n",
    "print(f\"[T1] DEV size  : {len(t1_dev_df)}\")\n",
    "\n",
    "# 2. Build prompts\n",
    "train_prompts_t1 = [\n",
    "    build_prompt_t1(txt, LANG) for txt in t1_train_df[\"text\"].astype(str).tolist()\n",
    "]\n",
    "dev_prompts_t1 = [\n",
    "    build_prompt_t1(txt, LANG) for txt in t1_dev_df[\"text\"].astype(str).tolist()\n",
    "]\n",
    "\n",
    "# 3. Run Qwen on TRAIN\n",
    "print(\"\\n[T1] Running Qwen on TRAIN...\")\n",
    "train_outputs_t1 = qwen_generate_batch(train_prompts_t1, max_new_tokens=8, batch_size=4)\n",
    "pred_train_t1 = np.array([parse_t1_output(o) for o in train_outputs_t1], dtype=int)\n",
    "\n",
    "# 4. Evaluate on TRAIN\n",
    "y_true_t1 = t1_train_df[\"polarization\"].values\n",
    "f1_t1 = macro_f1(y_true_t1, pred_train_t1)\n",
    "print(\"[T1] Qwen Macro-F1 on TRAIN (hard labels):\", f1_t1)\n",
    "\n",
    "# 5. Run Qwen on DEV\n",
    "print(\"\\n[T1] Running Qwen on DEV...\")\n",
    "dev_outputs_t1 = qwen_generate_batch(dev_prompts_t1, max_new_tokens=8, batch_size=4)\n",
    "pred_dev_t1 = np.array([parse_t1_output(o) for o in dev_outputs_t1], dtype=int)\n",
    "\n",
    "# 6. Save caches for ensemble (treat Qwen prediction as prob 0/1)\n",
    "cache_t1_train = pd.DataFrame({\n",
    "    \"id\":   t1_train_df[\"id\"].astype(str).values,\n",
    "    \"prob_pos\": pred_train_t1.astype(float),   # 0.0 or 1.0\n",
    "    \"label\":   y_true_t1.astype(int),\n",
    "})\n",
    "cache_t1_dev = pd.DataFrame({\n",
    "    \"id\":      t1_dev_df[\"id\"].astype(str).values,\n",
    "    \"prob_pos\": pred_dev_t1.astype(float),\n",
    "})\n",
    "\n",
    "t1_train_cache_path = CACHE_ROOT / \"t1_train_probs.csv\"\n",
    "t1_dev_cache_path   = CACHE_ROOT / \"t1_dev_probs.csv\"\n",
    "cache_t1_train.to_csv(t1_train_cache_path, index=False)\n",
    "cache_t1_dev.to_csv(t1_dev_cache_path, index=False)\n",
    "\n",
    "print(\"Saved T1 TRAIN cache:\", t1_train_cache_path)\n",
    "print(\"Saved T1 DEV cache  :\", t1_dev_cache_path)\n",
    "\n",
    "# 7. Optional: Qwen-only submission CSV for Subtask 1\n",
    "sub1 = pd.DataFrame({\n",
    "    \"id\":           t1_dev_df[\"id\"].astype(str).values,\n",
    "    \"polarization\": pred_dev_t1.astype(int),\n",
    "})\n",
    "sub1_path = SUB_ROOT / \"subtask_1\" / f\"pred_{lang_fname}.csv\"\n",
    "sub1.to_csv(sub1_path, index=False)\n",
    "print(\"Saved Qwen-only Subtask 1 submission:\", sub1_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6801db09",
   "metadata": {},
   "source": [
    "## Subtask 2 (multi-label 5) with Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c8e13f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T2] TRAIN size: 3222\n",
      "[T2] DEV size  : 160\n",
      "\n",
      "[T2] Running Qwen on TRAIN...\n",
      "[T2] Qwen Macro-F1 on TRAIN (hard multi-label): 0.05426726469403591\n",
      "\n",
      "[T2] Running Qwen on DEV...\n",
      "Saved T2 TRAIN cache: cache/qwen/eng/t2_train_probs.csv\n",
      "Saved T2 DEV cache  : cache/qwen/eng/t2_dev_probs.csv\n",
      "Saved Qwen-only Subtask 2 submission: submissions/qwen/subtask_2/pred_eng.csv\n"
     ]
    }
   ],
   "source": [
    "# ## Subtask 2 — Hate type (multi-label 5, Qwen)\n",
    "\n",
    "# 1. Load TRAIN + DEV\n",
    "t2_train_df = pd.read_csv(T2_TRAIN)\n",
    "t2_dev_df   = pd.read_csv(T2_DEV)\n",
    "\n",
    "required_train_cols_t2 = {\"id\", \"text\", *T2_LABELS}\n",
    "required_dev_cols_t2   = {\"id\", \"text\"}\n",
    "assert required_train_cols_t2.issubset(t2_train_df.columns), \\\n",
    "    f\"T2 TRAIN missing: {required_train_cols_t2 - set(t2_train_df.columns)}\"\n",
    "assert required_dev_cols_t2.issubset(t2_dev_df.columns), \\\n",
    "    f\"T2 DEV missing: {required_dev_cols_t2 - set(t2_dev_df.columns)}\"\n",
    "\n",
    "Y2_train = t2_train_df[T2_LABELS].values.astype(int)\n",
    "\n",
    "print(f\"[T2] TRAIN size: {len(t2_train_df)}\")\n",
    "print(f\"[T2] DEV size  : {len(t2_dev_df)}\")\n",
    "\n",
    "# 2. Build prompts\n",
    "train_prompts_t2 = [\n",
    "    build_prompt_t2(txt, LANG) for txt in t2_train_df[\"text\"].astype(str).tolist()\n",
    "]\n",
    "dev_prompts_t2 = [\n",
    "    build_prompt_t2(txt, LANG) for txt in t2_dev_df[\"text\"].astype(str).tolist()\n",
    "]\n",
    "\n",
    "# 3. Run Qwen on TRAIN\n",
    "print(\"\\n[T2] Running Qwen on TRAIN...\")\n",
    "train_outputs_t2 = qwen_generate_batch(train_prompts_t2, max_new_tokens=16, batch_size=4)\n",
    "pred_train_t2 = np.stack(\n",
    "    [parse_digit_vector(o, n_labels=len(T2_LABELS)) for o in train_outputs_t2],\n",
    "    axis=0,\n",
    ")  # [N,5]\n",
    "\n",
    "# 4. Evaluate on TRAIN (macro-F1 across labels)\n",
    "f1_t2 = f1_score(Y2_train, pred_train_t2, average=\"macro\", zero_division=0)\n",
    "print(\"[T2] Qwen Macro-F1 on TRAIN (hard multi-label):\", f1_t2)\n",
    "\n",
    "# 5. Run Qwen on DEV\n",
    "print(\"\\n[T2] Running Qwen on DEV...\")\n",
    "dev_outputs_t2 = qwen_generate_batch(dev_prompts_t2, max_new_tokens=16, batch_size=4)\n",
    "pred_dev_t2 = np.stack(\n",
    "    [parse_digit_vector(o, n_labels=len(T2_LABELS)) for o in dev_outputs_t2],\n",
    "    axis=0,\n",
    ")  # [N_dev,5]\n",
    "\n",
    "# 6. Save caches for ensemble (probabilities are just 0/1 from Qwen)\n",
    "cache_cols_train_t2 = {\"id\": t2_train_df[\"id\"].astype(str).values}\n",
    "cache_cols_dev_t2   = {\"id\": t2_dev_df[\"id\"].astype(str).values}\n",
    "\n",
    "for j, lab in enumerate(T2_LABELS):\n",
    "    cache_cols_train_t2[f\"prob_{lab}\"]  = pred_train_t2[:, j].astype(float)\n",
    "    cache_cols_train_t2[f\"label_{lab}\"] = Y2_train[:, j].astype(int)\n",
    "    cache_cols_dev_t2[f\"prob_{lab}\"]    = pred_dev_t2[:, j].astype(float)\n",
    "\n",
    "t2_train_cache = pd.DataFrame(cache_cols_train_t2)\n",
    "t2_dev_cache   = pd.DataFrame(cache_cols_dev_t2)\n",
    "\n",
    "t2_train_cache_path = CACHE_ROOT / \"t2_train_probs.csv\"\n",
    "t2_dev_cache_path   = CACHE_ROOT / \"t2_dev_probs.csv\"\n",
    "\n",
    "t2_train_cache.to_csv(t2_train_cache_path, index=False)\n",
    "t2_dev_cache.to_csv(t2_dev_cache_path, index=False)\n",
    "\n",
    "print(\"Saved T2 TRAIN cache:\", t2_train_cache_path)\n",
    "print(\"Saved T2 DEV cache  :\", t2_dev_cache_path)\n",
    "\n",
    "# Required header: id,political,racial/ethnic,religious,gender/sexual,other\n",
    "\n",
    "idx_gender    = T2_LABELS.index(\"gender/sexual\")\n",
    "idx_political = T2_LABELS.index(\"political\")\n",
    "idx_religious = T2_LABELS.index(\"religious\")\n",
    "idx_racial    = T2_LABELS.index(\"racial/ethnic\")\n",
    "idx_other     = T2_LABELS.index(\"other\")\n",
    "\n",
    "sub2 = pd.DataFrame({\n",
    "    \"id\":            t2_dev_df[\"id\"].astype(str).values,\n",
    "    \"political\":     pred_dev_t2[:, idx_political],\n",
    "    \"racial/ethnic\": pred_dev_t2[:, idx_racial],\n",
    "    \"religious\":     pred_dev_t2[:, idx_religious],\n",
    "    \"gender/sexual\": pred_dev_t2[:, idx_gender],\n",
    "    \"other\":         pred_dev_t2[:, idx_other],\n",
    "})\n",
    "sub2_path = SUB_ROOT / \"subtask_2\" / f\"pred_{lang_fname}.csv\"\n",
    "sub2.to_csv(sub2_path, index=False)\n",
    "print(\"Saved Qwen-only Subtask 2 submission:\", sub2_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd9885",
   "metadata": {},
   "source": [
    "## Subtask 3 (multi-label 6) with Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6157bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T3] TRAIN size: 3222\n",
      "[T3] DEV size  : 160\n",
      "\n",
      "[T3] Running Qwen on TRAIN...\n",
      "[T3] Qwen Macro-F1 on TRAIN (hard multi-label): 0.16516097002295121\n",
      "\n",
      "[T3] Running Qwen on DEV...\n",
      "Saved T3 TRAIN cache: cache/qwen/eng/t3_train_probs.csv\n",
      "Saved T3 DEV cache  : cache/qwen/eng/t3_dev_probs.csv\n",
      "Saved Qwen-only Subtask 3 submission: submissions/qwen/subtask_3/pred_eng.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Load TRAIN + DEV\n",
    "t3_train_df = pd.read_csv(T3_TRAIN)\n",
    "t3_dev_df   = pd.read_csv(T3_DEV)\n",
    "\n",
    "required_train_cols_t3 = {\"id\", \"text\", *T3_LABELS}\n",
    "required_dev_cols_t3   = {\"id\", \"text\"}\n",
    "assert required_train_cols_t3.issubset(t3_train_df.columns), \\\n",
    "    f\"T3 TRAIN missing: {required_train_cols_t3 - set(t3_train_df.columns)}\"\n",
    "assert required_dev_cols_t3.issubset(t3_dev_df.columns), \\\n",
    "    f\"T3 DEV missing: {required_dev_cols_t3 - set(t3_dev_df.columns)}\"\n",
    "\n",
    "Y3_train = t3_train_df[T3_LABELS].values.astype(int)\n",
    "\n",
    "print(f\"[T3] TRAIN size: {len(t3_train_df)}\")\n",
    "print(f\"[T3] DEV size  : {len(t3_dev_df)}\")\n",
    "\n",
    "# 2. Build prompts\n",
    "train_prompts_t3 = [\n",
    "    build_prompt_t3(txt, LANG) for txt in t3_train_df[\"text\"].astype(str).tolist()\n",
    "]\n",
    "dev_prompts_t3 = [\n",
    "    build_prompt_t3(txt, LANG) for txt in t3_dev_df[\"text\"].astype(str).tolist()\n",
    "]\n",
    "\n",
    "# 3. Run Qwen on TRAIN\n",
    "print(\"\\n[T3] Running Qwen on TRAIN...\")\n",
    "train_outputs_t3 = qwen_generate_batch(train_prompts_t3, max_new_tokens=16, batch_size=4)\n",
    "pred_train_t3 = np.stack(\n",
    "    [parse_digit_vector(o, n_labels=len(T3_LABELS)) for o in train_outputs_t3],\n",
    "    axis=0,\n",
    ")  # [N,6]\n",
    "\n",
    "# 4. Evaluate on TRAIN\n",
    "f1_t3 = f1_score(Y3_train, pred_train_t3, average=\"macro\", zero_division=0)\n",
    "print(\"[T3] Qwen Macro-F1 on TRAIN (hard multi-label):\", f1_t3)\n",
    "\n",
    "# 5. Run Qwen on DEV\n",
    "print(\"\\n[T3] Running Qwen on DEV...\")\n",
    "dev_outputs_t3 = qwen_generate_batch(dev_prompts_t3, max_new_tokens=16, batch_size=4)\n",
    "pred_dev_t3 = np.stack(\n",
    "    [parse_digit_vector(o, n_labels=len(T3_LABELS)) for o in dev_outputs_t3],\n",
    "    axis=0,\n",
    ")  # [N_dev,6]\n",
    "\n",
    "# 6. Save caches for ensemble\n",
    "cache_cols_train_t3 = {\"id\": t3_train_df[\"id\"].astype(str).values}\n",
    "cache_cols_dev_t3   = {\"id\": t3_dev_df[\"id\"].astype(str).values}\n",
    "\n",
    "for j, lab in enumerate(T3_LABELS):\n",
    "    cache_cols_train_t3[f\"prob_{lab}\"]  = pred_train_t3[:, j].astype(float)\n",
    "    cache_cols_train_t3[f\"label_{lab}\"] = Y3_train[:, j].astype(int)\n",
    "    cache_cols_dev_t3[f\"prob_{lab}\"]    = pred_dev_t3[:, j].astype(float)\n",
    "\n",
    "t3_train_cache = pd.DataFrame(cache_cols_train_t3)\n",
    "t3_dev_cache   = pd.DataFrame(cache_cols_dev_t3)\n",
    "\n",
    "t3_train_cache_path = CACHE_ROOT / \"t3_train_probs.csv\"\n",
    "t3_dev_cache_path   = CACHE_ROOT / \"t3_dev_probs.csv\"\n",
    "\n",
    "t3_train_cache.to_csv(t3_train_cache_path, index=False)\n",
    "t3_dev_cache.to_csv(t3_dev_cache_path, index=False)\n",
    "\n",
    "print(\"Saved T3 TRAIN cache:\", t3_train_cache_path)\n",
    "print(\"Saved T3 DEV cache  :\", t3_dev_cache_path)\n",
    "\n",
    "# 7. Qwen-only submission for Subtask 3\n",
    "idx_vil      = T3_LABELS.index(\"vilification\")\n",
    "idx_extreme  = T3_LABELS.index(\"extreme_language\")\n",
    "idx_stereo   = T3_LABELS.index(\"stereotype\")\n",
    "idx_invalid  = T3_LABELS.index(\"invalidation\")\n",
    "idx_lackemp  = T3_LABELS.index(\"lack_of_empathy\")\n",
    "idx_dehum    = T3_LABELS.index(\"dehumanization\")\n",
    "\n",
    "sub3 = pd.DataFrame({\n",
    "    \"id\":               t3_dev_df[\"id\"].astype(str).values,\n",
    "    \"stereotype\":       pred_dev_t3[:, idx_stereo],\n",
    "    \"vilification\":     pred_dev_t3[:, idx_vil],\n",
    "    \"dehumanization\":   pred_dev_t3[:, idx_dehum],\n",
    "    \"extreme_language\": pred_dev_t3[:, idx_extreme],\n",
    "    \"lack_of_empathy\":  pred_dev_t3[:, idx_lackemp],\n",
    "    \"invalidation\":     pred_dev_t3[:, idx_invalid],\n",
    "})\n",
    "sub3_path = SUB_ROOT / \"subtask_3\" / f\"pred_{lang_fname}.csv\"\n",
    "sub3.to_csv(sub3_path, index=False)\n",
    "print(\"Saved Qwen-only Subtask 3 submission:\", sub3_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
